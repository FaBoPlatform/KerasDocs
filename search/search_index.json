{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FaBo Keras Docs FaBo Keras Docs\u306b\u3064\u3044\u3066 FaBo Swift Docs\u306f\u3001Swift\u9006\u5f15\u304d\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u3059\u3002 \u4fee\u6b63\u4f9d\u983c\u7b49 Github\u306eRepo\u306b Issues \u3092\u3042\u3052\u308b\u3002 \u305d\u306e\u4ed6\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 DonkeyCar Docs Denbun Docs Circuit Docs Swift Docs Keras Docs Things Docs Nordic Docs DeviceWebAPI Docs","title":"FaBo Keras Docs"},{"location":"#fabo-keras-docs","text":"","title":"FaBo Keras Docs"},{"location":"#fabo-keras-docs_1","text":"FaBo Swift Docs\u306f\u3001Swift\u9006\u5f15\u304d\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u3059\u3002","title":"FaBo Keras Docs\u306b\u3064\u3044\u3066"},{"location":"#_1","text":"Github\u306eRepo\u306b Issues \u3092\u3042\u3052\u308b\u3002","title":"\u4fee\u6b63\u4f9d\u983c\u7b49"},{"location":"#_2","text":"DonkeyCar Docs Denbun Docs Circuit Docs Swift Docs Keras Docs Things Docs Nordic Docs DeviceWebAPI Docs","title":"\u305d\u306e\u4ed6\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8"},{"location":"android_support/","text":"\u52d5\u4f5c\u78ba\u8a8d \u6a5f\u7a2e MCU OS \u30e2\u30c7\u30eb \u52d5\u4f5c Nexus 5X Snapdragon 808 8.1.0 ssd_mobilenet_v1_0.75_depth_quantized_coco \u554f\u984c\u306a\u304f\u52d5\u4f5c Nexus 5X Snapdragon 808 8.1.0 ssd_mobilenet_v1_ppn_coco \u30a8\u30e9\u30fc1 \u30a8\u30e9\u30fc1 1 Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/conv.cc:260 real_multiplier < 1 .0 was not true.Node 28 failed to prepare.","title":"\u52d5\u4f5c\u78ba\u8a8d"},{"location":"android_support/#_1","text":"\u6a5f\u7a2e MCU OS \u30e2\u30c7\u30eb \u52d5\u4f5c Nexus 5X Snapdragon 808 8.1.0 ssd_mobilenet_v1_0.75_depth_quantized_coco \u554f\u984c\u306a\u304f\u52d5\u4f5c Nexus 5X Snapdragon 808 8.1.0 ssd_mobilenet_v1_ppn_coco \u30a8\u30e9\u30fc1","title":"\u52d5\u4f5c\u78ba\u8a8d"},{"location":"android_support/#1","text":"1 Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/conv.cc:260 real_multiplier < 1 .0 was not true.Node 28 failed to prepare.","title":"\u30a8\u30e9\u30fc1"},{"location":"automl/","text":"AutoML AutoML\u306f\u3001\u7c21\u5358\u306a\u51e6\u7406\u3067\u72ec\u81ea\u306e\u30e2\u30c7\u30eb\u304c\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\u73fe\u5728\u3001 Vision , Translate , Natural Language \u306e3\u3064\u304cBeta\u516c\u958b\u3055\u308c\u3066\u3044\u307e\u3059\u3002 AutoML Vision\u306e\u6e96\u5099 AutoML Vision\u3092\u4f7f\u7528\u3059\u308b\u306b\u306f\u3001\u6599\u91d1\u652f\u6255\u306e\u8a2d\u5b9a\u3068\u3001API\u306e\u6709\u52b9\u5316\u304c\u5fc5\u8981\u3067\u3059\u3002 AutoML\u7528\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u4f5c\u6210 \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u652f\u6255\u3044\u306e\u6709\u52b9\u5316 AutoML\u95a2\u9023 API\u306e\u6709\u52b9\u5316 Dataset\u306e\u65b0\u898f\u4f5c\u6210 Image\u306e\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9 Aizu Dataset \u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002\u4e2d\u306b\u3001\u8d64\u3079\u3053\u3068\u8d77\u304d\u4e0a\u304c\u308a\u5c0f\u6cd5\u5e2b\u306e\u753b\u50cf\u304c\u306f\u3044\u3063\u3066\u3044\u307e\u3059\u3002 Label\u4f5c\u6210 Image\u3078\u306eLabel\u3065\u3051 \u5b66\u7fd2 \u8a55\u4fa1","title":"AutoML"},{"location":"automl/#automl","text":"AutoML\u306f\u3001\u7c21\u5358\u306a\u51e6\u7406\u3067\u72ec\u81ea\u306e\u30e2\u30c7\u30eb\u304c\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\u73fe\u5728\u3001 Vision , Translate , Natural Language \u306e3\u3064\u304cBeta\u516c\u958b\u3055\u308c\u3066\u3044\u307e\u3059\u3002","title":"AutoML"},{"location":"automl/#automl-vision","text":"AutoML Vision\u3092\u4f7f\u7528\u3059\u308b\u306b\u306f\u3001\u6599\u91d1\u652f\u6255\u306e\u8a2d\u5b9a\u3068\u3001API\u306e\u6709\u52b9\u5316\u304c\u5fc5\u8981\u3067\u3059\u3002 AutoML\u7528\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u4f5c\u6210 \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u652f\u6255\u3044\u306e\u6709\u52b9\u5316 AutoML\u95a2\u9023 API\u306e\u6709\u52b9\u5316","title":"AutoML Vision\u306e\u6e96\u5099"},{"location":"automl/#dataset","text":"","title":"Dataset\u306e\u65b0\u898f\u4f5c\u6210"},{"location":"automl/#image","text":"Aizu Dataset \u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002\u4e2d\u306b\u3001\u8d64\u3079\u3053\u3068\u8d77\u304d\u4e0a\u304c\u308a\u5c0f\u6cd5\u5e2b\u306e\u753b\u50cf\u304c\u306f\u3044\u3063\u3066\u3044\u307e\u3059\u3002","title":"Image\u306e\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9"},{"location":"automl/#label","text":"","title":"Label\u4f5c\u6210"},{"location":"automl/#imagelabel","text":"","title":"Image\u3078\u306eLabel\u3065\u3051"},{"location":"automl/#_1","text":"","title":"\u5b66\u7fd2"},{"location":"automl/#_2","text":"","title":"\u8a55\u4fa1"},{"location":"automl_pi/","text":"AutoML","title":"AutoML"},{"location":"automl_pi/#automl","text":"","title":"AutoML"},{"location":"dev_mac/","text":"Local\u74b0\u5883\u306e\u69cb\u7bc9(MAC) pip install opencv-python pip install socketio pip install Image pip install keras pip install Flask pip install tensorflow \u5b66\u7fd2\u6e08\u307fModel\u306eDownload \u5b9f\u884c\u30bd\u30fc\u30b9 https://github.com/ymshao/End-to-End-Learning-for-Self-Driving-Cars/blob/master/drive.py \u3088\u308a drive.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 import argparse import base64 import json import cv2 import numpy as np import socketio import eventlet import eventlet.wsgi import time from PIL import Image from PIL import ImageOps from flask import Flask , render_template from io import BytesIO from keras.models import model_from_json from keras.preprocessing.image import ImageDataGenerator , array_to_img , img_to_array # Fix error with Keras and TensorFlow import tensorflow as tf tf . python . control_flow_ops = tf sio = socketio . Server () app = Flask ( __name__ ) model = None prev_image_array = None @sio.on ( 'telemetry' ) def telemetry ( sid , data ): # The current steering angle of the car steering_angle = data [ \"steering_angle\" ] # The current throttle of the car throttle = data [ \"throttle\" ] # The current speed of the car speed = data [ \"speed\" ] # The current image from the center camera of the car imgString = data [ \"image\" ] image = Image . open ( BytesIO ( base64 . b64decode ( imgString ))) image_array = np . asarray ( image ) transformed_image_array = image_array [ None , :, :, :] #resize the image transformed_image_array = ( cv2 . resize (( cv2 . cvtColor ( transformed_image_array [ 0 ], cv2 . COLOR_RGB2HSV ))[:,:, 1 ],( 32 , 16 ))) . reshape ( 1 , 16 , 32 , 1 ) # This model currently assumes that the features of the model are just the images. Feel free to change this. steering_angle = float ( model . predict ( transformed_image_array , batch_size = 1 )) # The driving model currently just outputs a constant throttle. Feel free to edit this. throttle = 0.2 #adaptive speed ''' if (float(speed) < 10): throttle = 0.4 else: # When speed is below 20 then increase throttle by speed_factor if ((float(speed)) < 25): speed_factor = 1.35 else: speed_factor = 1.0 if (abs(steering_angle) < 0.1): throttle = 0.3 * speed_factor elif (abs(steering_angle) < 0.5): throttle = 0.2 * speed_factor else: throttle = 0.15 * speed_factor ''' print ( 'Steering angle =' , ' %5.2f ' % ( float ( steering_angle )), 'Throttle =' , ' %.2f ' % ( float ( throttle )), 'Speed =' , ' %.2f ' % ( float ( speed ))) send_control ( steering_angle , throttle ) @sio.on ( 'connect' ) def connect ( sid , environ ): print ( \"connect \" , sid ) send_control ( 0 , 0 ) def send_control ( steering_angle , throttle ): sio . emit ( \"steer\" , data = { 'steering_angle' : steering_angle . __str__ (), 'throttle' : throttle . __str__ () }, skip_sid = True ) if __name__ == '__main__' : parser = argparse . ArgumentParser ( description = 'Remote Driving' ) parser . add_argument ( 'model' , type = str , help = 'Path to model definition json. Model weights should be on the same path.' ) args = parser . parse_args () with open ( args . model , 'r' ) as jfile : # NOTE: if you saved the file by calling json.dump(model.to_json(), ...) # then you will have to call: # # model = model_from_json(json.loads(jfile.read()))\\ # # instead. model = model_from_json ( jfile . read ()) model . compile ( \"adam\" , \"mse\" ) weights_file = args . model . replace ( 'json' , 'h5' ) model . load_weights ( weights_file ) # wrap Flask application with engineio's middleware app = socketio . Middleware ( sio , app ) # deploy as an eventlet WSGI server eventlet . wsgi . server ( eventlet . listen (( '' , 4567 )), app ) \u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u30fc\u306e\u8d77\u52d5 AI\u306e\u5b9f\u884c python drive.py model.json","title":"Dev mac"},{"location":"dev_mac/#localmac","text":"pip install opencv-python pip install socketio pip install Image pip install keras pip install Flask pip install tensorflow","title":"Local\u74b0\u5883\u306e\u69cb\u7bc9(MAC)"},{"location":"dev_mac/#modeldownload","text":"","title":"\u5b66\u7fd2\u6e08\u307fModel\u306eDownload"},{"location":"dev_mac/#_1","text":"https://github.com/ymshao/End-to-End-Learning-for-Self-Driving-Cars/blob/master/drive.py \u3088\u308a drive.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 import argparse import base64 import json import cv2 import numpy as np import socketio import eventlet import eventlet.wsgi import time from PIL import Image from PIL import ImageOps from flask import Flask , render_template from io import BytesIO from keras.models import model_from_json from keras.preprocessing.image import ImageDataGenerator , array_to_img , img_to_array # Fix error with Keras and TensorFlow import tensorflow as tf tf . python . control_flow_ops = tf sio = socketio . Server () app = Flask ( __name__ ) model = None prev_image_array = None @sio.on ( 'telemetry' ) def telemetry ( sid , data ): # The current steering angle of the car steering_angle = data [ \"steering_angle\" ] # The current throttle of the car throttle = data [ \"throttle\" ] # The current speed of the car speed = data [ \"speed\" ] # The current image from the center camera of the car imgString = data [ \"image\" ] image = Image . open ( BytesIO ( base64 . b64decode ( imgString ))) image_array = np . asarray ( image ) transformed_image_array = image_array [ None , :, :, :] #resize the image transformed_image_array = ( cv2 . resize (( cv2 . cvtColor ( transformed_image_array [ 0 ], cv2 . COLOR_RGB2HSV ))[:,:, 1 ],( 32 , 16 ))) . reshape ( 1 , 16 , 32 , 1 ) # This model currently assumes that the features of the model are just the images. Feel free to change this. steering_angle = float ( model . predict ( transformed_image_array , batch_size = 1 )) # The driving model currently just outputs a constant throttle. Feel free to edit this. throttle = 0.2 #adaptive speed ''' if (float(speed) < 10): throttle = 0.4 else: # When speed is below 20 then increase throttle by speed_factor if ((float(speed)) < 25): speed_factor = 1.35 else: speed_factor = 1.0 if (abs(steering_angle) < 0.1): throttle = 0.3 * speed_factor elif (abs(steering_angle) < 0.5): throttle = 0.2 * speed_factor else: throttle = 0.15 * speed_factor ''' print ( 'Steering angle =' , ' %5.2f ' % ( float ( steering_angle )), 'Throttle =' , ' %.2f ' % ( float ( throttle )), 'Speed =' , ' %.2f ' % ( float ( speed ))) send_control ( steering_angle , throttle ) @sio.on ( 'connect' ) def connect ( sid , environ ): print ( \"connect \" , sid ) send_control ( 0 , 0 ) def send_control ( steering_angle , throttle ): sio . emit ( \"steer\" , data = { 'steering_angle' : steering_angle . __str__ (), 'throttle' : throttle . __str__ () }, skip_sid = True ) if __name__ == '__main__' : parser = argparse . ArgumentParser ( description = 'Remote Driving' ) parser . add_argument ( 'model' , type = str , help = 'Path to model definition json. Model weights should be on the same path.' ) args = parser . parse_args () with open ( args . model , 'r' ) as jfile : # NOTE: if you saved the file by calling json.dump(model.to_json(), ...) # then you will have to call: # # model = model_from_json(json.loads(jfile.read()))\\ # # instead. model = model_from_json ( jfile . read ()) model . compile ( \"adam\" , \"mse\" ) weights_file = args . model . replace ( 'json' , 'h5' ) model . load_weights ( weights_file ) # wrap Flask application with engineio's middleware app = socketio . Middleware ( sio , app ) # deploy as an eventlet WSGI server eventlet . wsgi . server ( eventlet . listen (( '' , 4567 )), app )","title":"\u5b9f\u884c\u30bd\u30fc\u30b9"},{"location":"dev_mac/#_2","text":"","title":"\u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u30fc\u306e\u8d77\u52d5"},{"location":"dev_mac/#ai","text":"python drive.py model.json","title":"AI\u306e\u5b9f\u884c"},{"location":"end2end_dataset/","text":"End2End Dataset \u6559\u5e2b\u30c7\u30fc\u30bf https://github.com/ymshao/End-to-End-Learning-for-Self-Driving-Cars \u306b\u516c\u958b\u3055\u308c\u3066\u3044\u308b\u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u306b\u3088\u308a\u6559\u5e2b\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\u3002 driving_log.csv 1 2 3 4 5 center,left,right,steering,throttle,brake,speed IMG/center_2016_12_01_13_30_48_287.jpg, IMG/left_2016_12_01_13_30_48_287.jpg, IMG/right_2016_12_01_13_30_48_287.jpg, 0, 0, 0, 22.14829 IMG/center_2016_12_01_13_30_48_404.jpg, IMG/left_2016_12_01_13_30_48_404.jpg, IMG/right_2016_12_01_13_30_48_404.jpg, 0, 0, 0, 21.87963 IMG/center_2016_12_01_13_31_12_937.jpg, IMG/left_2016_12_01_13_31_12_937.jpg, IMG/right_2016_12_01_13_31_12_937.jpg, 0, 0, 0, 1.453011 IMG/center_2016_12_01_13_31_13_037.jpg, IMG/left_2016_12_01_13_31_13_037.jpg, IMG/right_2016_12_01_13_31_13_037.jpg, 0, 0, 0, 1.438419 \u30c7\u30fc\u30bf\u306e\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9 IMG\u30d5\u30a9\u30eb\u30c0\u3068\u3001driving_log.csv\u3092ZIP\u306b\u56fa\u3081\u3066Jupyter\u306b\u30a2\u30c3\u30d7\u3059\u308b\u3002 \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import csv from matplotlib import pyplot as plt def load_data (): folder_path = '' center_img = [] direction_handle = [] f = open ( folder_path + 'driving_log.csv' , 'r' ) reader = csv . reader ( f ) header = next ( reader ) for row in reader : img_name = row [ 0 ] img_path = folder_path + img_name img = plt . imread ( img_path ) center_img . append ( img ) direction_handle . append ( float ( row [ 3 ])) f . close () return ( center_img , direction_handle ) (center_img, direction_handle) = load_data() \u30c7\u30fc\u30bf\u306e\u5897\u5e45 delta\u3060\u3051\u5de6\u53f3\u306b\u305a\u3089\u3057\u305f\u30cf\u30f3\u30c9\u30eb\u306e\u89d2\u5ea6\u3068\u4e2d\u5fc3\u753b\u50cf\u3067\u30013\u500d\u306e\u30c7\u30fc\u30bf\u306b\u5897\u3084\u3059\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 def grow_data ( center_img , direction_handle , delta ): new_center = [] new_direction = [] for i in range ( len ( center_img )): for j in range ( 3 ): new_center . append ( features [ i ]) if j == 0 : new_direction . append ( direction_handle [ i ]) elif j == 1 : new_direction . append ( direction_handle [ i ] + float ( delta )) elif j == 2 : new_direction . append ( direction_handle [ i ] - float ( delta )) return ( new_center , new_direction ) delta = 0.2 (center_img, direction_handle) = grow_data(center_img, direction_handle, delta) \u753b\u50cf\u51e6\u7406 Crop\u3067\u7bc4\u56f2\u3092\u533a\u5207\u308a\u3001\u8272\u7a7a\u9593\u306e\u5909\u63db\u3068R,G,B\u306e\u3044\u305a\u308c\u304b1\u8272\u3092\u53d6\u5f97\u3059\u308b\u3002 1 2 3 4 5 6 7 8 9 10 import cv2 def process_img ( center_img ): new_center = [] for img in center_img : crop_img = img [ 60 : 140 , 0 : 320 , :] resize_img = cv2 . resize (( cv2 . cvtColor ( crop_img , cv2 . COLOR_RGB2BGR ))[:,:, 0 ],( 40 , 10 )) new_center . append ( resize_img ) return new_center center_data = process_img(center_img) \u78ba\u8a8d \u753b\u50cf\u3092\u8868\u793a\u3057\u3066\u78ba\u8a8d 1 2 3 4 5 6 7 8 9 10 11 def draw_img ( center_img , direction_handle , pos = 0 ): g_rows = 3 g_cols = 3 size = g_rows * g_cols fig , axs = plt . subplots ( ncols = g_rows , nrows = size / g_rows ) for h in range ( g_rows ): for i in range ( g_cols ): axs [ h ][ i ] . imshow ( center_img [ pos + i * h ], cmap = plt . cm . gray_r ,) axs [ h ][ i ] . set_title ( str ( direction_handle [ pos + h * i ])) plt . show () draw_img(center_data, direction_handle, 50) \u3053\u3053\u307e\u3067\u306e\u30bd\u30fc\u30b9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 import csv from matplotlib import pyplot as plt import cv2 def load_data (): folder_path = '' center_img = [] direction_handle = [] f = open ( folder_path + 'driving_log.csv' , 'r' ) reader = csv . reader ( f ) header = next ( reader ) for row in reader : img_name = row [ 0 ] img_path = folder_path + img_name img = plt . imread ( img_path ) center_img . append ( img ) direction_handle . append ( float ( row [ 3 ])) f . close () return ( center_img , direction_handle ) def draw_img ( center_img , direction_handle , pos = 0 ): g_rows = 3 g_cols = 3 size = g_rows * g_cols fig , axs = plt . subplots ( ncols = g_rows , nrows = size / g_rows ) for h in range ( g_rows ): for i in range ( g_cols ): axs [ h ][ i ] . imshow ( center_img [ pos + i * h ], cmap = plt . cm . gray_r ,) axs [ h ][ i ] . set_title ( str ( direction_handle [ pos + h * i ])) plt . show () def process_img ( center_img ): new_center = [] for img in center_img : crop_img = img [ 60 : 140 , 0 : 320 , :] resize_img = cv2 . resize (( cv2 . cvtColor ( crop_img , cv2 . COLOR_RGB2XYZ ))[:,:, 2 ],( 40 , 10 )) new_center . append ( resize_img ) return new_center def grow_data ( center_img , direction_handle , delta ): new_center = [] new_direction = [] for i in range ( len ( center_img )): for j in range ( 3 ): new_center . append ( center_img [ i ]) if j == 0 : new_direction . append ( direction_handle [ i ]) elif j == 1 : new_direction . append ( direction_handle [ i ] + float ( delta )) elif j == 2 : new_direction . append ( direction_handle [ i ] - float ( delta )) return ( new_center , new_direction ) if __name__ == '__main__' : ( center_img , direction_handle ) = load_data () delta = 0.2 ( center_img , direction_handle ) = grow_data ( center_img , direction_handle , delta ) center_data = process_img ( center_img ) draw_img ( center_data , direction_handle , 50 )","title":"End2End Dataset"},{"location":"end2end_dataset/#end2end-dataset","text":"","title":"End2End Dataset"},{"location":"end2end_dataset/#_1","text":"https://github.com/ymshao/End-to-End-Learning-for-Self-Driving-Cars \u306b\u516c\u958b\u3055\u308c\u3066\u3044\u308b\u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u306b\u3088\u308a\u6559\u5e2b\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\u3002 driving_log.csv 1 2 3 4 5 center,left,right,steering,throttle,brake,speed IMG/center_2016_12_01_13_30_48_287.jpg, IMG/left_2016_12_01_13_30_48_287.jpg, IMG/right_2016_12_01_13_30_48_287.jpg, 0, 0, 0, 22.14829 IMG/center_2016_12_01_13_30_48_404.jpg, IMG/left_2016_12_01_13_30_48_404.jpg, IMG/right_2016_12_01_13_30_48_404.jpg, 0, 0, 0, 21.87963 IMG/center_2016_12_01_13_31_12_937.jpg, IMG/left_2016_12_01_13_31_12_937.jpg, IMG/right_2016_12_01_13_31_12_937.jpg, 0, 0, 0, 1.453011 IMG/center_2016_12_01_13_31_13_037.jpg, IMG/left_2016_12_01_13_31_13_037.jpg, IMG/right_2016_12_01_13_31_13_037.jpg, 0, 0, 0, 1.438419","title":"\u6559\u5e2b\u30c7\u30fc\u30bf"},{"location":"end2end_dataset/#_2","text":"IMG\u30d5\u30a9\u30eb\u30c0\u3068\u3001driving_log.csv\u3092ZIP\u306b\u56fa\u3081\u3066Jupyter\u306b\u30a2\u30c3\u30d7\u3059\u308b\u3002","title":"\u30c7\u30fc\u30bf\u306e\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9"},{"location":"end2end_dataset/#_3","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import csv from matplotlib import pyplot as plt def load_data (): folder_path = '' center_img = [] direction_handle = [] f = open ( folder_path + 'driving_log.csv' , 'r' ) reader = csv . reader ( f ) header = next ( reader ) for row in reader : img_name = row [ 0 ] img_path = folder_path + img_name img = plt . imread ( img_path ) center_img . append ( img ) direction_handle . append ( float ( row [ 3 ])) f . close () return ( center_img , direction_handle ) (center_img, direction_handle) = load_data()","title":"\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f"},{"location":"end2end_dataset/#_4","text":"delta\u3060\u3051\u5de6\u53f3\u306b\u305a\u3089\u3057\u305f\u30cf\u30f3\u30c9\u30eb\u306e\u89d2\u5ea6\u3068\u4e2d\u5fc3\u753b\u50cf\u3067\u30013\u500d\u306e\u30c7\u30fc\u30bf\u306b\u5897\u3084\u3059\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 def grow_data ( center_img , direction_handle , delta ): new_center = [] new_direction = [] for i in range ( len ( center_img )): for j in range ( 3 ): new_center . append ( features [ i ]) if j == 0 : new_direction . append ( direction_handle [ i ]) elif j == 1 : new_direction . append ( direction_handle [ i ] + float ( delta )) elif j == 2 : new_direction . append ( direction_handle [ i ] - float ( delta )) return ( new_center , new_direction ) delta = 0.2 (center_img, direction_handle) = grow_data(center_img, direction_handle, delta)","title":"\u30c7\u30fc\u30bf\u306e\u5897\u5e45"},{"location":"end2end_dataset/#_5","text":"Crop\u3067\u7bc4\u56f2\u3092\u533a\u5207\u308a\u3001\u8272\u7a7a\u9593\u306e\u5909\u63db\u3068R,G,B\u306e\u3044\u305a\u308c\u304b1\u8272\u3092\u53d6\u5f97\u3059\u308b\u3002 1 2 3 4 5 6 7 8 9 10 import cv2 def process_img ( center_img ): new_center = [] for img in center_img : crop_img = img [ 60 : 140 , 0 : 320 , :] resize_img = cv2 . resize (( cv2 . cvtColor ( crop_img , cv2 . COLOR_RGB2BGR ))[:,:, 0 ],( 40 , 10 )) new_center . append ( resize_img ) return new_center center_data = process_img(center_img)","title":"\u753b\u50cf\u51e6\u7406"},{"location":"end2end_dataset/#_6","text":"\u753b\u50cf\u3092\u8868\u793a\u3057\u3066\u78ba\u8a8d 1 2 3 4 5 6 7 8 9 10 11 def draw_img ( center_img , direction_handle , pos = 0 ): g_rows = 3 g_cols = 3 size = g_rows * g_cols fig , axs = plt . subplots ( ncols = g_rows , nrows = size / g_rows ) for h in range ( g_rows ): for i in range ( g_cols ): axs [ h ][ i ] . imshow ( center_img [ pos + i * h ], cmap = plt . cm . gray_r ,) axs [ h ][ i ] . set_title ( str ( direction_handle [ pos + h * i ])) plt . show () draw_img(center_data, direction_handle, 50)","title":"\u78ba\u8a8d"},{"location":"end2end_dataset/#_7","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 import csv from matplotlib import pyplot as plt import cv2 def load_data (): folder_path = '' center_img = [] direction_handle = [] f = open ( folder_path + 'driving_log.csv' , 'r' ) reader = csv . reader ( f ) header = next ( reader ) for row in reader : img_name = row [ 0 ] img_path = folder_path + img_name img = plt . imread ( img_path ) center_img . append ( img ) direction_handle . append ( float ( row [ 3 ])) f . close () return ( center_img , direction_handle ) def draw_img ( center_img , direction_handle , pos = 0 ): g_rows = 3 g_cols = 3 size = g_rows * g_cols fig , axs = plt . subplots ( ncols = g_rows , nrows = size / g_rows ) for h in range ( g_rows ): for i in range ( g_cols ): axs [ h ][ i ] . imshow ( center_img [ pos + i * h ], cmap = plt . cm . gray_r ,) axs [ h ][ i ] . set_title ( str ( direction_handle [ pos + h * i ])) plt . show () def process_img ( center_img ): new_center = [] for img in center_img : crop_img = img [ 60 : 140 , 0 : 320 , :] resize_img = cv2 . resize (( cv2 . cvtColor ( crop_img , cv2 . COLOR_RGB2XYZ ))[:,:, 2 ],( 40 , 10 )) new_center . append ( resize_img ) return new_center def grow_data ( center_img , direction_handle , delta ): new_center = [] new_direction = [] for i in range ( len ( center_img )): for j in range ( 3 ): new_center . append ( center_img [ i ]) if j == 0 : new_direction . append ( direction_handle [ i ]) elif j == 1 : new_direction . append ( direction_handle [ i ] + float ( delta )) elif j == 2 : new_direction . append ( direction_handle [ i ] - float ( delta )) return ( new_center , new_direction ) if __name__ == '__main__' : ( center_img , direction_handle ) = load_data () delta = 0.2 ( center_img , direction_handle ) = grow_data ( center_img , direction_handle , delta ) center_data = process_img ( center_img ) draw_img ( center_data , direction_handle , 50 )","title":"\u3053\u3053\u307e\u3067\u306e\u30bd\u30fc\u30b9"},{"location":"end2end_emulator/","text":"Emulator \u4e0b\u8a18\u30ec\u30dd\u30b8\u30c8\u30ea\u3088\u308a\u3001\u5404\u74b0\u5883\u306e\u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3002 https://github.com/ymshao/End-to-End-Learning-for-Self-Driving-Cars Dataset\u306e\u4f5c\u6210 Emulator\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002 \u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u306b\u306f\u3001Training Mode\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002 Record\u3092\u62bc\u3059\u3068\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30d5\u30a9\u30eb\u30c0\u3092\u9078\u629e\u3059\u308b\u753b\u9762\u304c\u3067\u3066\u304d\u307e\u3059\u3002 \u518d\u3073\u3001Record\u3092\u62bc\u3059\u3068\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\u304c\u59cb\u307e\u308a\u307e\u3059\u3002\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\u4e2d\u306f\u3001Recornding\u306e\u8a18\u8ff0\u306b\u304b\u308f\u308a\u307e\u3059\u3002 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\u3092\u7d42\u308f\u308b\u5834\u5408\u306f\u3001Stop\u3092\u62bc\u3057\u307e\u3059\u3002 \u4f5c\u6210\u3055\u308c\u308b\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8 \u4f5c\u6210\u3055\u308c\u308b\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u306f\u4ee5\u4e0b\u306e\u30d5\u30a9\u30eb\u30c0\u306b\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002 driving_log.csv center,left,right,steering,throttle,brake,speed 1 2 3 4 5 center,left,right,steering,throttle,brake,speed IMG/center_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 287 .jpg, IMG/left_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 287 .jpg, IMG/right_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 287 .jpg, 0 , 0 , 0 , 22.14829 IMG/center_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 404 .jpg, IMG/left_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 404 .jpg, IMG/right_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 404 .jpg, 0 , 0 , 0 , 21.87963 IMG/center_ 2016 _ 12 _ 01 _ 13 _ 31 _ 12 _ 937 .jpg, IMG/left_ 2016 _ 12 _ 01 _ 13 _ 31 _ 12 _ 937 .jpg, IMG/right_ 2016 _ 12 _ 01 _ 13 _ 31 _ 12 _ 937 .jpg, 0 , 0 , 0 , 1.453011 IMG/center_ 2016 _ 12 _ 01 _ 13 _ 31 _ 13 _ 037 .jpg, IMG/left_ 2016 _ 12 _ 01 _ 13 _ 31 _ 13 _ 037 .jpg, IMG/right_ 2016 _ 12 _ 01 _ 13 _ 31 _ 13 _ 037 .jpg, 0 , 0 , 0 , 1.438419 IMG IMG\u30d5\u30a9\u30eb\u30c0\u306b\u306f\u3001\u53f3\u3001\u4e2d\u592e\u3001\u5de6\u306e\u753b\u50cf\u304c\u4fdd\u5b58\u3055\u308c\u3066\u3044\u304d\u307e\u3059\u3002","title":"Emulator"},{"location":"end2end_emulator/#emulator","text":"\u4e0b\u8a18\u30ec\u30dd\u30b8\u30c8\u30ea\u3088\u308a\u3001\u5404\u74b0\u5883\u306e\u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3002 https://github.com/ymshao/End-to-End-Learning-for-Self-Driving-Cars","title":"Emulator"},{"location":"end2end_emulator/#dataset","text":"Emulator\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002 \u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u306b\u306f\u3001Training Mode\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002 Record\u3092\u62bc\u3059\u3068\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30d5\u30a9\u30eb\u30c0\u3092\u9078\u629e\u3059\u308b\u753b\u9762\u304c\u3067\u3066\u304d\u307e\u3059\u3002 \u518d\u3073\u3001Record\u3092\u62bc\u3059\u3068\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\u304c\u59cb\u307e\u308a\u307e\u3059\u3002\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\u4e2d\u306f\u3001Recornding\u306e\u8a18\u8ff0\u306b\u304b\u308f\u308a\u307e\u3059\u3002 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\u3092\u7d42\u308f\u308b\u5834\u5408\u306f\u3001Stop\u3092\u62bc\u3057\u307e\u3059\u3002","title":"Dataset\u306e\u4f5c\u6210"},{"location":"end2end_emulator/#_1","text":"\u4f5c\u6210\u3055\u308c\u308b\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u306f\u4ee5\u4e0b\u306e\u30d5\u30a9\u30eb\u30c0\u306b\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002","title":"\u4f5c\u6210\u3055\u308c\u308b\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8"},{"location":"end2end_emulator/#driving_logcsv","text":"center,left,right,steering,throttle,brake,speed 1 2 3 4 5 center,left,right,steering,throttle,brake,speed IMG/center_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 287 .jpg, IMG/left_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 287 .jpg, IMG/right_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 287 .jpg, 0 , 0 , 0 , 22.14829 IMG/center_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 404 .jpg, IMG/left_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 404 .jpg, IMG/right_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 404 .jpg, 0 , 0 , 0 , 21.87963 IMG/center_ 2016 _ 12 _ 01 _ 13 _ 31 _ 12 _ 937 .jpg, IMG/left_ 2016 _ 12 _ 01 _ 13 _ 31 _ 12 _ 937 .jpg, IMG/right_ 2016 _ 12 _ 01 _ 13 _ 31 _ 12 _ 937 .jpg, 0 , 0 , 0 , 1.453011 IMG/center_ 2016 _ 12 _ 01 _ 13 _ 31 _ 13 _ 037 .jpg, IMG/left_ 2016 _ 12 _ 01 _ 13 _ 31 _ 13 _ 037 .jpg, IMG/right_ 2016 _ 12 _ 01 _ 13 _ 31 _ 13 _ 037 .jpg, 0 , 0 , 0 , 1.438419","title":"driving_log.csv"},{"location":"end2end_emulator/#img","text":"IMG\u30d5\u30a9\u30eb\u30c0\u306b\u306f\u3001\u53f3\u3001\u4e2d\u592e\u3001\u5de6\u306e\u753b\u50cf\u304c\u4fdd\u5b58\u3055\u308c\u3066\u3044\u304d\u307e\u3059\u3002","title":"IMG"},{"location":"end2end_model/","text":"End2End Model\u306e\u4f5c\u6210 \u6559\u5e2b\u30c7\u30fc\u30bf\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u4f5c\u6210 \u578b\u3092float32\u306b\u5b9a\u7fa9\u3059\u308b 1 2 3 4 import numpy as np center_data = np . array ( center_data ) . astype ( 'float32' ) direction_handle = np . array ( direction_handle ) . astype ( 'float32' ) \u5024\u306e\u30b7\u30e3\u30c3\u30d5\u30eb 1 2 3 from sklearn.utils import shuffle center_data , direction_handle = shuffle ( center_data , direction_handle ) \u6559\u5e2b\u30c7\u30fc\u30bf\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u5207\u308a\u51fa\u3057 1 2 3 from sklearn.model_selection import train_test_split train_center , test_center , train_direction , test_direction = train_test_split ( center_data , direction_handle , random_state = 0 , test_size = 0.1 ) Reshape 1 2 3 4 rows = 10 cols = 40 train_center = train_center . reshape ( train_center . shape [ 0 ], rows , cols , 1 ) test_center = test_center . reshape ( test_center . shape [ 0 ], rows , cols , 1 ) Model\u306e\u4f5c\u6210 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from keras.models import * from keras.layers import * from keras.optimizers import Adam def end2end_model (): model = Sequential () model . add ( Lambda ( lambda x : x / 127.5 - 1. , input_shape = ( rows , cols , 1 ))) model . add ( Convolution2D ( 8 , 3 , 3 , init = 'normal' , border_mode = 'valid' )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D (( 2 , 2 ), border_mode = 'valid' )) model . add ( Convolution2D ( 8 , 3 , 3 , init = 'normal' , border_mode = 'valid' )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D (( 2 , 2 ), border_mode = 'valid' )) model . add ( Dropout ( 0.2 )) model . add ( Flatten ()) model . add ( Dense ( 50 )) model . add ( Activation ( 'relu' )) model . add ( Dense ( 1 )) model . summary () return model \u5b66\u7fd2\u306e\u958b\u59cb 1 2 3 4 5 6 7 8 batch = 128 epoc = 10 adam = Adam ( lr = 0.001 , beta_1 = 0.9 , beta_2 = 0.999 , epsilon = 1e-08 , decay = 0.0 ) model = end2end () model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' , metrics = [ 'accuracy' ]) history = model . fit ( train_center , train_direction , batch_size = batch , nb_epoch = epoc , verbose = 1 , validation_data = ( test_center , test_direction )) \u3053\u3053\u307e\u3067\u306e\u30bd\u30fc\u30b9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 import csv from matplotlib import pyplot as plt import cv2 import numpy as np from sklearn.utils import shuffle from sklearn.model_selection import train_test_split from keras.models import * from keras.layers import * from keras.optimizers import Adam def load_data (): folder_path = '' center_img = [] direction_handle = [] f = open ( folder_path + 'driving_log.csv' , 'r' ) reader = csv . reader ( f ) header = next ( reader ) for row in reader : img_name = row [ 0 ] img_path = folder_path + img_name img = plt . imread ( img_path ) center_img . append ( img ) direction_handle . append ( float ( row [ 3 ])) f . close () return ( center_img , direction_handle ) def draw_img ( center_img , direction_handle , pos = 0 ): g_rows = 3 g_cols = 3 size = g_rows * g_cols fig , axs = plt . subplots ( ncols = g_rows , nrows = size / g_rows ) for h in range ( g_rows ): for i in range ( g_cols ): axs [ h ][ i ] . imshow ( center_img [ pos + i * h ], cmap = plt . cm . gray_r ,) axs [ h ][ i ] . set_title ( str ( direction_handle [ pos + h * i ])) plt . show () def process_img ( center_img ): new_center = [] for img in center_img : crop_img = img [ 60 : 140 , 0 : 320 , :] resize_img = cv2 . resize (( cv2 . cvtColor ( crop_img , cv2 . COLOR_RGB2XYZ ))[:,:, 2 ],( 40 , 10 )) new_center . append ( resize_img ) return new_center def grow_data ( center_img , direction_handle , delta ): new_center = [] new_direction = [] for i in range ( len ( center_img )): for j in range ( 3 ): new_center . append ( center_img [ i ]) if j == 0 : new_direction . append ( direction_handle [ i ]) elif j == 1 : new_direction . append ( direction_handle [ i ] + float ( delta )) elif j == 2 : new_direction . append ( direction_handle [ i ] - float ( delta )) return ( new_center , new_direction ) def end2end_model (): model = Sequential () model . add ( Lambda ( lambda x : x / 127.5 - 1. , input_shape = ( rows , cols , 1 ))) model . add ( Convolution2D ( 8 , 3 , 3 , init = 'normal' , border_mode = 'valid' )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D (( 2 , 2 ), border_mode = 'valid' )) model . add ( Convolution2D ( 8 , 3 , 3 , init = 'normal' , border_mode = 'valid' )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D (( 2 , 2 ), border_mode = 'valid' )) model . add ( Dropout ( 0.2 )) model . add ( Flatten ()) model . add ( Dense ( 50 )) model . add ( Activation ( 'relu' )) model . add ( Dense ( 1 )) model . summary () return model if __name__ == '__main__' : ( center_img , direction_handle ) = load_data () delta = 0.2 ( center_img , direction_handle ) = grow_data ( center_img , direction_handle , delta ) center_data = process_img ( center_img ) draw_img ( center_data , direction_handle , 50 ) center_data = np . array ( center_data ) . astype ( 'float32' ) direction_handle = np . array ( direction_handle ) . astype ( 'float32' ) center_data , direction_handle = shuffle ( center_data , direction_handle ) train_center , test_center , train_direction , test_direction = train_test_split ( center_data , direction_handle , random_state = 0 , test_size = 0.1 ) rows = 10 cols = 40 train_center = train_center . reshape ( train_center . shape [ 0 ], rows , cols , 1 ) test_center = test_center . reshape ( test_center . shape [ 0 ], rows , cols , 1 ) batch = 128 epoc = 10 adam = Adam ( lr = 0.001 , beta_1 = 0.9 , beta_2 = 0.999 , epsilon = 1e-08 , decay = 0.0 ) model = end2end () model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' , metrics = [ 'accuracy' ]) history = model . fit ( train_center , train_direction , batch_size = batch , nb_epoch = epoc , verbose = 1 , validation_data = ( test_center , test_direction ))","title":"End2End Model\u306e\u4f5c\u6210"},{"location":"end2end_model/#end2end-model","text":"","title":"End2End Model\u306e\u4f5c\u6210"},{"location":"end2end_model/#_1","text":"\u578b\u3092float32\u306b\u5b9a\u7fa9\u3059\u308b 1 2 3 4 import numpy as np center_data = np . array ( center_data ) . astype ( 'float32' ) direction_handle = np . array ( direction_handle ) . astype ( 'float32' ) \u5024\u306e\u30b7\u30e3\u30c3\u30d5\u30eb 1 2 3 from sklearn.utils import shuffle center_data , direction_handle = shuffle ( center_data , direction_handle ) \u6559\u5e2b\u30c7\u30fc\u30bf\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u5207\u308a\u51fa\u3057 1 2 3 from sklearn.model_selection import train_test_split train_center , test_center , train_direction , test_direction = train_test_split ( center_data , direction_handle , random_state = 0 , test_size = 0.1 ) Reshape 1 2 3 4 rows = 10 cols = 40 train_center = train_center . reshape ( train_center . shape [ 0 ], rows , cols , 1 ) test_center = test_center . reshape ( test_center . shape [ 0 ], rows , cols , 1 )","title":"\u6559\u5e2b\u30c7\u30fc\u30bf\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u4f5c\u6210"},{"location":"end2end_model/#model","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from keras.models import * from keras.layers import * from keras.optimizers import Adam def end2end_model (): model = Sequential () model . add ( Lambda ( lambda x : x / 127.5 - 1. , input_shape = ( rows , cols , 1 ))) model . add ( Convolution2D ( 8 , 3 , 3 , init = 'normal' , border_mode = 'valid' )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D (( 2 , 2 ), border_mode = 'valid' )) model . add ( Convolution2D ( 8 , 3 , 3 , init = 'normal' , border_mode = 'valid' )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D (( 2 , 2 ), border_mode = 'valid' )) model . add ( Dropout ( 0.2 )) model . add ( Flatten ()) model . add ( Dense ( 50 )) model . add ( Activation ( 'relu' )) model . add ( Dense ( 1 )) model . summary () return model","title":"Model\u306e\u4f5c\u6210"},{"location":"end2end_model/#_2","text":"1 2 3 4 5 6 7 8 batch = 128 epoc = 10 adam = Adam ( lr = 0.001 , beta_1 = 0.9 , beta_2 = 0.999 , epsilon = 1e-08 , decay = 0.0 ) model = end2end () model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' , metrics = [ 'accuracy' ]) history = model . fit ( train_center , train_direction , batch_size = batch , nb_epoch = epoc , verbose = 1 , validation_data = ( test_center , test_direction ))","title":"\u5b66\u7fd2\u306e\u958b\u59cb"},{"location":"end2end_model/#_3","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 import csv from matplotlib import pyplot as plt import cv2 import numpy as np from sklearn.utils import shuffle from sklearn.model_selection import train_test_split from keras.models import * from keras.layers import * from keras.optimizers import Adam def load_data (): folder_path = '' center_img = [] direction_handle = [] f = open ( folder_path + 'driving_log.csv' , 'r' ) reader = csv . reader ( f ) header = next ( reader ) for row in reader : img_name = row [ 0 ] img_path = folder_path + img_name img = plt . imread ( img_path ) center_img . append ( img ) direction_handle . append ( float ( row [ 3 ])) f . close () return ( center_img , direction_handle ) def draw_img ( center_img , direction_handle , pos = 0 ): g_rows = 3 g_cols = 3 size = g_rows * g_cols fig , axs = plt . subplots ( ncols = g_rows , nrows = size / g_rows ) for h in range ( g_rows ): for i in range ( g_cols ): axs [ h ][ i ] . imshow ( center_img [ pos + i * h ], cmap = plt . cm . gray_r ,) axs [ h ][ i ] . set_title ( str ( direction_handle [ pos + h * i ])) plt . show () def process_img ( center_img ): new_center = [] for img in center_img : crop_img = img [ 60 : 140 , 0 : 320 , :] resize_img = cv2 . resize (( cv2 . cvtColor ( crop_img , cv2 . COLOR_RGB2XYZ ))[:,:, 2 ],( 40 , 10 )) new_center . append ( resize_img ) return new_center def grow_data ( center_img , direction_handle , delta ): new_center = [] new_direction = [] for i in range ( len ( center_img )): for j in range ( 3 ): new_center . append ( center_img [ i ]) if j == 0 : new_direction . append ( direction_handle [ i ]) elif j == 1 : new_direction . append ( direction_handle [ i ] + float ( delta )) elif j == 2 : new_direction . append ( direction_handle [ i ] - float ( delta )) return ( new_center , new_direction ) def end2end_model (): model = Sequential () model . add ( Lambda ( lambda x : x / 127.5 - 1. , input_shape = ( rows , cols , 1 ))) model . add ( Convolution2D ( 8 , 3 , 3 , init = 'normal' , border_mode = 'valid' )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D (( 2 , 2 ), border_mode = 'valid' )) model . add ( Convolution2D ( 8 , 3 , 3 , init = 'normal' , border_mode = 'valid' )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D (( 2 , 2 ), border_mode = 'valid' )) model . add ( Dropout ( 0.2 )) model . add ( Flatten ()) model . add ( Dense ( 50 )) model . add ( Activation ( 'relu' )) model . add ( Dense ( 1 )) model . summary () return model if __name__ == '__main__' : ( center_img , direction_handle ) = load_data () delta = 0.2 ( center_img , direction_handle ) = grow_data ( center_img , direction_handle , delta ) center_data = process_img ( center_img ) draw_img ( center_data , direction_handle , 50 ) center_data = np . array ( center_data ) . astype ( 'float32' ) direction_handle = np . array ( direction_handle ) . astype ( 'float32' ) center_data , direction_handle = shuffle ( center_data , direction_handle ) train_center , test_center , train_direction , test_direction = train_test_split ( center_data , direction_handle , random_state = 0 , test_size = 0.1 ) rows = 10 cols = 40 train_center = train_center . reshape ( train_center . shape [ 0 ], rows , cols , 1 ) test_center = test_center . reshape ( test_center . shape [ 0 ], rows , cols , 1 ) batch = 128 epoc = 10 adam = Adam ( lr = 0.001 , beta_1 = 0.9 , beta_2 = 0.999 , epsilon = 1e-08 , decay = 0.0 ) model = end2end () model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' , metrics = [ 'accuracy' ]) history = model . fit ( train_center , train_direction , batch_size = batch , nb_epoch = epoc , verbose = 1 , validation_data = ( test_center , test_direction ))","title":"\u3053\u3053\u307e\u3067\u306e\u30bd\u30fc\u30b9"},{"location":"end2end_run/","text":"Local\u74b0\u5883\u306e\u69cb\u7bc9(MAC) 1 2 3 4 5 6 pip install opencv-python pip install socketio pip install Image pip install keras pip install Flask pip install tensorflow \u5b66\u7fd2\u6e08\u307fModel\u306eDownload \u5b9f\u884c\u30bd\u30fc\u30b9 https://github.com/ymshao/End-to-End-Learning-for-Self-Driving-Cars/blob/master/drive.py \u3088\u308a drive.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 import argparse import base64 import json import cv2 import numpy as np import socketio import eventlet import eventlet.wsgi import time from PIL import Image from PIL import ImageOps from flask import Flask , render_template from io import BytesIO from keras.models import model_from_json from keras.preprocessing.image import ImageDataGenerator , array_to_img , img_to_array # Fix error with Keras and TensorFlow import tensorflow as tf tf . python . control_flow_ops = tf sio = socketio . Server () app = Flask ( __name__ ) model = None prev_image_array = None @sio.on ( 'telemetry' ) def telemetry ( sid , data ): # The current steering angle of the car steering_angle = data [ \"steering_angle\" ] # The current throttle of the car throttle = data [ \"throttle\" ] # The current speed of the car speed = data [ \"speed\" ] # The current image from the center camera of the car imgString = data [ \"image\" ] image = Image . open ( BytesIO ( base64 . b64decode ( imgString ))) image_array = np . asarray ( image ) transformed_image_array = image_array [ None , :, :, :] #resize the image transformed_image_array = ( cv2 . resize (( cv2 . cvtColor ( transformed_image_array [ 0 ], cv2 . COLOR_RGB2HSV ))[:,:, 1 ],( 32 , 16 ))) . reshape ( 1 , 16 , 32 , 1 ) # This model currently assumes that the features of the model are just the images. Feel free to change this. steering_angle = float ( model . predict ( transformed_image_array , batch_size = 1 )) # The driving model currently just outputs a constant throttle. Feel free to edit this. throttle = 0.2 #adaptive speed ''' if (float(speed) < 10): throttle = 0.4 else: # When speed is below 20 then increase throttle by speed_factor if ((float(speed)) < 25): speed_factor = 1.35 else: speed_factor = 1.0 if (abs(steering_angle) < 0.1): throttle = 0.3 * speed_factor elif (abs(steering_angle) < 0.5): throttle = 0.2 * speed_factor else: throttle = 0.15 * speed_factor ''' print ( 'Steering angle =' , ' %5.2f ' % ( float ( steering_angle )), 'Throttle =' , ' %.2f ' % ( float ( throttle )), 'Speed =' , ' %.2f ' % ( float ( speed ))) send_control ( steering_angle , throttle ) @sio.on ( 'connect' ) def connect ( sid , environ ): print ( \"connect \" , sid ) send_control ( 0 , 0 ) def send_control ( steering_angle , throttle ): sio . emit ( \"steer\" , data = { 'steering_angle' : steering_angle . __str__ (), 'throttle' : throttle . __str__ () }, skip_sid = True ) if __name__ == '__main__' : parser = argparse . ArgumentParser ( description = 'Remote Driving' ) parser . add_argument ( 'model' , type = str , help = 'Path to model definition json. Model weights should be on the same path.' ) args = parser . parse_args () with open ( args . model , 'r' ) as jfile : # NOTE: if you saved the file by calling json.dump(model.to_json(), ...) # then you will have to call: # # model = model_from_json(json.loads(jfile.read()))\\ # # instead. model = model_from_json ( jfile . read ()) model . compile ( \"adam\" , \"mse\" ) weights_file = args . model . replace ( 'json' , 'h5' ) model . load_weights ( weights_file ) # wrap Flask application with engineio's middleware app = socketio . Middleware ( sio , app ) # deploy as an eventlet WSGI server eventlet . wsgi . server ( eventlet . listen (( '' , 4567 )), app ) \u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u30fc\u306e\u8d77\u52d5 \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u5b9f\u884c python drive.py model.json","title":"End2end run"},{"location":"end2end_run/#localmac","text":"1 2 3 4 5 6 pip install opencv-python pip install socketio pip install Image pip install keras pip install Flask pip install tensorflow","title":"Local\u74b0\u5883\u306e\u69cb\u7bc9(MAC)"},{"location":"end2end_run/#modeldownload","text":"","title":"\u5b66\u7fd2\u6e08\u307fModel\u306eDownload"},{"location":"end2end_run/#_1","text":"https://github.com/ymshao/End-to-End-Learning-for-Self-Driving-Cars/blob/master/drive.py \u3088\u308a drive.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 import argparse import base64 import json import cv2 import numpy as np import socketio import eventlet import eventlet.wsgi import time from PIL import Image from PIL import ImageOps from flask import Flask , render_template from io import BytesIO from keras.models import model_from_json from keras.preprocessing.image import ImageDataGenerator , array_to_img , img_to_array # Fix error with Keras and TensorFlow import tensorflow as tf tf . python . control_flow_ops = tf sio = socketio . Server () app = Flask ( __name__ ) model = None prev_image_array = None @sio.on ( 'telemetry' ) def telemetry ( sid , data ): # The current steering angle of the car steering_angle = data [ \"steering_angle\" ] # The current throttle of the car throttle = data [ \"throttle\" ] # The current speed of the car speed = data [ \"speed\" ] # The current image from the center camera of the car imgString = data [ \"image\" ] image = Image . open ( BytesIO ( base64 . b64decode ( imgString ))) image_array = np . asarray ( image ) transformed_image_array = image_array [ None , :, :, :] #resize the image transformed_image_array = ( cv2 . resize (( cv2 . cvtColor ( transformed_image_array [ 0 ], cv2 . COLOR_RGB2HSV ))[:,:, 1 ],( 32 , 16 ))) . reshape ( 1 , 16 , 32 , 1 ) # This model currently assumes that the features of the model are just the images. Feel free to change this. steering_angle = float ( model . predict ( transformed_image_array , batch_size = 1 )) # The driving model currently just outputs a constant throttle. Feel free to edit this. throttle = 0.2 #adaptive speed ''' if (float(speed) < 10): throttle = 0.4 else: # When speed is below 20 then increase throttle by speed_factor if ((float(speed)) < 25): speed_factor = 1.35 else: speed_factor = 1.0 if (abs(steering_angle) < 0.1): throttle = 0.3 * speed_factor elif (abs(steering_angle) < 0.5): throttle = 0.2 * speed_factor else: throttle = 0.15 * speed_factor ''' print ( 'Steering angle =' , ' %5.2f ' % ( float ( steering_angle )), 'Throttle =' , ' %.2f ' % ( float ( throttle )), 'Speed =' , ' %.2f ' % ( float ( speed ))) send_control ( steering_angle , throttle ) @sio.on ( 'connect' ) def connect ( sid , environ ): print ( \"connect \" , sid ) send_control ( 0 , 0 ) def send_control ( steering_angle , throttle ): sio . emit ( \"steer\" , data = { 'steering_angle' : steering_angle . __str__ (), 'throttle' : throttle . __str__ () }, skip_sid = True ) if __name__ == '__main__' : parser = argparse . ArgumentParser ( description = 'Remote Driving' ) parser . add_argument ( 'model' , type = str , help = 'Path to model definition json. Model weights should be on the same path.' ) args = parser . parse_args () with open ( args . model , 'r' ) as jfile : # NOTE: if you saved the file by calling json.dump(model.to_json(), ...) # then you will have to call: # # model = model_from_json(json.loads(jfile.read()))\\ # # instead. model = model_from_json ( jfile . read ()) model . compile ( \"adam\" , \"mse\" ) weights_file = args . model . replace ( 'json' , 'h5' ) model . load_weights ( weights_file ) # wrap Flask application with engineio's middleware app = socketio . Middleware ( sio , app ) # deploy as an eventlet WSGI server eventlet . wsgi . server ( eventlet . listen (( '' , 4567 )), app )","title":"\u5b9f\u884c\u30bd\u30fc\u30b9"},{"location":"end2end_run/#_2","text":"","title":"\u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u30fc\u306e\u8d77\u52d5"},{"location":"end2end_run/#_3","text":"python drive.py model.json","title":"\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u5b9f\u884c"},{"location":"end2end_save/","text":"End2End Model\u306e\u4f5c\u6210 \u30e2\u30c7\u30eb\u3068\u5b66\u7fd2\u6e08\u307f\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58 1 2 3 4 5 def model_save ( model_json , model_h5 ): json_model = model . to_json () with open ( model_json , \"w\" ) as f : f . write ( json_model ) model . save_weights ( model_h5 ) model_save(\"model.json\", \"model.h5\") \u3053\u3053\u307e\u3067\u306e\u30bd\u30fc\u30b9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 import csv from matplotlib import pyplot as plt import cv2 import numpy as np from sklearn.utils import shuffle from sklearn.model_selection import train_test_split from keras.models import * from keras.layers import * from keras.optimizers import Adam def load_data (): folder_path = '' center_img = [] direction_handle = [] f = open ( folder_path + 'driving_log.csv' , 'r' ) reader = csv . reader ( f ) header = next ( reader ) for row in reader : img_name = row [ 0 ] img_path = folder_path + img_name img = plt . imread ( img_path ) center_img . append ( img ) direction_handle . append ( float ( row [ 3 ])) f . close () return ( center_img , direction_handle ) def draw_img ( center_img , direction_handle , pos = 0 ): g_rows = 3 g_cols = 3 size = g_rows * g_cols fig , axs = plt . subplots ( ncols = g_rows , nrows = size / g_rows ) for h in range ( g_rows ): for i in range ( g_cols ): axs [ h ][ i ] . imshow ( center_img [ pos + i * h ], cmap = plt . cm . gray_r ,) axs [ h ][ i ] . set_title ( str ( direction_handle [ pos + h * i ])) plt . show () def process_img ( center_img ): new_center = [] for img in center_img : crop_img = img [ 60 : 140 , 0 : 320 , :] resize_img = cv2 . resize (( cv2 . cvtColor ( crop_img , cv2 . COLOR_RGB2XYZ ))[:,:, 2 ],( 40 , 10 )) new_center . append ( resize_img ) return new_center def grow_data ( center_img , direction_handle , delta ): new_center = [] new_direction = [] for i in range ( len ( center_img )): for j in range ( 3 ): new_center . append ( center_img [ i ]) if j == 0 : new_direction . append ( direction_handle [ i ]) elif j == 1 : new_direction . append ( direction_handle [ i ] + float ( delta )) elif j == 2 : new_direction . append ( direction_handle [ i ] - float ( delta )) return ( new_center , new_direction ) def end2end_model (): model = Sequential () model . add ( Lambda ( lambda x : x / 127.5 - 1. , input_shape = ( rows , cols , 1 ))) model . add ( Convolution2D ( 8 , 3 , 3 , init = 'normal' , border_mode = 'valid' )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D (( 2 , 2 ), border_mode = 'valid' )) model . add ( Convolution2D ( 8 , 3 , 3 , init = 'normal' , border_mode = 'valid' )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D (( 2 , 2 ), border_mode = 'valid' )) model . add ( Dropout ( 0.2 )) model . add ( Flatten ()) model . add ( Dense ( 50 )) model . add ( Activation ( 'relu' )) model . add ( Dense ( 1 )) model . summary () return model def model_save ( model_json , model_h5 ): json_model = model . to_json () with open ( model_json , \"w\" ) as f : f . write ( json_model ) model . save_weights ( model_h5 ) if __name__ == '__main__' : ( center_img , direction_handle ) = load_data () delta = 0.2 ( center_img , direction_handle ) = grow_data ( center_img , direction_handle , delta ) center_data = process_img ( center_img ) draw_img ( center_data , direction_handle , 50 ) center_data = np . array ( center_data ) . astype ( 'float32' ) direction_handle = np . array ( direction_handle ) . astype ( 'float32' ) center_data , direction_handle = shuffle ( center_data , direction_handle ) train_center , test_center , train_direction , test_direction = train_test_split ( center_data , direction_handle , random_state = 0 , test_size = 0.1 ) rows = 10 cols = 40 train_center = train_center . reshape ( train_center . shape [ 0 ], rows , cols , 1 ) test_center = test_center . reshape ( test_center . shape [ 0 ], rows , cols , 1 ) batch = 128 epoc = 10 adam = Adam ( lr = 0.001 , beta_1 = 0.9 , beta_2 = 0.999 , epsilon = 1e-08 , decay = 0.0 ) model = end2end () model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' , metrics = [ 'accuracy' ]) history = model . fit ( train_center , train_direction , batch_size = batch , nb_epoch = epoc , verbose = 1 , validation_data = ( test_center , test_direction )) model_save ( \"./model.json\" , \"./model.h5\" )","title":"End2End Model\u306e\u4f5c\u6210"},{"location":"end2end_save/#end2end-model","text":"","title":"End2End Model\u306e\u4f5c\u6210"},{"location":"end2end_save/#_1","text":"1 2 3 4 5 def model_save ( model_json , model_h5 ): json_model = model . to_json () with open ( model_json , \"w\" ) as f : f . write ( json_model ) model . save_weights ( model_h5 ) model_save(\"model.json\", \"model.h5\")","title":"\u30e2\u30c7\u30eb\u3068\u5b66\u7fd2\u6e08\u307f\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58"},{"location":"end2end_save/#_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 import csv from matplotlib import pyplot as plt import cv2 import numpy as np from sklearn.utils import shuffle from sklearn.model_selection import train_test_split from keras.models import * from keras.layers import * from keras.optimizers import Adam def load_data (): folder_path = '' center_img = [] direction_handle = [] f = open ( folder_path + 'driving_log.csv' , 'r' ) reader = csv . reader ( f ) header = next ( reader ) for row in reader : img_name = row [ 0 ] img_path = folder_path + img_name img = plt . imread ( img_path ) center_img . append ( img ) direction_handle . append ( float ( row [ 3 ])) f . close () return ( center_img , direction_handle ) def draw_img ( center_img , direction_handle , pos = 0 ): g_rows = 3 g_cols = 3 size = g_rows * g_cols fig , axs = plt . subplots ( ncols = g_rows , nrows = size / g_rows ) for h in range ( g_rows ): for i in range ( g_cols ): axs [ h ][ i ] . imshow ( center_img [ pos + i * h ], cmap = plt . cm . gray_r ,) axs [ h ][ i ] . set_title ( str ( direction_handle [ pos + h * i ])) plt . show () def process_img ( center_img ): new_center = [] for img in center_img : crop_img = img [ 60 : 140 , 0 : 320 , :] resize_img = cv2 . resize (( cv2 . cvtColor ( crop_img , cv2 . COLOR_RGB2XYZ ))[:,:, 2 ],( 40 , 10 )) new_center . append ( resize_img ) return new_center def grow_data ( center_img , direction_handle , delta ): new_center = [] new_direction = [] for i in range ( len ( center_img )): for j in range ( 3 ): new_center . append ( center_img [ i ]) if j == 0 : new_direction . append ( direction_handle [ i ]) elif j == 1 : new_direction . append ( direction_handle [ i ] + float ( delta )) elif j == 2 : new_direction . append ( direction_handle [ i ] - float ( delta )) return ( new_center , new_direction ) def end2end_model (): model = Sequential () model . add ( Lambda ( lambda x : x / 127.5 - 1. , input_shape = ( rows , cols , 1 ))) model . add ( Convolution2D ( 8 , 3 , 3 , init = 'normal' , border_mode = 'valid' )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D (( 2 , 2 ), border_mode = 'valid' )) model . add ( Convolution2D ( 8 , 3 , 3 , init = 'normal' , border_mode = 'valid' )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D (( 2 , 2 ), border_mode = 'valid' )) model . add ( Dropout ( 0.2 )) model . add ( Flatten ()) model . add ( Dense ( 50 )) model . add ( Activation ( 'relu' )) model . add ( Dense ( 1 )) model . summary () return model def model_save ( model_json , model_h5 ): json_model = model . to_json () with open ( model_json , \"w\" ) as f : f . write ( json_model ) model . save_weights ( model_h5 ) if __name__ == '__main__' : ( center_img , direction_handle ) = load_data () delta = 0.2 ( center_img , direction_handle ) = grow_data ( center_img , direction_handle , delta ) center_data = process_img ( center_img ) draw_img ( center_data , direction_handle , 50 ) center_data = np . array ( center_data ) . astype ( 'float32' ) direction_handle = np . array ( direction_handle ) . astype ( 'float32' ) center_data , direction_handle = shuffle ( center_data , direction_handle ) train_center , test_center , train_direction , test_direction = train_test_split ( center_data , direction_handle , random_state = 0 , test_size = 0.1 ) rows = 10 cols = 40 train_center = train_center . reshape ( train_center . shape [ 0 ], rows , cols , 1 ) test_center = test_center . reshape ( test_center . shape [ 0 ], rows , cols , 1 ) batch = 128 epoc = 10 adam = Adam ( lr = 0.001 , beta_1 = 0.9 , beta_2 = 0.999 , epsilon = 1e-08 , decay = 0.0 ) model = end2end () model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' , metrics = [ 'accuracy' ]) history = model . fit ( train_center , train_direction , batch_size = batch , nb_epoch = epoc , verbose = 1 , validation_data = ( test_center , test_direction )) model_save ( \"./model.json\" , \"./model.h5\" )","title":"\u3053\u3053\u307e\u3067\u306e\u30bd\u30fc\u30b9"},{"location":"gmm_mfcc/","text":"GMM on MFCC \u521d\u671f\u8a55\u4fa1\u30d0\u30fc\u30b8\u30e7\u30f3\u3002 GMM on MFCC \u305d\u308c\u305e\u308c\u306e\u4eba\u7269\u306e30\u79d2\u306e\u97f3\u58f0\u30c7\u30fc\u30bf\u3092\u7528\u610f\u3059\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 from sklearn import preprocessing import python_speech_features as mfcc import librosa import matplotlib.pyplot as plt import librosa.display from sklearn.mixture import GMM import _pickle as pickle import warnings import numpy as np warnings . filterwarnings ( \"ignore\" ) def wav2mfcc ( path ): audio , rate = librosa . load ( path ) mfcc_data = mfcc . mfcc ( audio , rate , 0.025 , 0.01 , 20 , appendEnergy = True ) mfcc_data = preprocessing . scale ( mfcc_data ) return ( mfcc_data , rate ) def drawGraph ( data , rate ): librosa . display . specshow ( data , sr = rate , x_axis = 'time' , y_axis = 'mel' ) plt . title ( 'mel power spectrogram' ) plt . colorbar ( format = ' %02.0f dB' ) plt . tight_layout () names = [ \"hozumi\" , \"okuyama\" , \"takeo\" , \"takano\" ] for name in names : ( mfcc_data , rate ) = wav2mfcc ( name + \".wav\" ) gmm = GMM ( n_components = 16 , n_iter = 200 , covariance_type = 'diag' , n_init = 3 ) gmm . fit ( mfcc_data ) pickle . dump ( gmm , open ( name + \".gmm\" , 'wb' )) models = [] for name in names : model = pickle . load ( open ( name + \".gmm\" , 'rb' )) models . append ( model ) ( test_data , rate ) = wav2mfcc ( \"hozumi_test.wav\" ) total_scores = np . zeros ( len ( names )) for i in range ( len ( models )): gmm = models [ i ] scores = np . array ( gmm . score ( test_data )) total_scores [ i ] = scores . sum () speaker = np . argmax ( total_scores ) print ( \"Speaker is\" , names [ speaker ])","title":"GMM on MFCC"},{"location":"gmm_mfcc/#gmm-on-mfcc","text":"\u521d\u671f\u8a55\u4fa1\u30d0\u30fc\u30b8\u30e7\u30f3\u3002","title":"GMM on MFCC"},{"location":"gmm_mfcc/#gmm-on-mfcc_1","text":"\u305d\u308c\u305e\u308c\u306e\u4eba\u7269\u306e30\u79d2\u306e\u97f3\u58f0\u30c7\u30fc\u30bf\u3092\u7528\u610f\u3059\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 from sklearn import preprocessing import python_speech_features as mfcc import librosa import matplotlib.pyplot as plt import librosa.display from sklearn.mixture import GMM import _pickle as pickle import warnings import numpy as np warnings . filterwarnings ( \"ignore\" ) def wav2mfcc ( path ): audio , rate = librosa . load ( path ) mfcc_data = mfcc . mfcc ( audio , rate , 0.025 , 0.01 , 20 , appendEnergy = True ) mfcc_data = preprocessing . scale ( mfcc_data ) return ( mfcc_data , rate ) def drawGraph ( data , rate ): librosa . display . specshow ( data , sr = rate , x_axis = 'time' , y_axis = 'mel' ) plt . title ( 'mel power spectrogram' ) plt . colorbar ( format = ' %02.0f dB' ) plt . tight_layout () names = [ \"hozumi\" , \"okuyama\" , \"takeo\" , \"takano\" ] for name in names : ( mfcc_data , rate ) = wav2mfcc ( name + \".wav\" ) gmm = GMM ( n_components = 16 , n_iter = 200 , covariance_type = 'diag' , n_init = 3 ) gmm . fit ( mfcc_data ) pickle . dump ( gmm , open ( name + \".gmm\" , 'wb' )) models = [] for name in names : model = pickle . load ( open ( name + \".gmm\" , 'rb' )) models . append ( model ) ( test_data , rate ) = wav2mfcc ( \"hozumi_test.wav\" ) total_scores = np . zeros ( len ( names )) for i in range ( len ( models )): gmm = models [ i ] scores = np . array ( gmm . score ( test_data )) total_scores [ i ] = scores . sum () speaker = np . argmax ( total_scores ) print ( \"Speaker is\" , names [ speaker ])","title":"GMM on MFCC"},{"location":"size/","text":"OpenCV Size\u306e\u5909\u66f4 1 2 3 4 5 6 7 8 9 10 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2BGR ) resize_image = cv2 . resize ( cvt_image ,( 20 , 20 )) plt . imshow ( resize_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"OpenCV"},{"location":"size/#opencv","text":"","title":"OpenCV"},{"location":"size/#size","text":"1 2 3 4 5 6 7 8 9 10 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2BGR ) resize_image = cv2 . resize ( cvt_image ,( 20 , 20 )) plt . imshow ( resize_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"Size\u306e\u5909\u66f4"},{"location":"spectrum/","text":"\u30b9\u30da\u30af\u30c8\u30e9\u30b0\u30e9\u30e0 \u30b0\u30e9\u30d5\u3092\u8868\u793a 1 2 3 4 5 import matplotlib.pyplot as plt import librosa wave , rate = librosa . load ( \"sample.wav\" ) spec = plt . specgram ( wave , Fs = rate ) 1 2 3 4 5 import scipy.io.wavfile as wio import matplotlib.pyplot as plt rate , data = wio . read ( \"sample_solo.wav\" ) spec = plt . specgram ( data , Fs = rate ) \u753b\u50cf\u3092\u4fdd\u5b58 1 2 3 4 5 6 7 import scipy.io.wavfile as wio import matplotlib.pyplot as plt rate , data = wio . read ( \"sample_solo.wav\" ) spec = plt . specgram ( data , Fs = rate ) plt . axis ( 'off' ) plt . savefig ( 'specrtum_solo.png' , bbox_inches = 'tight' , dpi = 50 , frameon = 'false' )","title":"\u30b9\u30da\u30af\u30c8\u30e9\u30b0\u30e9\u30e0"},{"location":"spectrum/#_1","text":"","title":"\u30b9\u30da\u30af\u30c8\u30e9\u30b0\u30e9\u30e0"},{"location":"spectrum/#_2","text":"1 2 3 4 5 import matplotlib.pyplot as plt import librosa wave , rate = librosa . load ( \"sample.wav\" ) spec = plt . specgram ( wave , Fs = rate ) 1 2 3 4 5 import scipy.io.wavfile as wio import matplotlib.pyplot as plt rate , data = wio . read ( \"sample_solo.wav\" ) spec = plt . specgram ( data , Fs = rate )","title":"\u30b0\u30e9\u30d5\u3092\u8868\u793a"},{"location":"spectrum/#_3","text":"1 2 3 4 5 6 7 import scipy.io.wavfile as wio import matplotlib.pyplot as plt rate , data = wio . read ( \"sample_solo.wav\" ) spec = plt . specgram ( data , Fs = rate ) plt . axis ( 'off' ) plt . savefig ( 'specrtum_solo.png' , bbox_inches = 'tight' , dpi = 50 , frameon = 'false' )","title":"\u753b\u50cf\u3092\u4fdd\u5b58"},{"location":"ssd/","text":"SSD Github\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8 Keras 2.0\u3067\u52d5\u4f5c\u3059\u308b\u3088\u3046\u306b\u6539\u826f\u3055\u308c\u305f\u30d7\u30ed\u30b8\u30a7\u30af\u30c8 https://github.com/SnowMasaya/ssd_keras Dataset http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar weights_SSD300.hdf5 https://mega.nz/#F!7RowVLCL!q3cEVRK9jyOSB9el3SssIA \u81ea\u4f5c\u306e\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8 \u4e0b\u8a18\u30c4\u30fc\u30eb\u3067\u30e1\u30bf\u753b\u50cf\u306e\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 https://github.com/tzutalin/ImageNet_Utils \u4e0b\u8a18Script\u3067\u3001pkl\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3059\u308b https://github.com/rykov8/ssd_keras/blob/master/PASCAL_VOC/get_data_from_XML.py SSD_training \u4e0b\u8a18\u3092\u4fee\u6b63 from ssd import SSD300 -> from ssd_v2 import SSD300v2 model = SSD300(input_shape, num_classes=NUM_CLASSES) -> model = SSD300v2(input_shape, num_classes=NUM_CLASSES)","title":"SSD"},{"location":"ssd/#ssd","text":"","title":"SSD"},{"location":"ssd/#github","text":"Keras 2.0\u3067\u52d5\u4f5c\u3059\u308b\u3088\u3046\u306b\u6539\u826f\u3055\u308c\u305f\u30d7\u30ed\u30b8\u30a7\u30af\u30c8 https://github.com/SnowMasaya/ssd_keras","title":"Github\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8"},{"location":"ssd/#dataset","text":"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar","title":"Dataset"},{"location":"ssd/#weights_ssd300hdf5","text":"https://mega.nz/#F!7RowVLCL!q3cEVRK9jyOSB9el3SssIA","title":"weights_SSD300.hdf5"},{"location":"ssd/#_1","text":"\u4e0b\u8a18\u30c4\u30fc\u30eb\u3067\u30e1\u30bf\u753b\u50cf\u306e\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 https://github.com/tzutalin/ImageNet_Utils \u4e0b\u8a18Script\u3067\u3001pkl\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3059\u308b https://github.com/rykov8/ssd_keras/blob/master/PASCAL_VOC/get_data_from_XML.py","title":"\u81ea\u4f5c\u306e\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8"},{"location":"ssd/#ssd_training","text":"\u4e0b\u8a18\u3092\u4fee\u6b63 from ssd import SSD300 -> from ssd_v2 import SSD300v2 model = SSD300(input_shape, num_classes=NUM_CLASSES) -> model = SSD300v2(input_shape, num_classes=NUM_CLASSES)","title":"SSD_training"},{"location":"ssd_mobilenet_v1_0.75_depth_coco/","text":"Git clone\u76f4\u5f8c\u306e\u5834\u5408\u3000 COCO API\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Shell 1 2 3 4 $ git clone https://github.com/cocodataset/cocoapi.git $ cd cocoapi/PythonAPI $ make $ cp -r pycocotools ../../models/research/ Shell 1 2 3 $ cd models/reasearch $ protoc object_detection/protos/*.proto --python_out = . docker\u5185\u3067\u5b9f\u884c Shell 1 $ export PYTHONPATH = $PYTHONPATH : ` pwd ` : ` pwd ` /slim \u30e2\u30c7\u30eb 1 2 3 4 $ cd /tmp $ curl -O http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03.tar.gz $ tar xzf ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03.tar.gz $ gsutil cp /tmp/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03/model.ckpt.* gs:// ${ YOUR_GCS_BUCKET } /data/ TPU\u3067\u5b66\u7fd2 Shell 1 2 $ vim object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync.config $ gsutil cp object_detection/samples/configs/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync.config gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config Shell 1 gcloud ml-engine jobs submit training ` whoami ` _object_detection_ ` date +%s ` --job-dir = gs:// ${ YOUR_GCS_BUCKET } /train_v1 --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz --module-name object_detection.model_tpu_main --runtime-version 1 .8 --scale-tier BASIC_TPU --region us-central1 -- --model_dir = gs:// ${ YOUR_GCS_BUCKET } /train_v1 --tpu_zone us-central1 --pipeline_config_path = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config \u30a8\u30e9\u30fc 1 2 master-replica-0 Traceback ( most recent call last ) : File \"/usr/lib/python2.7/runpy.py\" , line 174 , in _run_module_as_main \"__main__\" , fname, loader, pkg_name ) File \"/usr/lib/python2.7/runpy.py\" , line 72 , in _run_code exec code in run_globals File \"/root/.local/lib/python2.7/site-packages/object_detection/model_tpu_main.py\" , line 134 , in <module> tf.app.run () File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\" , line 126 , in run _sys.exit ( main ( argv )) File \"/root/.local/lib/python2.7/site-packages/object_detection/model_tpu_main.py\" , line 119 , in main estimator.train ( input_fn = train_input_fn, max_steps = train_steps ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 363 , in train loss = self._train_model ( input_fn, hooks, saving_listeners ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 843 , in _train_model return self._train_model_default ( input_fn, hooks, saving_listeners ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 856 , in _train_model_default features, labels, model_fn_lib.ModeKeys.TRAIN, self.config ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 831 , in _call_model_fn model_fn_results = self._model_fn ( features = features, **kwargs ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2036 , in _model_fn _train_on_tpu_system ( ctx, model_fn_wrapper, dequeue_fn )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2244 , in _train_on_tpu_system device_assignment = ctx.device_assignment ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\" , line 690 , in shard name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\" , line 516 , in replicate outputs = computation ( *computation_inputs ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2237 , in multi_tpu_train_steps_on_single_shard single_tpu_train_step, [ _INITIAL_LOSS ]) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 207 , in repeat cond, body_wrapper, inputs = inputs, infeed_queue = infeed_queue, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 169 , in while_loop name = \"\" ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 3224 , in while_loop result = loop_context.BuildLoop ( cond, body, loop_vars, shape_invariants ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 2956 , in BuildLoop pred, body, original_loop_vars, loop_vars, shape_invariants ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 2893 , in _BuildLoop body_result = body ( *packed_vars_for_body ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 120 , in body_wrapper outputs = body ( * ( inputs + dequeue_ops )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 203 , in body_wrapper return [ i + 1 ] + _convert_to_list ( body ( *args )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 1156 , in train_step self._call_model_fn ( features, labels )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 1317 , in _call_model_fn estimator_spec = self._model_fn ( features = features, **kwargs ) File \"/root/.local/lib/python2.7/site-packages/object_detection/model_lib.py\" , line 252 , in model_fn preprocessed_images, features [ fields.InputDataFields.true_image_shape ]) File \"/root/.local/lib/python2.7/site-packages/object_detection/meta_architectures/ssd_meta_arch.py\" , line 514 , in predict preprocessed_inputs ) File \"/root/.local/lib/python2.7/site-packages/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py\" , line 146 , in extract_features depth = depth_fn ( 256 )) File \"/root/.local/lib/python2.7/site-packages/object_detection/models/feature_map_generators.py\" , line 218 , in fpn_top_down_feature_maps top_down += residual File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\" , line 979 , in binary_op_wrapper return func ( x, y, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\" , line 297 , in add \"Add\" , x = x, y = y, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\" , line 787 , in _apply_op_helper op_def = op_def ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 3392 , in create_op op_def = op_def ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 1734 , in __init__ control_input_ops ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 1570 , in _create_c_op raise ValueError ( str ( e )) ValueError: Dimensions must be equal, but are 20 and 19 for 'FeatureExtractor/MobilenetV1/fpn/top_down/add' ( op: 'Add' ) with input shapes: [ 8 ,20,20,256 ] , [ 8 ,19,19,256 ] . \u53d6\u5f97 Shell 1 2 3 $ export CONFIG_FILE = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config $ export CHECKPOINT_PATH = gs:// ${ YOUR_GCS_BUCKET } /train_resnet/model.ckpt-2000 $ export OUTPUT_DIR = /tmp/tflite Shell 1 $ python object_detection/export_tflite_ssd_graph.py --pipeline_config_path = $CONFIG_FILE --trained_checkpoint_prefix = $CHECKPOINT_PATH --output_directory = $OUTPUT_DIR --add_postprocessing_op = true toco 1 bazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6 \u5f8c\u51e6\u7406 1 $ git reset --hard","title":"Ssd mobilenet v1 0.75 depth coco"},{"location":"ssd_mobilenet_v1_0.75_depth_coco/#_1","text":"1 2 3 4 $ cd /tmp $ curl -O http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03.tar.gz $ tar xzf ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03.tar.gz $ gsutil cp /tmp/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03/model.ckpt.* gs:// ${ YOUR_GCS_BUCKET } /data/","title":"\u30e2\u30c7\u30eb"},{"location":"ssd_mobilenet_v1_0.75_depth_coco/#tpu","text":"Shell 1 2 $ vim object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync.config $ gsutil cp object_detection/samples/configs/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync.config gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config Shell 1 gcloud ml-engine jobs submit training ` whoami ` _object_detection_ ` date +%s ` --job-dir = gs:// ${ YOUR_GCS_BUCKET } /train_v1 --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz --module-name object_detection.model_tpu_main --runtime-version 1 .8 --scale-tier BASIC_TPU --region us-central1 -- --model_dir = gs:// ${ YOUR_GCS_BUCKET } /train_v1 --tpu_zone us-central1 --pipeline_config_path = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config","title":"TPU\u3067\u5b66\u7fd2"},{"location":"ssd_mobilenet_v1_0.75_depth_coco/#_2","text":"1 2 master-replica-0 Traceback ( most recent call last ) : File \"/usr/lib/python2.7/runpy.py\" , line 174 , in _run_module_as_main \"__main__\" , fname, loader, pkg_name ) File \"/usr/lib/python2.7/runpy.py\" , line 72 , in _run_code exec code in run_globals File \"/root/.local/lib/python2.7/site-packages/object_detection/model_tpu_main.py\" , line 134 , in <module> tf.app.run () File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\" , line 126 , in run _sys.exit ( main ( argv )) File \"/root/.local/lib/python2.7/site-packages/object_detection/model_tpu_main.py\" , line 119 , in main estimator.train ( input_fn = train_input_fn, max_steps = train_steps ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 363 , in train loss = self._train_model ( input_fn, hooks, saving_listeners ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 843 , in _train_model return self._train_model_default ( input_fn, hooks, saving_listeners ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 856 , in _train_model_default features, labels, model_fn_lib.ModeKeys.TRAIN, self.config ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 831 , in _call_model_fn model_fn_results = self._model_fn ( features = features, **kwargs ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2036 , in _model_fn _train_on_tpu_system ( ctx, model_fn_wrapper, dequeue_fn )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2244 , in _train_on_tpu_system device_assignment = ctx.device_assignment ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\" , line 690 , in shard name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\" , line 516 , in replicate outputs = computation ( *computation_inputs ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2237 , in multi_tpu_train_steps_on_single_shard single_tpu_train_step, [ _INITIAL_LOSS ]) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 207 , in repeat cond, body_wrapper, inputs = inputs, infeed_queue = infeed_queue, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 169 , in while_loop name = \"\" ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 3224 , in while_loop result = loop_context.BuildLoop ( cond, body, loop_vars, shape_invariants ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 2956 , in BuildLoop pred, body, original_loop_vars, loop_vars, shape_invariants ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 2893 , in _BuildLoop body_result = body ( *packed_vars_for_body ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 120 , in body_wrapper outputs = body ( * ( inputs + dequeue_ops )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 203 , in body_wrapper return [ i + 1 ] + _convert_to_list ( body ( *args )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 1156 , in train_step self._call_model_fn ( features, labels )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 1317 , in _call_model_fn estimator_spec = self._model_fn ( features = features, **kwargs ) File \"/root/.local/lib/python2.7/site-packages/object_detection/model_lib.py\" , line 252 , in model_fn preprocessed_images, features [ fields.InputDataFields.true_image_shape ]) File \"/root/.local/lib/python2.7/site-packages/object_detection/meta_architectures/ssd_meta_arch.py\" , line 514 , in predict preprocessed_inputs ) File \"/root/.local/lib/python2.7/site-packages/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py\" , line 146 , in extract_features depth = depth_fn ( 256 )) File \"/root/.local/lib/python2.7/site-packages/object_detection/models/feature_map_generators.py\" , line 218 , in fpn_top_down_feature_maps top_down += residual File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\" , line 979 , in binary_op_wrapper return func ( x, y, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\" , line 297 , in add \"Add\" , x = x, y = y, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\" , line 787 , in _apply_op_helper op_def = op_def ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 3392 , in create_op op_def = op_def ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 1734 , in __init__ control_input_ops ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 1570 , in _create_c_op raise ValueError ( str ( e )) ValueError: Dimensions must be equal, but are 20 and 19 for 'FeatureExtractor/MobilenetV1/fpn/top_down/add' ( op: 'Add' ) with input shapes: [ 8 ,20,20,256 ] , [ 8 ,19,19,256 ] .","title":"\u30a8\u30e9\u30fc"},{"location":"ssd_mobilenet_v1_0.75_depth_coco/#_3","text":"Shell 1 2 3 $ export CONFIG_FILE = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config $ export CHECKPOINT_PATH = gs:// ${ YOUR_GCS_BUCKET } /train_resnet/model.ckpt-2000 $ export OUTPUT_DIR = /tmp/tflite Shell 1 $ python object_detection/export_tflite_ssd_graph.py --pipeline_config_path = $CONFIG_FILE --trained_checkpoint_prefix = $CHECKPOINT_PATH --output_directory = $OUTPUT_DIR --add_postprocessing_op = true","title":"\u53d6\u5f97"},{"location":"ssd_mobilenet_v1_0.75_depth_coco/#toco","text":"1 bazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6","title":"toco"},{"location":"ssd_mobilenet_v1_0.75_depth_coco/#_4","text":"1 $ git reset --hard","title":"\u5f8c\u51e6\u7406"},{"location":"ssd_mobilenet_v1_fpn_coco/","text":"Git clone\u76f4\u5f8c\u306e\u5834\u5408 COCO API\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Shell 1 2 3 4 $ git clone https://github.com/cocodataset/cocoapi.git $ cd cocoapi/PythonAPI $ make $ cp -r pycocotools ../../models/research/ Shell 1 2 3 $ cd models/reasearch $ protoc object_detection/protos/*.proto --python_out = . docker\u5185\u3067\u5b9f\u884c Shell 1 $ export PYTHONPATH = $PYTHONPATH : ` pwd ` : ` pwd ` /slim \u30e2\u30c7\u30eb Shell 1 2 3 4 5 6 7 8 9 10 $ cd /tmp $ curl -O http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz $ tar xzf ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz $ gsutil cp /tmp/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/model.ckpt.* gs:// ${ YOUR_GCS_BUCKET } /data/ ``` # TPU\u3067\u5b66\u7fd2 ``` shell $ vim object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config $ gsutil cp object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config Shell 1 $ gcloud ml-engine jobs submit training ` whoami ` _object_detection_ ` date +%s ` --job-dir = gs:// ${ YOUR_GCS_BUCKET } /train_fpn --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz --module-name object_detection.model_tpu_main --runtime-version 1 .8 --scale-tier BASIC_TPU --region us-central1 -- --model_dir = gs:// ${ YOUR_GCS_BUCKET } /train_fpn --tpu_zone us-central1 --pipeline_config_path = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config \u30a8\u30e9\u30fc 1 2 master-replica-0 Traceback ( most recent call last ) : File \"/usr/lib/python2.7/runpy.py\" , line 174 , in _run_module_as_main \"__main__\" , fname, loader, pkg_name ) File \"/usr/lib/python2.7/runpy.py\" , line 72 , in _run_code exec code in run_globals File \"/root/.local/lib/python2.7/site-packages/object_detection/model_tpu_main.py\" , line 134 , in <module> tf.app.run () File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\" , line 126 , in run _sys.exit ( main ( argv )) File \"/root/.local/lib/python2.7/site-packages/object_detection/model_tpu_main.py\" , line 119 , in main estimator.train ( input_fn = train_input_fn, max_steps = train_steps ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 363 , in train loss = self._train_model ( input_fn, hooks, saving_listeners ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 843 , in _train_model return self._train_model_default ( input_fn, hooks, saving_listeners ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 856 , in _train_model_default features, labels, model_fn_lib.ModeKeys.TRAIN, self.config ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 831 , in _call_model_fn model_fn_results = self._model_fn ( features = features, **kwargs ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2036 , in _model_fn _train_on_tpu_system ( ctx, model_fn_wrapper, dequeue_fn )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2244 , in _train_on_tpu_system device_assignment = ctx.device_assignment ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\" , line 690 , in shard name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\" , line 516 , in replicate outputs = computation ( *computation_inputs ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2237 , in multi_tpu_train_steps_on_single_shard single_tpu_train_step, [ _INITIAL_LOSS ]) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 207 , in repeat cond, body_wrapper, inputs = inputs, infeed_queue = infeed_queue, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 169 , in while_loop name = \"\" ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 3224 , in while_loop result = loop_context.BuildLoop ( cond, body, loop_vars, shape_invariants ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 2956 , in BuildLoop pred, body, original_loop_vars, loop_vars, shape_invariants ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 2893 , in _BuildLoop body_result = body ( *packed_vars_for_body ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 120 , in body_wrapper outputs = body ( * ( inputs + dequeue_ops )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 203 , in body_wrapper return [ i + 1 ] + _convert_to_list ( body ( *args )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 1156 , in train_step self._call_model_fn ( features, labels )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 1317 , in _call_model_fn estimator_spec = self._model_fn ( features = features, **kwargs ) File \"/root/.local/lib/python2.7/site-packages/object_detection/model_lib.py\" , line 252 , in model_fn preprocessed_images, features [ fields.InputDataFields.true_image_shape ]) File \"/root/.local/lib/python2.7/site-packages/object_detection/meta_architectures/ssd_meta_arch.py\" , line 514 , in predict preprocessed_inputs ) File \"/root/.local/lib/python2.7/site-packages/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py\" , line 146 , in extract_features depth = depth_fn ( 256 )) File \"/root/.local/lib/python2.7/site-packages/object_detection/models/feature_map_generators.py\" , line 218 , in fpn_top_down_feature_maps top_down += residual File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\" , line 979 , in binary_op_wrapper return func ( x, y, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\" , line 297 , in add \"Add\" , x = x, y = y, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\" , line 787 , in _apply_op_helper op_def = op_def ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 3392 , in create_op op_def = op_def ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 1734 , in __init__ control_input_ops ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 1570 , in _create_c_op raise ValueError ( str ( e )) ValueError: Dimensions must be equal, but are 20 and 19 for 'FeatureExtractor/MobilenetV1/fpn/top_down/add' ( op: 'Add' ) with input shapes: [ 8 ,20,20,256 ] , [ 8 ,19,19,256 ] . \u53d6\u5f97 Shell 1 2 3 $ export CONFIG_FILE = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config $ export CHECKPOINT_PATH = gs:// ${ YOUR_GCS_BUCKET } /train_fpn/model.ckpt-2000 $ export OUTPUT_DIR = /tmp/tflite Shell 1 $ python object_detection/export_tflite_ssd_graph.py --pipeline_config_path = $CONFIG_FILE --trained_checkpoint_prefix = $CHECKPOINT_PATH --output_directory = $OUTPUT_DIR --add_postprocessing_op = true toco 1 $ bazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file = $OUTPUT_DIR /tflite_graph.pb --output_file = $OUTPUT_DIR /detect.tflite --input_shapes = 1 ,300,300,3 --input_arrays = normalized_input_image_tensor --output_arrays = 'TFLite_Detection_PostProcess' , 'TFLite_Detection_PostProcess:1' , 'TFLite_Detection_PostProcess:2' , 'TFLite_Detection_PostProcess:3' --inference_type = QUANTIZED_UINT8 --mean_values = 128 --std_values = 128 --change_concat_input_ranges = false --allow_custom_ops --default_ranges_min = 0 --default_ranges_max = 6 \u5f8c\u51e6\u7406 1 $ git reset --hard","title":"Git clone\u76f4\u5f8c\u306e\u5834\u5408"},{"location":"ssd_mobilenet_v1_fpn_coco/#git-clone","text":"COCO API\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Shell 1 2 3 4 $ git clone https://github.com/cocodataset/cocoapi.git $ cd cocoapi/PythonAPI $ make $ cp -r pycocotools ../../models/research/ Shell 1 2 3 $ cd models/reasearch $ protoc object_detection/protos/*.proto --python_out = . docker\u5185\u3067\u5b9f\u884c Shell 1 $ export PYTHONPATH = $PYTHONPATH : ` pwd ` : ` pwd ` /slim","title":"Git clone\u76f4\u5f8c\u306e\u5834\u5408"},{"location":"ssd_mobilenet_v1_fpn_coco/#_1","text":"Shell 1 2 3 4 5 6 7 8 9 10 $ cd /tmp $ curl -O http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz $ tar xzf ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz $ gsutil cp /tmp/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/model.ckpt.* gs:// ${ YOUR_GCS_BUCKET } /data/ ``` # TPU\u3067\u5b66\u7fd2 ``` shell $ vim object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config $ gsutil cp object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config Shell 1 $ gcloud ml-engine jobs submit training ` whoami ` _object_detection_ ` date +%s ` --job-dir = gs:// ${ YOUR_GCS_BUCKET } /train_fpn --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz --module-name object_detection.model_tpu_main --runtime-version 1 .8 --scale-tier BASIC_TPU --region us-central1 -- --model_dir = gs:// ${ YOUR_GCS_BUCKET } /train_fpn --tpu_zone us-central1 --pipeline_config_path = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config","title":"\u30e2\u30c7\u30eb"},{"location":"ssd_mobilenet_v1_fpn_coco/#_2","text":"1 2 master-replica-0 Traceback ( most recent call last ) : File \"/usr/lib/python2.7/runpy.py\" , line 174 , in _run_module_as_main \"__main__\" , fname, loader, pkg_name ) File \"/usr/lib/python2.7/runpy.py\" , line 72 , in _run_code exec code in run_globals File \"/root/.local/lib/python2.7/site-packages/object_detection/model_tpu_main.py\" , line 134 , in <module> tf.app.run () File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\" , line 126 , in run _sys.exit ( main ( argv )) File \"/root/.local/lib/python2.7/site-packages/object_detection/model_tpu_main.py\" , line 119 , in main estimator.train ( input_fn = train_input_fn, max_steps = train_steps ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 363 , in train loss = self._train_model ( input_fn, hooks, saving_listeners ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 843 , in _train_model return self._train_model_default ( input_fn, hooks, saving_listeners ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 856 , in _train_model_default features, labels, model_fn_lib.ModeKeys.TRAIN, self.config ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 831 , in _call_model_fn model_fn_results = self._model_fn ( features = features, **kwargs ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2036 , in _model_fn _train_on_tpu_system ( ctx, model_fn_wrapper, dequeue_fn )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2244 , in _train_on_tpu_system device_assignment = ctx.device_assignment ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\" , line 690 , in shard name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\" , line 516 , in replicate outputs = computation ( *computation_inputs ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2237 , in multi_tpu_train_steps_on_single_shard single_tpu_train_step, [ _INITIAL_LOSS ]) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 207 , in repeat cond, body_wrapper, inputs = inputs, infeed_queue = infeed_queue, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 169 , in while_loop name = \"\" ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 3224 , in while_loop result = loop_context.BuildLoop ( cond, body, loop_vars, shape_invariants ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 2956 , in BuildLoop pred, body, original_loop_vars, loop_vars, shape_invariants ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 2893 , in _BuildLoop body_result = body ( *packed_vars_for_body ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 120 , in body_wrapper outputs = body ( * ( inputs + dequeue_ops )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 203 , in body_wrapper return [ i + 1 ] + _convert_to_list ( body ( *args )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 1156 , in train_step self._call_model_fn ( features, labels )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 1317 , in _call_model_fn estimator_spec = self._model_fn ( features = features, **kwargs ) File \"/root/.local/lib/python2.7/site-packages/object_detection/model_lib.py\" , line 252 , in model_fn preprocessed_images, features [ fields.InputDataFields.true_image_shape ]) File \"/root/.local/lib/python2.7/site-packages/object_detection/meta_architectures/ssd_meta_arch.py\" , line 514 , in predict preprocessed_inputs ) File \"/root/.local/lib/python2.7/site-packages/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py\" , line 146 , in extract_features depth = depth_fn ( 256 )) File \"/root/.local/lib/python2.7/site-packages/object_detection/models/feature_map_generators.py\" , line 218 , in fpn_top_down_feature_maps top_down += residual File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\" , line 979 , in binary_op_wrapper return func ( x, y, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\" , line 297 , in add \"Add\" , x = x, y = y, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\" , line 787 , in _apply_op_helper op_def = op_def ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 3392 , in create_op op_def = op_def ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 1734 , in __init__ control_input_ops ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 1570 , in _create_c_op raise ValueError ( str ( e )) ValueError: Dimensions must be equal, but are 20 and 19 for 'FeatureExtractor/MobilenetV1/fpn/top_down/add' ( op: 'Add' ) with input shapes: [ 8 ,20,20,256 ] , [ 8 ,19,19,256 ] .","title":"\u30a8\u30e9\u30fc"},{"location":"ssd_mobilenet_v1_fpn_coco/#_3","text":"Shell 1 2 3 $ export CONFIG_FILE = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config $ export CHECKPOINT_PATH = gs:// ${ YOUR_GCS_BUCKET } /train_fpn/model.ckpt-2000 $ export OUTPUT_DIR = /tmp/tflite Shell 1 $ python object_detection/export_tflite_ssd_graph.py --pipeline_config_path = $CONFIG_FILE --trained_checkpoint_prefix = $CHECKPOINT_PATH --output_directory = $OUTPUT_DIR --add_postprocessing_op = true","title":"\u53d6\u5f97"},{"location":"ssd_mobilenet_v1_fpn_coco/#toco","text":"1 $ bazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file = $OUTPUT_DIR /tflite_graph.pb --output_file = $OUTPUT_DIR /detect.tflite --input_shapes = 1 ,300,300,3 --input_arrays = normalized_input_image_tensor --output_arrays = 'TFLite_Detection_PostProcess' , 'TFLite_Detection_PostProcess:1' , 'TFLite_Detection_PostProcess:2' , 'TFLite_Detection_PostProcess:3' --inference_type = QUANTIZED_UINT8 --mean_values = 128 --std_values = 128 --change_concat_input_ranges = false --allow_custom_ops --default_ranges_min = 0 --default_ranges_max = 6","title":"toco"},{"location":"ssd_mobilenet_v1_fpn_coco/#_4","text":"1 $ git reset --hard","title":"\u5f8c\u51e6\u7406"},{"location":"ssd_mobilenet_v1_ppn_coco/","text":"Git clone\u76f4\u5f8c\u306e\u5834\u5408 COCO API\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Shell 1 2 3 4 $ git clone https://github.com/cocodataset/cocoapi.git $ cd cocoapi/PythonAPI $ make $ cp -r pycocotools ../../models/research/ Shell 1 2 3 $ cd models/reasearch $ protoc object_detection/protos/*.proto --python_out = . docker\u5185\u3067\u5b9f\u884c Shell 1 $ export PYTHONPATH = $PYTHONPATH : ` pwd ` : ` pwd ` /slim \u30e2\u30c7\u30eb Shell 1 2 3 4 5 6 7 8 9 10 $ cd /tmp $ curl -O http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync_2018_07_03.tar.gz $ tar xzf ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync_2018_07_03.tar.gz $ gsutil cp /tmp/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync_2018_07_03/model.ckpt.* gs:// ${ YOUR_GCS_BUCKET } /data/ ``` # TPU\u3067\u5b66\u7fd2 ``` shell $ vim object_detection/samples/configs/object_detection/samples/configs/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync.config $ gsutil cp object_detection/samples/configs/object_detection/samples/configs/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync.config gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config Shell 1 gcloud ml-engine jobs submit training ` whoami ` _object_detection_ ` date +%s ` --job-dir = gs:// ${ YOUR_GCS_BUCKET } /train_ppn --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz --module-name object_detection.model_tpu_main --runtime-version 1 .8 --scale-tier BASIC_TPU --region us-central1 -- --model_dir = gs:// ${ YOUR_GCS_BUCKET } /train_ppn --tpu_zone us-central1 --pipeline_config_path = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config \u53d6\u5f97 Shell 1 2 3 $ export CONFIG_FILE = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config $ export CHECKPOINT_PATH = gs:// ${ YOUR_GCS_BUCKET } /train_ppn/model.ckpt-2000 $ export OUTPUT_DIR = /tmp/tflite Shell 1 $ python object_detection/export_tflite_ssd_graph.py --pipeline_config_path = $CONFIG_FILE --trained_checkpoint_prefix = $CHECKPOINT_PATH --output_directory = $OUTPUT_DIR --add_postprocessing_op = true toco 1 bazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6 \u5f8c\u51e6\u7406 1 $ git reset --hard","title":"Git clone\u76f4\u5f8c\u306e\u5834\u5408"},{"location":"ssd_mobilenet_v1_ppn_coco/#git-clone","text":"COCO API\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Shell 1 2 3 4 $ git clone https://github.com/cocodataset/cocoapi.git $ cd cocoapi/PythonAPI $ make $ cp -r pycocotools ../../models/research/ Shell 1 2 3 $ cd models/reasearch $ protoc object_detection/protos/*.proto --python_out = . docker\u5185\u3067\u5b9f\u884c Shell 1 $ export PYTHONPATH = $PYTHONPATH : ` pwd ` : ` pwd ` /slim","title":"Git clone\u76f4\u5f8c\u306e\u5834\u5408"},{"location":"ssd_mobilenet_v1_ppn_coco/#_1","text":"Shell 1 2 3 4 5 6 7 8 9 10 $ cd /tmp $ curl -O http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync_2018_07_03.tar.gz $ tar xzf ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync_2018_07_03.tar.gz $ gsutil cp /tmp/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync_2018_07_03/model.ckpt.* gs:// ${ YOUR_GCS_BUCKET } /data/ ``` # TPU\u3067\u5b66\u7fd2 ``` shell $ vim object_detection/samples/configs/object_detection/samples/configs/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync.config $ gsutil cp object_detection/samples/configs/object_detection/samples/configs/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync.config gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config Shell 1 gcloud ml-engine jobs submit training ` whoami ` _object_detection_ ` date +%s ` --job-dir = gs:// ${ YOUR_GCS_BUCKET } /train_ppn --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz --module-name object_detection.model_tpu_main --runtime-version 1 .8 --scale-tier BASIC_TPU --region us-central1 -- --model_dir = gs:// ${ YOUR_GCS_BUCKET } /train_ppn --tpu_zone us-central1 --pipeline_config_path = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config","title":"\u30e2\u30c7\u30eb"},{"location":"ssd_mobilenet_v1_ppn_coco/#_2","text":"Shell 1 2 3 $ export CONFIG_FILE = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config $ export CHECKPOINT_PATH = gs:// ${ YOUR_GCS_BUCKET } /train_ppn/model.ckpt-2000 $ export OUTPUT_DIR = /tmp/tflite Shell 1 $ python object_detection/export_tflite_ssd_graph.py --pipeline_config_path = $CONFIG_FILE --trained_checkpoint_prefix = $CHECKPOINT_PATH --output_directory = $OUTPUT_DIR --add_postprocessing_op = true","title":"\u53d6\u5f97"},{"location":"ssd_mobilenet_v1_ppn_coco/#toco","text":"1 bazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6","title":"toco"},{"location":"ssd_mobilenet_v1_ppn_coco/#_3","text":"1 $ git reset --hard","title":"\u5f8c\u51e6\u7406"},{"location":"ssd_mobilenet_v1_quantized_coco/","text":"Git clone\u76f4\u5f8c\u306e\u5834\u5408\u3000 COCO API\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Shell 1 2 3 4 $ git clone https://github.com/cocodataset/cocoapi.git $ cd cocoapi/PythonAPI $ make $ cp -r pycocotools ../../models/research/ Shell 1 2 3 $ cd models/reasearch $ protoc object_detection/protos/*.proto --python_out = . docker\u5185\u3067\u5b9f\u884c Shell 1 $ export PYTHONPATH = $PYTHONPATH : ` pwd ` : ` pwd ` /slim \u30e2\u30c7\u30eb 1 2 3 4 $ cd /tmp $ curl -O http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz $ tar xzf ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz $ gsutil cp /tmp/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18/model.ckpt.* gs:// ${ YOUR_GCS_BUCKET } /data/ TPU\u3067\u5b66\u7fd2 Shell 1 2 $ vim object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config $ gsutil cp object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config Shell 1 gcloud ml-engine jobs submit training ` whoami ` _object_detection_ ` date +%s ` --job-dir = gs:// ${ YOUR_GCS_BUCKET } /train_quantized --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz --module-name object_detection.model_tpu_main --runtime-version 1 .8 --scale-tier BASIC_TPU --region us-central1 -- --model_dir = gs:// ${ YOUR_GCS_BUCKET } /train_quantized --tpu_zone us-central1 --pipeline_config_path = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config \u30a8\u30e9\u30fc 1 2 master-replica-0 Traceback ( most recent call last ) : File \"/usr/lib/python2.7/runpy.py\" , line 174 , in _run_module_as_main \"__main__\" , fname, loader, pkg_name ) File \"/usr/lib/python2.7/runpy.py\" , line 72 , in _run_code exec code in run_globals File \"/root/.local/lib/python2.7/site-packages/object_detection/model_tpu_main.py\" , line 134 , in <module> tf.app.run () File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\" , line 126 , in run _sys.exit ( main ( argv )) File \"/root/.local/lib/python2.7/site-packages/object_detection/model_tpu_main.py\" , line 119 , in main estimator.train ( input_fn = train_input_fn, max_steps = train_steps ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 363 , in train loss = self._train_model ( input_fn, hooks, saving_listeners ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 843 , in _train_model return self._train_model_default ( input_fn, hooks, saving_listeners ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 856 , in _train_model_default features, labels, model_fn_lib.ModeKeys.TRAIN, self.config ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 831 , in _call_model_fn model_fn_results = self._model_fn ( features = features, **kwargs ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2036 , in _model_fn _train_on_tpu_system ( ctx, model_fn_wrapper, dequeue_fn )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2244 , in _train_on_tpu_system device_assignment = ctx.device_assignment ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\" , line 690 , in shard name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\" , line 516 , in replicate outputs = computation ( *computation_inputs ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2237 , in multi_tpu_train_steps_on_single_shard single_tpu_train_step, [ _INITIAL_LOSS ]) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 207 , in repeat cond, body_wrapper, inputs = inputs, infeed_queue = infeed_queue, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 169 , in while_loop name = \"\" ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 3224 , in while_loop result = loop_context.BuildLoop ( cond, body, loop_vars, shape_invariants ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 2956 , in BuildLoop pred, body, original_loop_vars, loop_vars, shape_invariants ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 2893 , in _BuildLoop body_result = body ( *packed_vars_for_body ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 120 , in body_wrapper outputs = body ( * ( inputs + dequeue_ops )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 203 , in body_wrapper return [ i + 1 ] + _convert_to_list ( body ( *args )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 1156 , in train_step self._call_model_fn ( features, labels )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 1317 , in _call_model_fn estimator_spec = self._model_fn ( features = features, **kwargs ) File \"/root/.local/lib/python2.7/site-packages/object_detection/model_lib.py\" , line 252 , in model_fn preprocessed_images, features [ fields.InputDataFields.true_image_shape ]) File \"/root/.local/lib/python2.7/site-packages/object_detection/meta_architectures/ssd_meta_arch.py\" , line 514 , in predict preprocessed_inputs ) File \"/root/.local/lib/python2.7/site-packages/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py\" , line 146 , in extract_features depth = depth_fn ( 256 )) File \"/root/.local/lib/python2.7/site-packages/object_detection/models/feature_map_generators.py\" , line 218 , in fpn_top_down_feature_maps top_down += residual File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\" , line 979 , in binary_op_wrapper return func ( x, y, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\" , line 297 , in add \"Add\" , x = x, y = y, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\" , line 787 , in _apply_op_helper op_def = op_def ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 3392 , in create_op op_def = op_def ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 1734 , in __init__ control_input_ops ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 1570 , in _create_c_op raise ValueError ( str ( e )) ValueError: Dimensions must be equal, but are 20 and 19 for 'FeatureExtractor/MobilenetV1/fpn/top_down/add' ( op: 'Add' ) with input shapes: [ 8 ,20,20,256 ] , [ 8 ,19,19,256 ] . \u53d6\u5f97 Shell 1 2 3 $ export CONFIG_FILE = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config $ export CHECKPOINT_PATH = gs:// ${ YOUR_GCS_BUCKET } /train_resnet/model.ckpt-2000 $ export OUTPUT_DIR = /tmp/tflite Shell 1 $ python object_detection/export_tflite_ssd_graph.py --pipeline_config_path = $CONFIG_FILE --trained_checkpoint_prefix = $CHECKPOINT_PATH --output_directory = $OUTPUT_DIR --add_postprocessing_op = true toco 1 bazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6 \u5f8c\u51e6\u7406 1 $ git reset --hard","title":"Ssd mobilenet v1 quantized coco"},{"location":"ssd_mobilenet_v1_quantized_coco/#_1","text":"1 2 3 4 $ cd /tmp $ curl -O http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz $ tar xzf ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz $ gsutil cp /tmp/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18/model.ckpt.* gs:// ${ YOUR_GCS_BUCKET } /data/","title":"\u30e2\u30c7\u30eb"},{"location":"ssd_mobilenet_v1_quantized_coco/#tpu","text":"Shell 1 2 $ vim object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config $ gsutil cp object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config Shell 1 gcloud ml-engine jobs submit training ` whoami ` _object_detection_ ` date +%s ` --job-dir = gs:// ${ YOUR_GCS_BUCKET } /train_quantized --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz --module-name object_detection.model_tpu_main --runtime-version 1 .8 --scale-tier BASIC_TPU --region us-central1 -- --model_dir = gs:// ${ YOUR_GCS_BUCKET } /train_quantized --tpu_zone us-central1 --pipeline_config_path = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config","title":"TPU\u3067\u5b66\u7fd2"},{"location":"ssd_mobilenet_v1_quantized_coco/#_2","text":"1 2 master-replica-0 Traceback ( most recent call last ) : File \"/usr/lib/python2.7/runpy.py\" , line 174 , in _run_module_as_main \"__main__\" , fname, loader, pkg_name ) File \"/usr/lib/python2.7/runpy.py\" , line 72 , in _run_code exec code in run_globals File \"/root/.local/lib/python2.7/site-packages/object_detection/model_tpu_main.py\" , line 134 , in <module> tf.app.run () File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\" , line 126 , in run _sys.exit ( main ( argv )) File \"/root/.local/lib/python2.7/site-packages/object_detection/model_tpu_main.py\" , line 119 , in main estimator.train ( input_fn = train_input_fn, max_steps = train_steps ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 363 , in train loss = self._train_model ( input_fn, hooks, saving_listeners ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 843 , in _train_model return self._train_model_default ( input_fn, hooks, saving_listeners ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 856 , in _train_model_default features, labels, model_fn_lib.ModeKeys.TRAIN, self.config ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 831 , in _call_model_fn model_fn_results = self._model_fn ( features = features, **kwargs ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2036 , in _model_fn _train_on_tpu_system ( ctx, model_fn_wrapper, dequeue_fn )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2244 , in _train_on_tpu_system device_assignment = ctx.device_assignment ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\" , line 690 , in shard name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\" , line 516 , in replicate outputs = computation ( *computation_inputs ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2237 , in multi_tpu_train_steps_on_single_shard single_tpu_train_step, [ _INITIAL_LOSS ]) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 207 , in repeat cond, body_wrapper, inputs = inputs, infeed_queue = infeed_queue, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 169 , in while_loop name = \"\" ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 3224 , in while_loop result = loop_context.BuildLoop ( cond, body, loop_vars, shape_invariants ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 2956 , in BuildLoop pred, body, original_loop_vars, loop_vars, shape_invariants ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 2893 , in _BuildLoop body_result = body ( *packed_vars_for_body ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 120 , in body_wrapper outputs = body ( * ( inputs + dequeue_ops )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 203 , in body_wrapper return [ i + 1 ] + _convert_to_list ( body ( *args )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 1156 , in train_step self._call_model_fn ( features, labels )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 1317 , in _call_model_fn estimator_spec = self._model_fn ( features = features, **kwargs ) File \"/root/.local/lib/python2.7/site-packages/object_detection/model_lib.py\" , line 252 , in model_fn preprocessed_images, features [ fields.InputDataFields.true_image_shape ]) File \"/root/.local/lib/python2.7/site-packages/object_detection/meta_architectures/ssd_meta_arch.py\" , line 514 , in predict preprocessed_inputs ) File \"/root/.local/lib/python2.7/site-packages/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py\" , line 146 , in extract_features depth = depth_fn ( 256 )) File \"/root/.local/lib/python2.7/site-packages/object_detection/models/feature_map_generators.py\" , line 218 , in fpn_top_down_feature_maps top_down += residual File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\" , line 979 , in binary_op_wrapper return func ( x, y, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\" , line 297 , in add \"Add\" , x = x, y = y, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\" , line 787 , in _apply_op_helper op_def = op_def ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 3392 , in create_op op_def = op_def ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 1734 , in __init__ control_input_ops ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 1570 , in _create_c_op raise ValueError ( str ( e )) ValueError: Dimensions must be equal, but are 20 and 19 for 'FeatureExtractor/MobilenetV1/fpn/top_down/add' ( op: 'Add' ) with input shapes: [ 8 ,20,20,256 ] , [ 8 ,19,19,256 ] .","title":"\u30a8\u30e9\u30fc"},{"location":"ssd_mobilenet_v1_quantized_coco/#_3","text":"Shell 1 2 3 $ export CONFIG_FILE = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config $ export CHECKPOINT_PATH = gs:// ${ YOUR_GCS_BUCKET } /train_resnet/model.ckpt-2000 $ export OUTPUT_DIR = /tmp/tflite Shell 1 $ python object_detection/export_tflite_ssd_graph.py --pipeline_config_path = $CONFIG_FILE --trained_checkpoint_prefix = $CHECKPOINT_PATH --output_directory = $OUTPUT_DIR --add_postprocessing_op = true","title":"\u53d6\u5f97"},{"location":"ssd_mobilenet_v1_quantized_coco/#toco","text":"1 bazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6","title":"toco"},{"location":"ssd_mobilenet_v1_quantized_coco/#_4","text":"1 $ git reset --hard","title":"\u5f8c\u51e6\u7406"},{"location":"ssd_resnet_50_fpn_coco/","text":"Git clone\u76f4\u5f8c\u306e\u5834\u5408\u3000 COCO API\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Shell 1 2 3 4 $ git clone https://github.com/cocodataset/cocoapi.git $ cd cocoapi/PythonAPI $ make $ cp -r pycocotools ../../models/research/ Shell 1 2 3 $ cd models/reasearch $ protoc object_detection/protos/*.proto --python_out = . docker\u5185\u3067\u5b9f\u884c Shell 1 $ export PYTHONPATH = $PYTHONPATH : ` pwd ` : ` pwd ` /slim \u30e2\u30c7\u30eb 1 2 3 4 $ cd /tmp $ curl -O http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz $ tar xzf ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz $ gsutil cp /tmp/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/model.ckpt.* gs:// ${ YOUR_GCS_BUCKET } /data/ TPU\u3067\u5b66\u7fd2 Shell 1 2 $ vim object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config $ gsutil cp object_detection/samples/configs/object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config Shell 1 gcloud ml-engine jobs submit training ` whoami ` _object_detection_ ` date +%s ` --job-dir = gs:// ${ YOUR_GCS_BUCKET } /train_resnet --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz --module-name object_detection.model_tpu_main --runtime-version 1 .8 --scale-tier BASIC_TPU --region us-central1 -- --model_dir = gs:// ${ YOUR_GCS_BUCKET } /train_resnet --tpu_zone us-central1 --pipeline_config_path = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config \u30a8\u30e9\u30fc 1 2 master-replica-0 Traceback ( most recent call last ) : File \"/usr/lib/python2.7/runpy.py\" , line 174 , in _run_module_as_main \"__main__\" , fname, loader, pkg_name ) File \"/usr/lib/python2.7/runpy.py\" , line 72 , in _run_code exec code in run_globals File \"/root/.local/lib/python2.7/site-packages/object_detection/model_tpu_main.py\" , line 134 , in <module> tf.app.run () File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\" , line 126 , in run _sys.exit ( main ( argv )) File \"/root/.local/lib/python2.7/site-packages/object_detection/model_tpu_main.py\" , line 119 , in main estimator.train ( input_fn = train_input_fn, max_steps = train_steps ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 363 , in train loss = self._train_model ( input_fn, hooks, saving_listeners ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 843 , in _train_model return self._train_model_default ( input_fn, hooks, saving_listeners ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 856 , in _train_model_default features, labels, model_fn_lib.ModeKeys.TRAIN, self.config ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 831 , in _call_model_fn model_fn_results = self._model_fn ( features = features, **kwargs ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2036 , in _model_fn _train_on_tpu_system ( ctx, model_fn_wrapper, dequeue_fn )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2244 , in _train_on_tpu_system device_assignment = ctx.device_assignment ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\" , line 690 , in shard name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\" , line 516 , in replicate outputs = computation ( *computation_inputs ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2237 , in multi_tpu_train_steps_on_single_shard single_tpu_train_step, [ _INITIAL_LOSS ]) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 207 , in repeat cond, body_wrapper, inputs = inputs, infeed_queue = infeed_queue, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 169 , in while_loop name = \"\" ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 3224 , in while_loop result = loop_context.BuildLoop ( cond, body, loop_vars, shape_invariants ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 2956 , in BuildLoop pred, body, original_loop_vars, loop_vars, shape_invariants ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 2893 , in _BuildLoop body_result = body ( *packed_vars_for_body ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 120 , in body_wrapper outputs = body ( * ( inputs + dequeue_ops )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 203 , in body_wrapper return [ i + 1 ] + _convert_to_list ( body ( *args )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 1156 , in train_step self._call_model_fn ( features, labels )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 1317 , in _call_model_fn estimator_spec = self._model_fn ( features = features, **kwargs ) File \"/root/.local/lib/python2.7/site-packages/object_detection/model_lib.py\" , line 252 , in model_fn preprocessed_images, features [ fields.InputDataFields.true_image_shape ]) File \"/root/.local/lib/python2.7/site-packages/object_detection/meta_architectures/ssd_meta_arch.py\" , line 514 , in predict preprocessed_inputs ) File \"/root/.local/lib/python2.7/site-packages/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py\" , line 146 , in extract_features depth = depth_fn ( 256 )) File \"/root/.local/lib/python2.7/site-packages/object_detection/models/feature_map_generators.py\" , line 218 , in fpn_top_down_feature_maps top_down += residual File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\" , line 979 , in binary_op_wrapper return func ( x, y, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\" , line 297 , in add \"Add\" , x = x, y = y, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\" , line 787 , in _apply_op_helper op_def = op_def ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 3392 , in create_op op_def = op_def ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 1734 , in __init__ control_input_ops ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 1570 , in _create_c_op raise ValueError ( str ( e )) ValueError: Dimensions must be equal, but are 20 and 19 for 'FeatureExtractor/MobilenetV1/fpn/top_down/add' ( op: 'Add' ) with input shapes: [ 8 ,20,20,256 ] , [ 8 ,19,19,256 ] . \u53d6\u5f97 Shell 1 2 3 $ export CONFIG_FILE = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config $ export CHECKPOINT_PATH = gs:// ${ YOUR_GCS_BUCKET } /train_resnet/model.ckpt-2000 $ export OUTPUT_DIR = /tmp/tflite Shell 1 $ python object_detection/export_tflite_ssd_graph.py --pipeline_config_path = $CONFIG_FILE --trained_checkpoint_prefix = $CHECKPOINT_PATH --output_directory = $OUTPUT_DIR --add_postprocessing_op = true toco 1 bazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6 \u5f8c\u51e6\u7406 1 $ git reset --hard","title":"Ssd resnet 50 fpn coco"},{"location":"ssd_resnet_50_fpn_coco/#_1","text":"1 2 3 4 $ cd /tmp $ curl -O http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz $ tar xzf ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz $ gsutil cp /tmp/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/model.ckpt.* gs:// ${ YOUR_GCS_BUCKET } /data/","title":"\u30e2\u30c7\u30eb"},{"location":"ssd_resnet_50_fpn_coco/#tpu","text":"Shell 1 2 $ vim object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config $ gsutil cp object_detection/samples/configs/object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config Shell 1 gcloud ml-engine jobs submit training ` whoami ` _object_detection_ ` date +%s ` --job-dir = gs:// ${ YOUR_GCS_BUCKET } /train_resnet --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz --module-name object_detection.model_tpu_main --runtime-version 1 .8 --scale-tier BASIC_TPU --region us-central1 -- --model_dir = gs:// ${ YOUR_GCS_BUCKET } /train_resnet --tpu_zone us-central1 --pipeline_config_path = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config","title":"TPU\u3067\u5b66\u7fd2"},{"location":"ssd_resnet_50_fpn_coco/#_2","text":"1 2 master-replica-0 Traceback ( most recent call last ) : File \"/usr/lib/python2.7/runpy.py\" , line 174 , in _run_module_as_main \"__main__\" , fname, loader, pkg_name ) File \"/usr/lib/python2.7/runpy.py\" , line 72 , in _run_code exec code in run_globals File \"/root/.local/lib/python2.7/site-packages/object_detection/model_tpu_main.py\" , line 134 , in <module> tf.app.run () File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\" , line 126 , in run _sys.exit ( main ( argv )) File \"/root/.local/lib/python2.7/site-packages/object_detection/model_tpu_main.py\" , line 119 , in main estimator.train ( input_fn = train_input_fn, max_steps = train_steps ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 363 , in train loss = self._train_model ( input_fn, hooks, saving_listeners ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 843 , in _train_model return self._train_model_default ( input_fn, hooks, saving_listeners ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 856 , in _train_model_default features, labels, model_fn_lib.ModeKeys.TRAIN, self.config ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\" , line 831 , in _call_model_fn model_fn_results = self._model_fn ( features = features, **kwargs ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2036 , in _model_fn _train_on_tpu_system ( ctx, model_fn_wrapper, dequeue_fn )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2244 , in _train_on_tpu_system device_assignment = ctx.device_assignment ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\" , line 690 , in shard name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\" , line 516 , in replicate outputs = computation ( *computation_inputs ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 2237 , in multi_tpu_train_steps_on_single_shard single_tpu_train_step, [ _INITIAL_LOSS ]) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 207 , in repeat cond, body_wrapper, inputs = inputs, infeed_queue = infeed_queue, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 169 , in while_loop name = \"\" ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 3224 , in while_loop result = loop_context.BuildLoop ( cond, body, loop_vars, shape_invariants ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 2956 , in BuildLoop pred, body, original_loop_vars, loop_vars, shape_invariants ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\" , line 2893 , in _BuildLoop body_result = body ( *packed_vars_for_body ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 120 , in body_wrapper outputs = body ( * ( inputs + dequeue_ops )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\" , line 203 , in body_wrapper return [ i + 1 ] + _convert_to_list ( body ( *args )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 1156 , in train_step self._call_model_fn ( features, labels )) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\" , line 1317 , in _call_model_fn estimator_spec = self._model_fn ( features = features, **kwargs ) File \"/root/.local/lib/python2.7/site-packages/object_detection/model_lib.py\" , line 252 , in model_fn preprocessed_images, features [ fields.InputDataFields.true_image_shape ]) File \"/root/.local/lib/python2.7/site-packages/object_detection/meta_architectures/ssd_meta_arch.py\" , line 514 , in predict preprocessed_inputs ) File \"/root/.local/lib/python2.7/site-packages/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py\" , line 146 , in extract_features depth = depth_fn ( 256 )) File \"/root/.local/lib/python2.7/site-packages/object_detection/models/feature_map_generators.py\" , line 218 , in fpn_top_down_feature_maps top_down += residual File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\" , line 979 , in binary_op_wrapper return func ( x, y, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\" , line 297 , in add \"Add\" , x = x, y = y, name = name ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\" , line 787 , in _apply_op_helper op_def = op_def ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 3392 , in create_op op_def = op_def ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 1734 , in __init__ control_input_ops ) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\" , line 1570 , in _create_c_op raise ValueError ( str ( e )) ValueError: Dimensions must be equal, but are 20 and 19 for 'FeatureExtractor/MobilenetV1/fpn/top_down/add' ( op: 'Add' ) with input shapes: [ 8 ,20,20,256 ] , [ 8 ,19,19,256 ] .","title":"\u30a8\u30e9\u30fc"},{"location":"ssd_resnet_50_fpn_coco/#_3","text":"Shell 1 2 3 $ export CONFIG_FILE = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config $ export CHECKPOINT_PATH = gs:// ${ YOUR_GCS_BUCKET } /train_resnet/model.ckpt-2000 $ export OUTPUT_DIR = /tmp/tflite Shell 1 $ python object_detection/export_tflite_ssd_graph.py --pipeline_config_path = $CONFIG_FILE --trained_checkpoint_prefix = $CHECKPOINT_PATH --output_directory = $OUTPUT_DIR --add_postprocessing_op = true","title":"\u53d6\u5f97"},{"location":"ssd_resnet_50_fpn_coco/#toco","text":"1 bazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6","title":"toco"},{"location":"ssd_resnet_50_fpn_coco/#_4","text":"1 $ git reset --hard","title":"\u5f8c\u51e6\u7406"},{"location":"tips/","text":"Tips \u8272\u7a7a\u9593\u306e\u5909\u63db\u3068\u8272\u6210\u5206\u306e\u5206\u89e3 1 2 3 4 5 6 7 8 9 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2XYZ ) z_image = cvt_image [:,:, 2 ] plt . imshow ( z_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"Tips"},{"location":"tips/#tips","text":"","title":"Tips"},{"location":"tips/#_1","text":"1 2 3 4 5 6 7 8 9 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2XYZ ) z_image = cvt_image [:,:, 2 ] plt . imshow ( z_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"\u8272\u7a7a\u9593\u306e\u5909\u63db\u3068\u8272\u6210\u5206\u306e\u5206\u89e3"},{"location":"tpu_android/","text":"TPU Android TOCO Shell 1 2 3 $ export CONFIG_FILE = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config $ export CHECKPOINT_PATH = gs:// ${ YOUR_GCS_BUCKET } /train/model.ckpt-2000 $ export OUTPUT_DIR = /tmp/tflite Shell 1 2 3 4 5 python object_detection/export_tflite_ssd_graph.py \\ --pipeline_config_path = $CONFIG_FILE \\ --trained_checkpoint_prefix = $CHECKPOINT_PATH \\ --output_directory = $OUTPUT_DIR \\ --add_postprocessing_op = true 1 2 3 4 5 / usr / local / lib / python2 . 7 / dist - packages / h5py / __init__ . py : 36 : FutureWarning : Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated . In future , it will be treated as `np.float64 == np.dtype(float).type` . from ._conv import register_converters as _register_converters 2018 - 08 - 14 22 : 23 : 32.219149 : I tensorflow / core / platform / cpu_feature_guard . cc : 140 ] Your CPU supports instructions that this TensorFlow binary was not compiled to use : AVX2 FMA Converted 387 variables to const ops . 2018 - 08 - 14 22 : 23 : 40.674300 : I tensorflow / tools / graph_transforms / transform_graph . cc : 264 ] Applying strip_unused_nodes /tmp/tflite\u306b\u4e0b\u8a18\u30d5\u30a1\u30a4\u30eb\u304c\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3055\u308c\u308b\u3002 1 tflite_graph.pb tflite_graph.pbtxt TensorFlow $ git clone https://github.com/tensorflow/tensorflow Copy tensorflow/examples/android/\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f7f\u3046\u3002tensorflow/contrib/lite/examples/android/\u306eDetectoreActivity.java\u3068TFLiteObjectDetectionAPIModel.java\u3092\u4f7f\u7528\u3059\u308b\u3002 tensorflow\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5 1 2 $ cp tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/DetectorActivity.java tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java $ cp tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java tensorflow/examples/android/src/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java TOCO Docker\u306bBazel\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Shell 1 2 3 4 $ sudo apt-get install pkg-config zip g++ zlib1g-dev unzip python $ wget https://github.com/bazelbuild/bazel/releases/download/0.16.0/bazel-0.16.0-installer-linux-x86_64.sh $ chmod +x bazel-0.16.0-installer-linux-x86_64.sh $ ./bazel-<version>-installer-linux-x86_64.sh --user Shall 1 $ export PATH = \" $PATH : $HOME /bin\" Shell 1 2 3 4 5 6 7 8 9 10 11 bazel run -c opt tensorflow/contrib/lite/toco:toco -- \\ --input_file = $OUTPUT_DIR /tflite_graph.pb \\ --output_file = $OUTPUT_DIR /detect.tflite \\ --input_shapes = 1 ,300,300,3 \\ --input_arrays = normalized_input_image_tensor \\ --output_arrays = 'TFLite_Detection_PostProcess' , 'TFLite_Detection_PostProcess:1' , 'TFLite_Detection_PostProcess:2' , 'TFLite_Detection_PostProcess:3' \\ --inference_type = QUANTIZED_UINT8 \\ --mean_values = 128 \\ --std_values = 128 \\ --change_concat_input_ranges = false \\ --allow_custom_ops 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 INFO : Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded). INFO : Found 1 target ... Target //tensorflow/contrib/lite/toco:toco up-to-date: bazel - bin /tensorflow/contrib/lite/toco/ toco INFO : Elapsed time : 0.254 s , Critical Path : 0.00 s INFO : 0 processes . INFO : Build completed successfully , 1 total action INFO : Running command line : bazel - bin /tensorflow/contrib/lite/toco/toco '--input_file=/tmp/tflite/tflite_graph.pb' '--output_file=/tmp/tflite/ detect . tflite ' ' -- input_shapes = 1 , 300 , 300 , 3 ' ' -- input_arrays = normalized_input_image_tensor ' ' -- output_arrays = TFLite_Detection_PostProcess , TFLite_Detection_PostProcess : 1 , TFLite_Detection_PostProcess : 2 , TFLite_Detection_PostProcess : 3 ' ' -- inference_type = QUANTIZED_UINT8 ' ' -- mean_values = 128 ' ' -- std_values = 128 ' ' -- change_concat_input_ranges = falINFO : Build completed successfully , 1 total action 2018 - 08 - 14 22 : 26 : 06.150620 : I tensorflow /contrib/lite/toco/i mport_tensorflow . cc : 1055 ] Converting unsupported operation : TFLite_Detection_PostProcess 2018 - 08 - 14 22 : 26 : 06.177519 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] Before Removing unused ops : 900 operators , 1355 arrays ( 0 quantized ) 2018 - 08 - 14 22 : 26 : 06.207086 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] Before general graph transformations : 900 operators , 1355 arrays ( 0 quantized ) 2018 - 08 - 14 22 : 26 : 06.557619 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] After general graph transformations pass 1 : 112 operators , 224 arrays ( 1 quantized ) 2018 - 08 - 14 22 : 26 : 06.560343 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] Before pre - quantization graph transformations : 112 operators , 224 arrays ( 1 quantized ) 2018 - 08 - 14 22 : 26 : 06.561888 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] After pre - quantization graph transformations pass 1 : 65 operators , 177 arrays ( 1 quantized ) 2018 - 08 - 14 22 : 26 : 06.563376 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] Before quantization graph transformations : 65 operators , 177 arrays ( 1 quantized ) 2018 - 08 - 14 22 : 26 : 06.606566 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] After quantization graph transformations pass 1 : 71 operators , 183 arrays ( 151 quantized ) 2018 - 08 - 14 22 : 26 : 06.610353 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] After quantization graph transformations pass 2 : 71 operators , 183 arrays ( 155 quantized ) 2018 - 08 - 14 22 : 26 : 06.613796 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] After quantization graph transformations pass 3 : 66 operators , 178 arrays ( 157 quantized ) 2018 - 08 - 14 22 : 26 : 06.617289 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] After quantization graph transformations pass 4 : 66 operators , 178 arrays ( 158 quantized ) 2018 - 08 - 14 22 : 26 : 06.620144 : W tensorflow /contrib/lite/toco/graph_transformations/ quantize . cc : 99 ] Constant array anchors lacks MinMax information . To make up for that , we will now compute the MinMax from actual array elements . That will result in quantization parameters that probably do not match whichever arithmetic was used during training , and thus will probably be a cause of poor inference accuracy . 2018 - 08 - 14 22 : 26 : 06.620393 : W tensorflow /contrib/lite/toco/graph_transformations/ quantize . cc : 599 ] ( Unsupported TensorFlow op : TFLite_Detection_PostProcess ) is a quantized opbut it has a model flag that sets the output arrays to float . 2018 - 08 - 14 22 : 26 : 06.620444 : W tensorflow /contrib/lite/toco/graph_transformations/ quantize . cc : 599 ] ( Unsupported TensorFlow op : TFLite_Detection_PostProcess ) is a quantized opbut it has a model flag that sets the output arrays to float . 2018 - 08 - 14 22 : 26 : 06.621551 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] After quantization graph transformations pass 5 : 64 operators , 176 arrays ( 159 quantized ) 2018 - 08 - 14 22 : 26 : 06.622068 : W tensorflow /contrib/lite/toco/graph_transformations/ quantize . cc : 599 ] ( Unsupported TensorFlow op : TFLite_Detection_PostProcess ) is a quantized opbut it has a model flag that sets the output arrays to float . 2018 - 08 - 14 22 : 26 : 06.625193 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] Before shuffling of FC weights : 64 operators , 176 arrays ( 159 quantized ) 2018 - 08 - 14 22 : 26 : 06.626365 : I tensorflow /contrib/lite/toco/ allocate_transient_arrays . cc : 329 ] Total transient array allocated size : 2160064 bytes , theoretical optimal value : 1620032 bytes . 2018 - 08 - 14 22 : 26 : 06.626757 : I tensorflow /contrib/lite/toco/ toco_tooling . cc : 392 ] Estimated count of arithmetic ops : 1.36705 billion ( note that a multiply - add is counted as 2 ops ). 2018 - 08 - 14 22 : 26 : 06.627287 : W tensorflow /contrib/lite/toco/tflite/ operator . cc : 1163 ] Ignoring unsupported type in list attribute with key '_output_types' detect.tfline, pet_labels_list.txt \u3092 tensorflow/examples/android/assets \u306b\u30b3\u30d4\u30fc\u3002 Android Studio tensroflow/examples/android \u3092Android Studio\u3067\u958b\u304f Build.gradle 1 def nativeBuildSystem = 'none' 1 2 3 4 5 android { aaptOptions { noCompress 'tflite' } } 1 2 3 4 dependencies { compile 'org.tensorflow:tensorflow-android:+' compile 'org.tensorflow:tensorflow-lite:+' }","title":"TPU Android"},{"location":"tpu_android/#tpu-android","text":"","title":"TPU Android"},{"location":"tpu_android/#toco","text":"Shell 1 2 3 $ export CONFIG_FILE = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config $ export CHECKPOINT_PATH = gs:// ${ YOUR_GCS_BUCKET } /train/model.ckpt-2000 $ export OUTPUT_DIR = /tmp/tflite Shell 1 2 3 4 5 python object_detection/export_tflite_ssd_graph.py \\ --pipeline_config_path = $CONFIG_FILE \\ --trained_checkpoint_prefix = $CHECKPOINT_PATH \\ --output_directory = $OUTPUT_DIR \\ --add_postprocessing_op = true 1 2 3 4 5 / usr / local / lib / python2 . 7 / dist - packages / h5py / __init__ . py : 36 : FutureWarning : Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated . In future , it will be treated as `np.float64 == np.dtype(float).type` . from ._conv import register_converters as _register_converters 2018 - 08 - 14 22 : 23 : 32.219149 : I tensorflow / core / platform / cpu_feature_guard . cc : 140 ] Your CPU supports instructions that this TensorFlow binary was not compiled to use : AVX2 FMA Converted 387 variables to const ops . 2018 - 08 - 14 22 : 23 : 40.674300 : I tensorflow / tools / graph_transforms / transform_graph . cc : 264 ] Applying strip_unused_nodes /tmp/tflite\u306b\u4e0b\u8a18\u30d5\u30a1\u30a4\u30eb\u304c\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3055\u308c\u308b\u3002 1 tflite_graph.pb tflite_graph.pbtxt","title":"TOCO"},{"location":"tpu_android/#tensorflow","text":"$ git clone https://github.com/tensorflow/tensorflow","title":"TensorFlow"},{"location":"tpu_android/#copy","text":"tensorflow/examples/android/\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f7f\u3046\u3002tensorflow/contrib/lite/examples/android/\u306eDetectoreActivity.java\u3068TFLiteObjectDetectionAPIModel.java\u3092\u4f7f\u7528\u3059\u308b\u3002 tensorflow\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5 1 2 $ cp tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/DetectorActivity.java tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java $ cp tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java tensorflow/examples/android/src/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java","title":"Copy"},{"location":"tpu_android/#toco_1","text":"Docker\u306bBazel\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Shell 1 2 3 4 $ sudo apt-get install pkg-config zip g++ zlib1g-dev unzip python $ wget https://github.com/bazelbuild/bazel/releases/download/0.16.0/bazel-0.16.0-installer-linux-x86_64.sh $ chmod +x bazel-0.16.0-installer-linux-x86_64.sh $ ./bazel-<version>-installer-linux-x86_64.sh --user Shall 1 $ export PATH = \" $PATH : $HOME /bin\" Shell 1 2 3 4 5 6 7 8 9 10 11 bazel run -c opt tensorflow/contrib/lite/toco:toco -- \\ --input_file = $OUTPUT_DIR /tflite_graph.pb \\ --output_file = $OUTPUT_DIR /detect.tflite \\ --input_shapes = 1 ,300,300,3 \\ --input_arrays = normalized_input_image_tensor \\ --output_arrays = 'TFLite_Detection_PostProcess' , 'TFLite_Detection_PostProcess:1' , 'TFLite_Detection_PostProcess:2' , 'TFLite_Detection_PostProcess:3' \\ --inference_type = QUANTIZED_UINT8 \\ --mean_values = 128 \\ --std_values = 128 \\ --change_concat_input_ranges = false \\ --allow_custom_ops 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 INFO : Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded). INFO : Found 1 target ... Target //tensorflow/contrib/lite/toco:toco up-to-date: bazel - bin /tensorflow/contrib/lite/toco/ toco INFO : Elapsed time : 0.254 s , Critical Path : 0.00 s INFO : 0 processes . INFO : Build completed successfully , 1 total action INFO : Running command line : bazel - bin /tensorflow/contrib/lite/toco/toco '--input_file=/tmp/tflite/tflite_graph.pb' '--output_file=/tmp/tflite/ detect . tflite ' ' -- input_shapes = 1 , 300 , 300 , 3 ' ' -- input_arrays = normalized_input_image_tensor ' ' -- output_arrays = TFLite_Detection_PostProcess , TFLite_Detection_PostProcess : 1 , TFLite_Detection_PostProcess : 2 , TFLite_Detection_PostProcess : 3 ' ' -- inference_type = QUANTIZED_UINT8 ' ' -- mean_values = 128 ' ' -- std_values = 128 ' ' -- change_concat_input_ranges = falINFO : Build completed successfully , 1 total action 2018 - 08 - 14 22 : 26 : 06.150620 : I tensorflow /contrib/lite/toco/i mport_tensorflow . cc : 1055 ] Converting unsupported operation : TFLite_Detection_PostProcess 2018 - 08 - 14 22 : 26 : 06.177519 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] Before Removing unused ops : 900 operators , 1355 arrays ( 0 quantized ) 2018 - 08 - 14 22 : 26 : 06.207086 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] Before general graph transformations : 900 operators , 1355 arrays ( 0 quantized ) 2018 - 08 - 14 22 : 26 : 06.557619 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] After general graph transformations pass 1 : 112 operators , 224 arrays ( 1 quantized ) 2018 - 08 - 14 22 : 26 : 06.560343 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] Before pre - quantization graph transformations : 112 operators , 224 arrays ( 1 quantized ) 2018 - 08 - 14 22 : 26 : 06.561888 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] After pre - quantization graph transformations pass 1 : 65 operators , 177 arrays ( 1 quantized ) 2018 - 08 - 14 22 : 26 : 06.563376 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] Before quantization graph transformations : 65 operators , 177 arrays ( 1 quantized ) 2018 - 08 - 14 22 : 26 : 06.606566 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] After quantization graph transformations pass 1 : 71 operators , 183 arrays ( 151 quantized ) 2018 - 08 - 14 22 : 26 : 06.610353 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] After quantization graph transformations pass 2 : 71 operators , 183 arrays ( 155 quantized ) 2018 - 08 - 14 22 : 26 : 06.613796 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] After quantization graph transformations pass 3 : 66 operators , 178 arrays ( 157 quantized ) 2018 - 08 - 14 22 : 26 : 06.617289 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] After quantization graph transformations pass 4 : 66 operators , 178 arrays ( 158 quantized ) 2018 - 08 - 14 22 : 26 : 06.620144 : W tensorflow /contrib/lite/toco/graph_transformations/ quantize . cc : 99 ] Constant array anchors lacks MinMax information . To make up for that , we will now compute the MinMax from actual array elements . That will result in quantization parameters that probably do not match whichever arithmetic was used during training , and thus will probably be a cause of poor inference accuracy . 2018 - 08 - 14 22 : 26 : 06.620393 : W tensorflow /contrib/lite/toco/graph_transformations/ quantize . cc : 599 ] ( Unsupported TensorFlow op : TFLite_Detection_PostProcess ) is a quantized opbut it has a model flag that sets the output arrays to float . 2018 - 08 - 14 22 : 26 : 06.620444 : W tensorflow /contrib/lite/toco/graph_transformations/ quantize . cc : 599 ] ( Unsupported TensorFlow op : TFLite_Detection_PostProcess ) is a quantized opbut it has a model flag that sets the output arrays to float . 2018 - 08 - 14 22 : 26 : 06.621551 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] After quantization graph transformations pass 5 : 64 operators , 176 arrays ( 159 quantized ) 2018 - 08 - 14 22 : 26 : 06.622068 : W tensorflow /contrib/lite/toco/graph_transformations/ quantize . cc : 599 ] ( Unsupported TensorFlow op : TFLite_Detection_PostProcess ) is a quantized opbut it has a model flag that sets the output arrays to float . 2018 - 08 - 14 22 : 26 : 06.625193 : I tensorflow /contrib/lite/toco/graph_transformations/g raph_transformations . cc : 39 ] Before shuffling of FC weights : 64 operators , 176 arrays ( 159 quantized ) 2018 - 08 - 14 22 : 26 : 06.626365 : I tensorflow /contrib/lite/toco/ allocate_transient_arrays . cc : 329 ] Total transient array allocated size : 2160064 bytes , theoretical optimal value : 1620032 bytes . 2018 - 08 - 14 22 : 26 : 06.626757 : I tensorflow /contrib/lite/toco/ toco_tooling . cc : 392 ] Estimated count of arithmetic ops : 1.36705 billion ( note that a multiply - add is counted as 2 ops ). 2018 - 08 - 14 22 : 26 : 06.627287 : W tensorflow /contrib/lite/toco/tflite/ operator . cc : 1163 ] Ignoring unsupported type in list attribute with key '_output_types' detect.tfline, pet_labels_list.txt \u3092 tensorflow/examples/android/assets \u306b\u30b3\u30d4\u30fc\u3002","title":"TOCO"},{"location":"tpu_android/#android-studio","text":"tensroflow/examples/android \u3092Android Studio\u3067\u958b\u304f","title":"Android Studio"},{"location":"tpu_android/#buildgradle","text":"1 def nativeBuildSystem = 'none' 1 2 3 4 5 android { aaptOptions { noCompress 'tflite' } } 1 2 3 4 dependencies { compile 'org.tensorflow:tensorflow-android:+' compile 'org.tensorflow:tensorflow-lite:+' }","title":"Build.gradle"},{"location":"tpu_config/","text":"TPU Config Shell 1 $ vi object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config Sell 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 # SSD with Mobilenet v1 0.75 depth multiplied feature extractor and focal loss. # Trained on COCO14, initialized from Imagenet classification checkpoint # Achieves 17.5 mAP on COCO14 minival dataset. Doubling the number of training # steps gets to 18.4. # This config is TPU compatible model { ssd { inplace_batchnorm_update : true freeze_batchnorm : false num_classes : 90 box_coder { faster_rcnn_box_coder { y_scale : 10.0 x_scale : 10.0 height_scale : 5.0 width_scale : 5.0 } } matcher { argmax_matcher { matched_threshold : 0.5 unmatched_threshold : 0.5 ignore_thresholds : false negatives_lower_than_unmatched : true force_match_for_each_row : true use_matmul_gather : true } } similarity_calculator { iou_similarity { } } encode_background_as_zeros : true anchor_generator { ssd_anchor_generator { num_layers : 6 min_scale : 0.2 max_scale : 0.95 aspect_ratios : 1.0 aspect_ratios : 2.0 aspect_ratios : 0.5 aspect_ratios : 3.0 aspect_ratios : 0.3333 } } image_resizer { fixed_shape_resizer { height : 300 width : 300 } } box_predictor { convolutional_box_predictor { min_depth : 0 max_depth : 0 num_layers_before_predictor : 0 use_dropout : false dropout_keep_probability : 0.8 kernel_size : 1 box_code_size : 4 apply_sigmoid_to_scores : false class_prediction_bias_init : -4.6 conv_hyperparams { activation : RELU_6 , regularizer { l2_regularizer { weight : 0.00004 } } initializer { random_normal_initializer { stddev : 0.01 mean : 0.0 } } batch_norm { train : true , scale : true , center : true , decay : 0.97 , epsilon : 0.001 , } } } } feature_extractor { type : ' ssd_mobilenet_v1 ' min_depth : 16 depth_multiplier : 0.75 conv_hyperparams { activation : RELU_6 , regularizer { l2_regularizer { weight : 0.00004 } } initializer { truncated_normal_initializer { stddev : 0.03 mean : 0.0 } } batch_norm { train : true , scale : true , center : true , decay : 0.97 , epsilon : 0.001 , } } override_base_feature_extractor_hyperparams : true } loss { classification_loss { weighted_sigmoid_focal { alpha : 0.75 , gamma : 2.0 } } localization_loss { weighted_smooth_l1 { delta : 1.0 } } classification_weight : 1.0 localization_weight : 1.0 } normalize_loss_by_num_matches : true normalize_loc_loss_by_codesize : true post_processing { batch_non_max_suppression { score_threshold : 1e-8 iou_threshold : 0.6 max_detections_per_class : 100 max_total_detections : 100 } score_converter : SIGMOID } } } train_config : { fine_tune_checkpoint : \" PATH_TO_BE_CONFIGURED / model.ckpt \" batch_size : 2048 sync_replicas : true startup_delay_steps : 0 replicas_to_aggregate : 8 num_steps : 10000 data_augmentation_options { random_horizontal_flip { } } data_augmentation_options { ssd_random_crop { } } optimizer { momentum_optimizer : { learning_rate : { cosine_decay_learning_rate { learning_rate_base : 0.9 total_steps : 10000 warmup_learning_rate : 0.3 warmup_steps : 300 } } momentum_optimizer_value : 0.9 } use_moving_average : false } max_number_of_boxes : 100 unpad_groundtruth_tensors : false } train_input_reader : { tf_record_input_reader { input_path : \" PATH_TO_BE_CONFIGURED / mscoco_train.record -00000 - of -00100 \" } label_map_path : \" PATH_TO_BE_CONFIGURED / mscoco_label_map.pbtxt \" } eval_config : { metrics_set : \" coco_detection_metrics \" use_moving_averages : false num_examples : 8000 } eval_input_reader : { tf_record_input_reader { input_path : \" PATH_TO_BE_CONFIGURED / mscoco_val.record -00000 - of -00010 \" } label_map_path : \" PATH_TO_BE_CONFIGURED / mscoco_label_map.pbtxt \" shuffle : false num_readers : 1 } \u5909\u66f4\u70b91 1 2 train_config: { fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\" 1 2 3 train_config: { fine_tune_checkpoint: \"gs://your-bucket/data/model.ckpt\" fine_tune_checkpoint_type: \"detection\" \u5909\u66f4\u70b92 1 2 3 4 5 6 train_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record-00000-of-00100\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" } 1 2 3 4 5 6 train_input_reader: { tf_record_input_reader { input_path: \"gs://YOUR_GCS_BUCKET/data/pet_faces_train*\" } label_map_path: \"gs://YOUR_GCS_BUCKET/data/pet_label_map.pbtxt\" } \u5909\u66f4\u70b93 1 2 3 4 5 6 7 8 eval_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record-00000-of-00010\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" shuffle: false num_readers: 1 } 1 2 3 4 5 6 7 8 eval_input_reader: { tf_record_input_reader { input_path: \"gs://YOUR_GCS_BUCKET/data/pet_faces_val*\" } label_map_path: \"gs://YOUR_GCS_BUCKET/data/pet_label_map.pbtxt\" shuffle: false num_readers: 1 } \u30a2\u30c3\u30d7\u30ed\u30fc\u30c9 1 $ gsutil cp object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config","title":"TPU"},{"location":"tpu_config/#tpu","text":"","title":"TPU"},{"location":"tpu_config/#config","text":"Shell 1 $ vi object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config Sell 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 # SSD with Mobilenet v1 0.75 depth multiplied feature extractor and focal loss. # Trained on COCO14, initialized from Imagenet classification checkpoint # Achieves 17.5 mAP on COCO14 minival dataset. Doubling the number of training # steps gets to 18.4. # This config is TPU compatible model { ssd { inplace_batchnorm_update : true freeze_batchnorm : false num_classes : 90 box_coder { faster_rcnn_box_coder { y_scale : 10.0 x_scale : 10.0 height_scale : 5.0 width_scale : 5.0 } } matcher { argmax_matcher { matched_threshold : 0.5 unmatched_threshold : 0.5 ignore_thresholds : false negatives_lower_than_unmatched : true force_match_for_each_row : true use_matmul_gather : true } } similarity_calculator { iou_similarity { } } encode_background_as_zeros : true anchor_generator { ssd_anchor_generator { num_layers : 6 min_scale : 0.2 max_scale : 0.95 aspect_ratios : 1.0 aspect_ratios : 2.0 aspect_ratios : 0.5 aspect_ratios : 3.0 aspect_ratios : 0.3333 } } image_resizer { fixed_shape_resizer { height : 300 width : 300 } } box_predictor { convolutional_box_predictor { min_depth : 0 max_depth : 0 num_layers_before_predictor : 0 use_dropout : false dropout_keep_probability : 0.8 kernel_size : 1 box_code_size : 4 apply_sigmoid_to_scores : false class_prediction_bias_init : -4.6 conv_hyperparams { activation : RELU_6 , regularizer { l2_regularizer { weight : 0.00004 } } initializer { random_normal_initializer { stddev : 0.01 mean : 0.0 } } batch_norm { train : true , scale : true , center : true , decay : 0.97 , epsilon : 0.001 , } } } } feature_extractor { type : ' ssd_mobilenet_v1 ' min_depth : 16 depth_multiplier : 0.75 conv_hyperparams { activation : RELU_6 , regularizer { l2_regularizer { weight : 0.00004 } } initializer { truncated_normal_initializer { stddev : 0.03 mean : 0.0 } } batch_norm { train : true , scale : true , center : true , decay : 0.97 , epsilon : 0.001 , } } override_base_feature_extractor_hyperparams : true } loss { classification_loss { weighted_sigmoid_focal { alpha : 0.75 , gamma : 2.0 } } localization_loss { weighted_smooth_l1 { delta : 1.0 } } classification_weight : 1.0 localization_weight : 1.0 } normalize_loss_by_num_matches : true normalize_loc_loss_by_codesize : true post_processing { batch_non_max_suppression { score_threshold : 1e-8 iou_threshold : 0.6 max_detections_per_class : 100 max_total_detections : 100 } score_converter : SIGMOID } } } train_config : { fine_tune_checkpoint : \" PATH_TO_BE_CONFIGURED / model.ckpt \" batch_size : 2048 sync_replicas : true startup_delay_steps : 0 replicas_to_aggregate : 8 num_steps : 10000 data_augmentation_options { random_horizontal_flip { } } data_augmentation_options { ssd_random_crop { } } optimizer { momentum_optimizer : { learning_rate : { cosine_decay_learning_rate { learning_rate_base : 0.9 total_steps : 10000 warmup_learning_rate : 0.3 warmup_steps : 300 } } momentum_optimizer_value : 0.9 } use_moving_average : false } max_number_of_boxes : 100 unpad_groundtruth_tensors : false } train_input_reader : { tf_record_input_reader { input_path : \" PATH_TO_BE_CONFIGURED / mscoco_train.record -00000 - of -00100 \" } label_map_path : \" PATH_TO_BE_CONFIGURED / mscoco_label_map.pbtxt \" } eval_config : { metrics_set : \" coco_detection_metrics \" use_moving_averages : false num_examples : 8000 } eval_input_reader : { tf_record_input_reader { input_path : \" PATH_TO_BE_CONFIGURED / mscoco_val.record -00000 - of -00010 \" } label_map_path : \" PATH_TO_BE_CONFIGURED / mscoco_label_map.pbtxt \" shuffle : false num_readers : 1 }","title":"Config"},{"location":"tpu_config/#1","text":"1 2 train_config: { fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\" 1 2 3 train_config: { fine_tune_checkpoint: \"gs://your-bucket/data/model.ckpt\" fine_tune_checkpoint_type: \"detection\"","title":"\u5909\u66f4\u70b91"},{"location":"tpu_config/#2","text":"1 2 3 4 5 6 train_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record-00000-of-00100\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" } 1 2 3 4 5 6 train_input_reader: { tf_record_input_reader { input_path: \"gs://YOUR_GCS_BUCKET/data/pet_faces_train*\" } label_map_path: \"gs://YOUR_GCS_BUCKET/data/pet_label_map.pbtxt\" }","title":"\u5909\u66f4\u70b92"},{"location":"tpu_config/#3","text":"1 2 3 4 5 6 7 8 eval_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record-00000-of-00010\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" shuffle: false num_readers: 1 } 1 2 3 4 5 6 7 8 eval_input_reader: { tf_record_input_reader { input_path: \"gs://YOUR_GCS_BUCKET/data/pet_faces_val*\" } label_map_path: \"gs://YOUR_GCS_BUCKET/data/pet_label_map.pbtxt\" shuffle: false num_readers: 1 }","title":"\u5909\u66f4\u70b93"},{"location":"tpu_config/#_1","text":"1 $ gsutil cp object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config","title":"\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9"},{"location":"tpu_dataset/","text":"Dataset \u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u306e\u7528\u610f docker\u5185\u3067\u5b9f\u884c 1 2 3 $ mkdir /tmp/pet_faces_tfrecord/ $ cd /tmp/pet_faces_tfrecord/ $ curl \"http://download.tensorflow.org/models/object_detection/pet_faces_tfrecord.tar.gz\" | tar xzf - Dataset\u3092GCS\u306b\u30a2\u30c3\u30d7 docker\u5185\u3067\u5b9f\u884c 1 $ gsutil -m cp -r /tmp/pet_faces_tfrecord/pet_faces* gs:// ${ YOUR_GCS_BUCKET } /data/ pet_label_map.pbtxt\u3092GCS\u306b\u30a2\u30c3\u30d7 docker\u5185\u3067\u5b9f\u884c 1 2 3 $ cd /notebooks $ cd models/research $ gsutil cp object_detection/data/pet_label_map.pbtxt gs:// ${ YOUR_GCS_BUCKET } /data/pet_label_map.pbtxt SSD MobileNet checkpoint\u3092GCS\u306b\u30a2\u30c3\u30d7 docker\u5185\u3067\u5b9f\u884c 1 2 3 4 $ cd /tmp $ curl -O http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03.tar.gz $ tar xzf ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03.tar.gz $ gsutil cp /tmp/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03/model.ckpt.* gs:// ${ YOUR_GCS_BUCKET } /data/","title":"Dataset"},{"location":"tpu_dataset/#dataset","text":"","title":"Dataset"},{"location":"tpu_dataset/#_1","text":"docker\u5185\u3067\u5b9f\u884c 1 2 3 $ mkdir /tmp/pet_faces_tfrecord/ $ cd /tmp/pet_faces_tfrecord/ $ curl \"http://download.tensorflow.org/models/object_detection/pet_faces_tfrecord.tar.gz\" | tar xzf -","title":"\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u306e\u7528\u610f"},{"location":"tpu_dataset/#datasetgcs","text":"docker\u5185\u3067\u5b9f\u884c 1 $ gsutil -m cp -r /tmp/pet_faces_tfrecord/pet_faces* gs:// ${ YOUR_GCS_BUCKET } /data/","title":"Dataset\u3092GCS\u306b\u30a2\u30c3\u30d7"},{"location":"tpu_dataset/#pet_label_mappbtxtgcs","text":"docker\u5185\u3067\u5b9f\u884c 1 2 3 $ cd /notebooks $ cd models/research $ gsutil cp object_detection/data/pet_label_map.pbtxt gs:// ${ YOUR_GCS_BUCKET } /data/pet_label_map.pbtxt","title":"pet_label_map.pbtxt\u3092GCS\u306b\u30a2\u30c3\u30d7"},{"location":"tpu_dataset/#ssd-mobilenet-checkpointgcs","text":"docker\u5185\u3067\u5b9f\u884c 1 2 3 4 $ cd /tmp $ curl -O http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03.tar.gz $ tar xzf ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03.tar.gz $ gsutil cp /tmp/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03/model.ckpt.* gs:// ${ YOUR_GCS_BUCKET } /data/","title":"SSD MobileNet checkpoint\u3092GCS\u306b\u30a2\u30c3\u30d7"},{"location":"tpu_gcp/","text":"GCP\u306e\u8a2d\u5b9a \u53c2\u8003 https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193 GCP Project\u306e\u4f5c\u6210 https://console.cloud.google.com/projectcreate \u304b\u3089\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3059\u308b\u3002 \u652f\u6255\u3044\u306e\u6709\u52b9\u5316 TPU\u306f\u57fa\u672c\u6709\u6599\u306a\u306e\u3067\u3001\u652f\u6255\u3044\u306e\u30a2\u30ab\u30a6\u30f3\u30c8\u8a2d\u5b9a\u3092\u304a\u3053\u306a\u3046 Cloud Machine Learning Engine Compute\u306eAPI\u6709\u52b9\u5316 https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component \u304b\u3089Cloud Machine Learning Engine Compute\u306eAPI\u3092\u6709\u52b9\u5316\u3059\u308b \u30ed\u30fc\u30ab\u30eb\u306bDocker\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb $ docker run -it -p 8888:8888 tensorflow/tensorflow \u3059\u3067\u306bdocker image\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6e08\u307f\u306e\u5834\u5408 1 $ docker ps \u3067Container ID\u3092\u53d6\u5f97\u3057\u3001 1 $ dockder start \u8d77\u52d5\u3057\u305f\u30b3\u30f3\u30c6\u30caID \u3067\u8d77\u52d5\u3059\u308b\u3002 Google Cloud SDK\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u8d77\u52d5\u3057\u305fDocker\u306b\u30ed\u30b0\u30a4\u30f3\u3059\u308b Shell 1 2 3 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 349f71072537 tensorflow/tensorflow \"/run_jupyter.sh --a\u2026\" 2 weeks ago Up 3 days 6006 /tcp, 0 .0.0.0:8888->8888/tcp keen_euclid Shell 1 $ docker exec -it 349f71072537 /bin/bash Google Cloud SDK \u3092 https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu \u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u3001docker\u5185\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002 docker\u5185\u3067\u5b9f\u884c 1 2 3 4 $ export CLOUD_SDK_REPO = \"cloud-sdk- $( lsb_release -c -s ) \" && \\ echo \"deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \\ curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - && \\ apt-get update -y && apt-get install google-cloud-sdk -y \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u8a2d\u5b9a GCP\u306e\u30db\u30fc\u30e0\u306b\u623b\u308a\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u30b3\u30d4\u30fc\u3057\u3066\u304f\u308b\u3002 \u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u6307\u5b9a\u3057\u3066\u3001config\u3092\u5b9f\u884c docker\u5185\u3067\u5b9f\u884c 1 $ gcloud config set project \"\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\" \u30d0\u30b1\u30c3\u30c8\u306e\u4f5c\u6210 docker\u5185\u3067\u5b9f\u884c 1 $ gcloud auth login \u3067\u3001GCP\u3068Docker\u9593\u3067\u306e\u30ed\u30b0\u30a4\u30f3\u8a8d\u8a3c\u3092\u304a\u3053\u306a\u3046\u3002 \u30c7\u30fc\u30bf\u7b49\u3092\u304a\u304f\u30d0\u30b1\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u3002 docker\u5185\u3067\u5b9f\u884c 1 $ gsutil mb gs://\u30d0\u30b1\u30c3\u30c8\u540d \u74b0\u5883\u5909\u6570\u306e\u6574\u5099 docker\u5185\u3067\u5b9f\u884c 1 2 $ export PROJECT = \"\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\" $ export YOUR_GCS_BUCKET = \"\u30d0\u30b1\u30c3\u30c8\u540d\" docker\u5185\u3067\u5b9f\u884c Shell 1 2 $ curl -H \"Authorization: Bearer $( gcloud auth print-access-token ) \" \\ https://ml.googleapis.com/v1/projects/ ${ PROJECT } :getConfig Shell 1 2 3 4 5 6 7 { \"serviceAccount\" : \"service-###########@cloud-ml.google.com.iam.gserviceaccount.com\" , \"serviceAccountProject\" : \"#############\" \"config\" : { \"tpuServiceAccount\" : \"service-##########.iam.gserviceaccount.com\" } } docker\u5185\u3067\u5b9f\u884c 1 $ export TPU_ACCOUNT = \"tpuServiceAccount\u306e\u5024\" docker\u5185\u3067\u5b9f\u884c 1 2 $ gcloud projects add-iam-policy-binding $PROJECT \\ --member serviceAccount: $TPU_ACCOUNT --role roles/ml.serviceAgent member\u60c5\u5831\u304c\u3067\u3066\u304f\u308c\u3070\u3001\u6210\u529f\u3002","title":"GCP\u306e\u8a2d\u5b9a"},{"location":"tpu_gcp/#gcp","text":"","title":"GCP\u306e\u8a2d\u5b9a"},{"location":"tpu_gcp/#_1","text":"https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193","title":"\u53c2\u8003"},{"location":"tpu_gcp/#gcp_1","text":"","title":"GCP"},{"location":"tpu_gcp/#project","text":"https://console.cloud.google.com/projectcreate \u304b\u3089\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3059\u308b\u3002","title":"Project\u306e\u4f5c\u6210"},{"location":"tpu_gcp/#_2","text":"TPU\u306f\u57fa\u672c\u6709\u6599\u306a\u306e\u3067\u3001\u652f\u6255\u3044\u306e\u30a2\u30ab\u30a6\u30f3\u30c8\u8a2d\u5b9a\u3092\u304a\u3053\u306a\u3046","title":"\u652f\u6255\u3044\u306e\u6709\u52b9\u5316"},{"location":"tpu_gcp/#cloud-machine-learning-engine-computeapi","text":"https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component \u304b\u3089Cloud Machine Learning Engine Compute\u306eAPI\u3092\u6709\u52b9\u5316\u3059\u308b","title":"Cloud Machine Learning Engine Compute\u306eAPI\u6709\u52b9\u5316"},{"location":"tpu_gcp/#docker","text":"$ docker run -it -p 8888:8888 tensorflow/tensorflow \u3059\u3067\u306bdocker image\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6e08\u307f\u306e\u5834\u5408 1 $ docker ps \u3067Container ID\u3092\u53d6\u5f97\u3057\u3001 1 $ dockder start \u8d77\u52d5\u3057\u305f\u30b3\u30f3\u30c6\u30caID \u3067\u8d77\u52d5\u3059\u308b\u3002","title":"\u30ed\u30fc\u30ab\u30eb\u306bDocker\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"tpu_gcp/#google-cloud-sdk","text":"\u8d77\u52d5\u3057\u305fDocker\u306b\u30ed\u30b0\u30a4\u30f3\u3059\u308b Shell 1 2 3 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 349f71072537 tensorflow/tensorflow \"/run_jupyter.sh --a\u2026\" 2 weeks ago Up 3 days 6006 /tcp, 0 .0.0.0:8888->8888/tcp keen_euclid Shell 1 $ docker exec -it 349f71072537 /bin/bash Google Cloud SDK \u3092 https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu \u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u3001docker\u5185\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002 docker\u5185\u3067\u5b9f\u884c 1 2 3 4 $ export CLOUD_SDK_REPO = \"cloud-sdk- $( lsb_release -c -s ) \" && \\ echo \"deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \\ curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - && \\ apt-get update -y && apt-get install google-cloud-sdk -y","title":"Google Cloud SDK\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"tpu_gcp/#_3","text":"GCP\u306e\u30db\u30fc\u30e0\u306b\u623b\u308a\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u30b3\u30d4\u30fc\u3057\u3066\u304f\u308b\u3002 \u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u6307\u5b9a\u3057\u3066\u3001config\u3092\u5b9f\u884c docker\u5185\u3067\u5b9f\u884c 1 $ gcloud config set project \"\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\"","title":"\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u8a2d\u5b9a"},{"location":"tpu_gcp/#_4","text":"docker\u5185\u3067\u5b9f\u884c 1 $ gcloud auth login \u3067\u3001GCP\u3068Docker\u9593\u3067\u306e\u30ed\u30b0\u30a4\u30f3\u8a8d\u8a3c\u3092\u304a\u3053\u306a\u3046\u3002 \u30c7\u30fc\u30bf\u7b49\u3092\u304a\u304f\u30d0\u30b1\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u3002 docker\u5185\u3067\u5b9f\u884c 1 $ gsutil mb gs://\u30d0\u30b1\u30c3\u30c8\u540d","title":"\u30d0\u30b1\u30c3\u30c8\u306e\u4f5c\u6210"},{"location":"tpu_gcp/#_5","text":"docker\u5185\u3067\u5b9f\u884c 1 2 $ export PROJECT = \"\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\" $ export YOUR_GCS_BUCKET = \"\u30d0\u30b1\u30c3\u30c8\u540d\" docker\u5185\u3067\u5b9f\u884c Shell 1 2 $ curl -H \"Authorization: Bearer $( gcloud auth print-access-token ) \" \\ https://ml.googleapis.com/v1/projects/ ${ PROJECT } :getConfig Shell 1 2 3 4 5 6 7 { \"serviceAccount\" : \"service-###########@cloud-ml.google.com.iam.gserviceaccount.com\" , \"serviceAccountProject\" : \"#############\" \"config\" : { \"tpuServiceAccount\" : \"service-##########.iam.gserviceaccount.com\" } } docker\u5185\u3067\u5b9f\u884c 1 $ export TPU_ACCOUNT = \"tpuServiceAccount\u306e\u5024\" docker\u5185\u3067\u5b9f\u884c 1 2 $ gcloud projects add-iam-policy-binding $PROJECT \\ --member serviceAccount: $TPU_ACCOUNT --role roles/ml.serviceAgent member\u60c5\u5831\u304c\u3067\u3066\u304f\u308c\u3070\u3001\u6210\u529f\u3002","title":"\u74b0\u5883\u5909\u6570\u306e\u6574\u5099"},{"location":"tpu_run/","text":"TPU TF Slim docker\u5185\u3067\u5b9f\u884c 1 2 3 $ bash object_detection/dataset_tools/create_pycocotools_package.sh /tmp/pycocotools $ python setup.py sdist $ ( cd slim && python setup.py sdist ) \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0Job --job-dir\u306f\u3001\u5b66\u7fd2\u6bce\u306b\u9055\u3046\u30d5\u30a9\u30eb\u30c0\u3092\u6307\u5b9a\u3059\u308b(\u305d\u3046\u3057\u306a\u3044\u3068\u30a8\u30e9\u30fc)\u3002 docker\u5185\u3067\u5b9f\u884c 1 2 3 4 5 6 7 8 9 10 11 $ gcloud ml-engine jobs submit training ` whoami ` _object_detection_ ` date +%s ` \\ --job-dir = gs:// ${ YOUR_GCS_BUCKET } /train \\ --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz \\ --module-name object_detection.model_tpu_main \\ --runtime-version 1 .8 \\ --scale-tier BASIC_TPU \\ --region us-central1 \\ -- \\ --model_dir = gs:// ${ YOUR_GCS_BUCKET } /train \\ --tpu_zone us-central1 \\ --pipeline_config_path = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config \u8a55\u4fa1Job docker\u5185\u3067\u5b9f\u884c 1 2 3 4 5 6 7 8 9 10 11 gcloud ml-engine jobs submit training ` whoami ` _object_detection_eval_validation_ ` date +%s ` \\ --job-dir = gs:// ${ YOUR_GCS_BUCKET } /train \\ --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz \\ --module-name object_detection.model_main \\ --runtime-version 1 .8 \\ --scale-tier BASIC_GPU \\ --region us-central1 \\ -- \\ --model_dir = gs:// ${ YOUR_GCS_BUCKET } /train \\ --pipeline_config_path = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config \\ --checkpoint_dir = gs:// ${ YOUR_GCS_BUCKET } /train TensorBoard\u3067\u7d50\u679c\u8868\u793a docker\u5185\u3067\u5b9f\u884c 1 $ tensorboard --logdir = gs:// ${ YOUR_GCS_BUCKET } /train","title":"TPU"},{"location":"tpu_run/#tpu","text":"","title":"TPU"},{"location":"tpu_run/#tf-slim","text":"docker\u5185\u3067\u5b9f\u884c 1 2 3 $ bash object_detection/dataset_tools/create_pycocotools_package.sh /tmp/pycocotools $ python setup.py sdist $ ( cd slim && python setup.py sdist )","title":"TF Slim"},{"location":"tpu_run/#job","text":"--job-dir\u306f\u3001\u5b66\u7fd2\u6bce\u306b\u9055\u3046\u30d5\u30a9\u30eb\u30c0\u3092\u6307\u5b9a\u3059\u308b(\u305d\u3046\u3057\u306a\u3044\u3068\u30a8\u30e9\u30fc)\u3002 docker\u5185\u3067\u5b9f\u884c 1 2 3 4 5 6 7 8 9 10 11 $ gcloud ml-engine jobs submit training ` whoami ` _object_detection_ ` date +%s ` \\ --job-dir = gs:// ${ YOUR_GCS_BUCKET } /train \\ --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz \\ --module-name object_detection.model_tpu_main \\ --runtime-version 1 .8 \\ --scale-tier BASIC_TPU \\ --region us-central1 \\ -- \\ --model_dir = gs:// ${ YOUR_GCS_BUCKET } /train \\ --tpu_zone us-central1 \\ --pipeline_config_path = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config","title":"\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0Job"},{"location":"tpu_run/#job_1","text":"docker\u5185\u3067\u5b9f\u884c 1 2 3 4 5 6 7 8 9 10 11 gcloud ml-engine jobs submit training ` whoami ` _object_detection_eval_validation_ ` date +%s ` \\ --job-dir = gs:// ${ YOUR_GCS_BUCKET } /train \\ --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz \\ --module-name object_detection.model_main \\ --runtime-version 1 .8 \\ --scale-tier BASIC_GPU \\ --region us-central1 \\ -- \\ --model_dir = gs:// ${ YOUR_GCS_BUCKET } /train \\ --pipeline_config_path = gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config \\ --checkpoint_dir = gs:// ${ YOUR_GCS_BUCKET } /train","title":"\u8a55\u4fa1Job"},{"location":"tpu_run/#tensorboard","text":"docker\u5185\u3067\u5b9f\u884c 1 $ tensorboard --logdir = gs:// ${ YOUR_GCS_BUCKET } /train","title":"TensorBoard\u3067\u7d50\u679c\u8868\u793a"},{"location":"tpu_support/","text":"TPU\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u308b\u30e2\u30c7\u30eb https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md |Model\u540d|Speed (ms)| COCO mAP[^1]|config| |:--|:--|:--|:--|:--| |ssd_mobilenet_v1_0.75_depth_coco | 26 | 18 | config | |ssd_mobilenet_v1_quantized_coco | 29 | 18 | config | |ssd_mobilenet_v1_0.75_depth_quantized_coco | 29 | 16 | config | |ssd_mobilenet_v1_ppn_coco | 26 |20 | config | |ssd_mobilenet_v1_fpn_coco | 56 |32 | config |ssd_resnet_50_fpn_coco | 76 | 35 | config |","title":"TPU\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u308b\u30e2\u30c7\u30eb"},{"location":"tpu_support/#tpu","text":"https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md |Model\u540d|Speed (ms)| COCO mAP[^1]|config| |:--|:--|:--|:--|:--| |ssd_mobilenet_v1_0.75_depth_coco | 26 | 18 | config | |ssd_mobilenet_v1_quantized_coco | 29 | 18 | config | |ssd_mobilenet_v1_0.75_depth_quantized_coco | 29 | 16 | config | |ssd_mobilenet_v1_ppn_coco | 26 |20 | config | |ssd_mobilenet_v1_fpn_coco | 56 |32 | config |ssd_resnet_50_fpn_coco | 76 | 35 | config |","title":"TPU\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u308b\u30e2\u30c7\u30eb"},{"location":"tpu_tf/","text":"TensorFlow\u306e\u8a2d\u5b9a Git\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb docker\u5185\u3067\u5b9f\u884c 1 $ apt-get install git TensorFlow\u3068Model\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092Clone docker\u5185\u3067\u5b9f\u884c 1 $ git clone https://github.com/tensorflow/tensorflow docker\u5185\u3067\u5b9f\u884c 1 $ git clone https://github.com/tensorflow/model Object Detection\u3067\u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u8a2d\u5b9a docker\u5185\u3067\u5b9f\u884c 1 2 3 4 $ pip install --user Cython $ pip install --user contextlib2 $ pip install --user pillow $ pip install --user lxml COCO API\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb docker\u5185\u3067\u5b9f\u884c 1 2 3 4 $ git clone https://github.com/cocodataset/cocoapi.git $ cd cocoapi/PythonAPI $ make $ cp -r pycocotools ../../models/research/ Protobuf\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb docker\u5185\u3067\u5b9f\u884c 1 2 3 4 $ curl -OL https://github.com/google/protobuf/releases/download/v3.2.0/protoc-3.2.0-linux-x86_64.zip $ unzip protoc-3.2.0-linux-x86_64.zip -d protoc3 $ mv protoc3/bin/* /usr/bin/ $ mv protoc3/include/* /usr/include/ Protobuf\u306e\u7de8\u96c6 docker\u5185\u3067\u5b9f\u884c 1 2 3 4 $ git clone https://github.com/cocodataset/cocoapi.git $ cd cocoapi/PythonAPI $ make $ cp -r pycocotools ../../models/research/ docker\u5185\u3067\u5b9f\u884c 1 2 $ cd models/reasearch $ protoc object_detection/protos/*.proto --python_out = . docker\u5185\u3067\u5b9f\u884c 1 $ export PYTHONPATH = $PYTHONPATH : ` pwd ` : ` pwd ` /slim model_builder_test.py\u306e\u5b9f\u884c docker\u5185\u3067\u5b9f\u884c Shell 1 $ python object_detection/builders/model_builder_test.py 1 2 3 4 5 6 7 8 / usr / local / lib / python2 . 7 / dist - packages / h5py / __init__ . py : 36 : FutureWarning : Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated . In future , it will be treated as `np.float64 == np.dtype(float).type` . from ._conv import register_converters as _register_converters .................. ---------------------------------------------------------------------- Ran 18 tests in 0.084 s OK","title":"TensorFlow\u306e\u8a2d\u5b9a"},{"location":"tpu_tf/#tensorflow","text":"","title":"TensorFlow\u306e\u8a2d\u5b9a"},{"location":"tpu_tf/#git","text":"docker\u5185\u3067\u5b9f\u884c 1 $ apt-get install git","title":"Git\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"tpu_tf/#tensorflowmodelclone","text":"docker\u5185\u3067\u5b9f\u884c 1 $ git clone https://github.com/tensorflow/tensorflow docker\u5185\u3067\u5b9f\u884c 1 $ git clone https://github.com/tensorflow/model","title":"TensorFlow\u3068Model\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092Clone"},{"location":"tpu_tf/#object-detection","text":"docker\u5185\u3067\u5b9f\u884c 1 2 3 4 $ pip install --user Cython $ pip install --user contextlib2 $ pip install --user pillow $ pip install --user lxml","title":"Object Detection\u3067\u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u8a2d\u5b9a"},{"location":"tpu_tf/#coco-api","text":"docker\u5185\u3067\u5b9f\u884c 1 2 3 4 $ git clone https://github.com/cocodataset/cocoapi.git $ cd cocoapi/PythonAPI $ make $ cp -r pycocotools ../../models/research/","title":"COCO API\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"tpu_tf/#protobuf","text":"docker\u5185\u3067\u5b9f\u884c 1 2 3 4 $ curl -OL https://github.com/google/protobuf/releases/download/v3.2.0/protoc-3.2.0-linux-x86_64.zip $ unzip protoc-3.2.0-linux-x86_64.zip -d protoc3 $ mv protoc3/bin/* /usr/bin/ $ mv protoc3/include/* /usr/include/","title":"Protobuf\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"tpu_tf/#protobuf_1","text":"docker\u5185\u3067\u5b9f\u884c 1 2 3 4 $ git clone https://github.com/cocodataset/cocoapi.git $ cd cocoapi/PythonAPI $ make $ cp -r pycocotools ../../models/research/ docker\u5185\u3067\u5b9f\u884c 1 2 $ cd models/reasearch $ protoc object_detection/protos/*.proto --python_out = . docker\u5185\u3067\u5b9f\u884c 1 $ export PYTHONPATH = $PYTHONPATH : ` pwd ` : ` pwd ` /slim","title":"Protobuf\u306e\u7de8\u96c6"},{"location":"tpu_tf/#model_builder_testpy","text":"docker\u5185\u3067\u5b9f\u884c Shell 1 $ python object_detection/builders/model_builder_test.py 1 2 3 4 5 6 7 8 / usr / local / lib / python2 . 7 / dist - packages / h5py / __init__ . py : 36 : FutureWarning : Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated . In future , it will be treated as `np.float64 == np.dtype(float).type` . from ._conv import register_converters as _register_converters .................. ---------------------------------------------------------------------- Ran 18 tests in 0.084 s OK","title":"model_builder_test.py\u306e\u5b9f\u884c"},{"location":"tpu_tpu/","text":"TPU Config Shell 1 $ vi object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config Sell 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 # SSD with Mobilenet v1 0.75 depth multiplied feature extractor and focal loss. # Trained on COCO14, initialized from Imagenet classification checkpoint # Achieves 17.5 mAP on COCO14 minival dataset. Doubling the number of training # steps gets to 18.4. # This config is TPU compatible model { ssd { inplace_batchnorm_update : true freeze_batchnorm : false num_classes : 90 box_coder { faster_rcnn_box_coder { y_scale : 10.0 x_scale : 10.0 height_scale : 5.0 width_scale : 5.0 } } matcher { argmax_matcher { matched_threshold : 0.5 unmatched_threshold : 0.5 ignore_thresholds : false negatives_lower_than_unmatched : true force_match_for_each_row : true use_matmul_gather : true } } similarity_calculator { iou_similarity { } } encode_background_as_zeros : true anchor_generator { ssd_anchor_generator { num_layers : 6 min_scale : 0.2 max_scale : 0.95 aspect_ratios : 1.0 aspect_ratios : 2.0 aspect_ratios : 0.5 aspect_ratios : 3.0 aspect_ratios : 0.3333 } } image_resizer { fixed_shape_resizer { height : 300 width : 300 } } box_predictor { convolutional_box_predictor { min_depth : 0 max_depth : 0 num_layers_before_predictor : 0 use_dropout : false dropout_keep_probability : 0.8 kernel_size : 1 box_code_size : 4 apply_sigmoid_to_scores : false class_prediction_bias_init : -4.6 conv_hyperparams { activation : RELU_6 , regularizer { l2_regularizer { weight : 0.00004 } } initializer { random_normal_initializer { stddev : 0.01 mean : 0.0 } } batch_norm { train : true , scale : true , center : true , decay : 0.97 , epsilon : 0.001 , } } } } feature_extractor { type : ' ssd_mobilenet_v1 ' min_depth : 16 depth_multiplier : 0.75 conv_hyperparams { activation : RELU_6 , regularizer { l2_regularizer { weight : 0.00004 } } initializer { truncated_normal_initializer { stddev : 0.03 mean : 0.0 } } batch_norm { train : true , scale : true , center : true , decay : 0.97 , epsilon : 0.001 , } } override_base_feature_extractor_hyperparams : true } loss { classification_loss { weighted_sigmoid_focal { alpha : 0.75 , gamma : 2.0 } } localization_loss { weighted_smooth_l1 { delta : 1.0 } } classification_weight : 1.0 localization_weight : 1.0 } normalize_loss_by_num_matches : true normalize_loc_loss_by_codesize : true post_processing { batch_non_max_suppression { score_threshold : 1e-8 iou_threshold : 0.6 max_detections_per_class : 100 max_total_detections : 100 } score_converter : SIGMOID } } } train_config : { fine_tune_checkpoint : \" PATH_TO_BE_CONFIGURED / model.ckpt \" batch_size : 2048 sync_replicas : true startup_delay_steps : 0 replicas_to_aggregate : 8 num_steps : 10000 data_augmentation_options { random_horizontal_flip { } } data_augmentation_options { ssd_random_crop { } } optimizer { momentum_optimizer : { learning_rate : { cosine_decay_learning_rate { learning_rate_base : 0.9 total_steps : 10000 warmup_learning_rate : 0.3 warmup_steps : 300 } } momentum_optimizer_value : 0.9 } use_moving_average : false } max_number_of_boxes : 100 unpad_groundtruth_tensors : false } train_input_reader : { tf_record_input_reader { input_path : \" PATH_TO_BE_CONFIGURED / mscoco_train.record -00000 - of -00100 \" } label_map_path : \" PATH_TO_BE_CONFIGURED / mscoco_label_map.pbtxt \" } eval_config : { metrics_set : \" coco_detection_metrics \" use_moving_averages : false num_examples : 8000 } eval_input_reader : { tf_record_input_reader { input_path : \" PATH_TO_BE_CONFIGURED / mscoco_val.record -00000 - of -00010 \" } label_map_path : \" PATH_TO_BE_CONFIGURED / mscoco_label_map.pbtxt \" shuffle : false num_readers : 1 } \u5909\u66f4\u70b91 1 2 train_config: { fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\" 1 2 3 train_config: { fine_tune_checkpoint: \"gs://your-bucket/data/model.ckpt\" fine_tune_checkpoint_type: \"detection\" \u5909\u66f4\u70b92 1 2 3 4 5 6 train_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record-00000-of-00100\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" } 1 2 3 4 5 6 train_input_reader: { tf_record_input_reader { input_path: \"gs://YOUR_GCS_BUCKET/data/pet_faces_train*\" } label_map_path: \"gs://YOUR_GCS_BUCKET/data/pet_label_map.pbtxt\" } \u5909\u66f4\u70b93 1 2 3 4 5 6 7 8 eval_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record-00000-of-00010\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" shuffle: false num_readers: 1 } 1 2 3 4 5 6 7 8 eval_input_reader: { tf_record_input_reader { input_path: \"gs://YOUR_GCS_BUCKET/data/pet_faces_val*\" } label_map_path: \"gs://YOUR_GCS_BUCKET/data/pet_label_map.pbtxt\" shuffle: false num_readers: 1 } Config\u3092GCS\u306b\u30a2\u30c3\u30d7 1 $ gsutil cp object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config","title":"TPU"},{"location":"tpu_tpu/#tpu","text":"","title":"TPU"},{"location":"tpu_tpu/#config","text":"Shell 1 $ vi object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config Sell 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 # SSD with Mobilenet v1 0.75 depth multiplied feature extractor and focal loss. # Trained on COCO14, initialized from Imagenet classification checkpoint # Achieves 17.5 mAP on COCO14 minival dataset. Doubling the number of training # steps gets to 18.4. # This config is TPU compatible model { ssd { inplace_batchnorm_update : true freeze_batchnorm : false num_classes : 90 box_coder { faster_rcnn_box_coder { y_scale : 10.0 x_scale : 10.0 height_scale : 5.0 width_scale : 5.0 } } matcher { argmax_matcher { matched_threshold : 0.5 unmatched_threshold : 0.5 ignore_thresholds : false negatives_lower_than_unmatched : true force_match_for_each_row : true use_matmul_gather : true } } similarity_calculator { iou_similarity { } } encode_background_as_zeros : true anchor_generator { ssd_anchor_generator { num_layers : 6 min_scale : 0.2 max_scale : 0.95 aspect_ratios : 1.0 aspect_ratios : 2.0 aspect_ratios : 0.5 aspect_ratios : 3.0 aspect_ratios : 0.3333 } } image_resizer { fixed_shape_resizer { height : 300 width : 300 } } box_predictor { convolutional_box_predictor { min_depth : 0 max_depth : 0 num_layers_before_predictor : 0 use_dropout : false dropout_keep_probability : 0.8 kernel_size : 1 box_code_size : 4 apply_sigmoid_to_scores : false class_prediction_bias_init : -4.6 conv_hyperparams { activation : RELU_6 , regularizer { l2_regularizer { weight : 0.00004 } } initializer { random_normal_initializer { stddev : 0.01 mean : 0.0 } } batch_norm { train : true , scale : true , center : true , decay : 0.97 , epsilon : 0.001 , } } } } feature_extractor { type : ' ssd_mobilenet_v1 ' min_depth : 16 depth_multiplier : 0.75 conv_hyperparams { activation : RELU_6 , regularizer { l2_regularizer { weight : 0.00004 } } initializer { truncated_normal_initializer { stddev : 0.03 mean : 0.0 } } batch_norm { train : true , scale : true , center : true , decay : 0.97 , epsilon : 0.001 , } } override_base_feature_extractor_hyperparams : true } loss { classification_loss { weighted_sigmoid_focal { alpha : 0.75 , gamma : 2.0 } } localization_loss { weighted_smooth_l1 { delta : 1.0 } } classification_weight : 1.0 localization_weight : 1.0 } normalize_loss_by_num_matches : true normalize_loc_loss_by_codesize : true post_processing { batch_non_max_suppression { score_threshold : 1e-8 iou_threshold : 0.6 max_detections_per_class : 100 max_total_detections : 100 } score_converter : SIGMOID } } } train_config : { fine_tune_checkpoint : \" PATH_TO_BE_CONFIGURED / model.ckpt \" batch_size : 2048 sync_replicas : true startup_delay_steps : 0 replicas_to_aggregate : 8 num_steps : 10000 data_augmentation_options { random_horizontal_flip { } } data_augmentation_options { ssd_random_crop { } } optimizer { momentum_optimizer : { learning_rate : { cosine_decay_learning_rate { learning_rate_base : 0.9 total_steps : 10000 warmup_learning_rate : 0.3 warmup_steps : 300 } } momentum_optimizer_value : 0.9 } use_moving_average : false } max_number_of_boxes : 100 unpad_groundtruth_tensors : false } train_input_reader : { tf_record_input_reader { input_path : \" PATH_TO_BE_CONFIGURED / mscoco_train.record -00000 - of -00100 \" } label_map_path : \" PATH_TO_BE_CONFIGURED / mscoco_label_map.pbtxt \" } eval_config : { metrics_set : \" coco_detection_metrics \" use_moving_averages : false num_examples : 8000 } eval_input_reader : { tf_record_input_reader { input_path : \" PATH_TO_BE_CONFIGURED / mscoco_val.record -00000 - of -00010 \" } label_map_path : \" PATH_TO_BE_CONFIGURED / mscoco_label_map.pbtxt \" shuffle : false num_readers : 1 }","title":"Config"},{"location":"tpu_tpu/#1","text":"1 2 train_config: { fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\" 1 2 3 train_config: { fine_tune_checkpoint: \"gs://your-bucket/data/model.ckpt\" fine_tune_checkpoint_type: \"detection\"","title":"\u5909\u66f4\u70b91"},{"location":"tpu_tpu/#2","text":"1 2 3 4 5 6 train_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record-00000-of-00100\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" } 1 2 3 4 5 6 train_input_reader: { tf_record_input_reader { input_path: \"gs://YOUR_GCS_BUCKET/data/pet_faces_train*\" } label_map_path: \"gs://YOUR_GCS_BUCKET/data/pet_label_map.pbtxt\" }","title":"\u5909\u66f4\u70b92"},{"location":"tpu_tpu/#3","text":"1 2 3 4 5 6 7 8 eval_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record-00000-of-00010\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" shuffle: false num_readers: 1 } 1 2 3 4 5 6 7 8 eval_input_reader: { tf_record_input_reader { input_path: \"gs://YOUR_GCS_BUCKET/data/pet_faces_val*\" } label_map_path: \"gs://YOUR_GCS_BUCKET/data/pet_label_map.pbtxt\" shuffle: false num_readers: 1 }","title":"\u5909\u66f4\u70b93"},{"location":"tpu_tpu/#configgcs","text":"1 $ gsutil cp object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config gs:// ${ YOUR_GCS_BUCKET } /data/pipeline.config","title":"Config\u3092GCS\u306b\u30a2\u30c3\u30d7"},{"location":"traffic_sign/","text":"\u4fe1\u53f7\u8a8d\u8b58 Dataset !wget http://benchmark.ini.rub.de/Dataset/GTSRB_Final_Training_Images.zip !unzip GTSRB_Final_Training_Images.zip OpenCV\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u3044\u308c\u308b !apt-get update !apt-get -y install python-opencv Model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 from __future__ import print_function import keras from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense , Dropout , Flatten from keras.layers import Conv2D , MaxPooling2D from keras import backend as K import cv2 import csv from matplotlib import pyplot as plt import random import numpy as np from sklearn.utils import shuffle from sklearn.model_selection import train_test_split from keras.optimizers import Adam batch_size = 128 num_classes = 42 epochs = 20 rows , cols = 25 , 25 labels = [] features = [] for i in range ( 42 ): logs = [] if i < 10 : file_path = 'GTSRB/Final_Training/Images/0000' + str ( i ) file_name = file_path + '/GT-0000' + str ( i ) + '.csv' elif i < 100 : file_path = 'GTSRB/Final_Training/Images/000' + str ( i ) file_name = file_path + '/GT-000' + str ( i ) + '.csv' with open ( file_name , 'rt' ) as file : reader = csv . reader ( file , delimiter = \";\" , doublequote = True , lineterminator = \" \\r\\n \" , quotechar = '\"' , skipinitialspace = True ) for line in reader : logs . append ( line ) log_labels = logs . pop ( 0 ) for i in range ( len ( logs )): img_name = logs [ i ][ 0 ] img_path = file_path + '/' + img_name img = plt . imread ( img_path ) resized = cv2 . resize (( cv2 . cvtColor ( img , cv2 . COLOR_RGB2XYZ ))[:,:, 1 ],( cols , rows )) features . append ( resized ) labels . append ( int ( logs [ i ][ 7 ])) features = np . array ( features ) . astype ( 'float32' ) labels = np . array ( labels ) . astype ( 'float32' ) features = np . append ( features , features [:,:,:: - 1 ], axis = 0 ) labels = np . append ( labels , - labels , axis = 0 ) features , labels = shuffle ( features , labels ) train_features , test_features , train_labels , test_labels = train_test_split ( features , labels , random_state = 0 , test_size = 0.1 ) #reshape the data to feed into the network train_features = train_features . reshape ( train_features . shape [ 0 ], rows , cols , 1 ) test_features = test_features . reshape ( test_features . shape [ 0 ], rows , cols , 1 ) train_labels = keras . utils . to_categorical ( train_labels , num_classes ) test_labels = keras . utils . to_categorical ( test_labels , num_classes ) model = Sequential () model . add ( Conv2D ( 32 , kernel_size = ( 3 , 3 ), activation = 'relu' , input_shape = ( rows , cols , 1 ))) model . add ( Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Dropout ( 0.25 )) model . add ( Flatten ()) model . add ( Dense ( 128 , activation = 'relu' )) model . add ( Dropout ( 0.5 )) model . add ( Dense ( num_classes , activation = 'softmax' )) model . summary () model . compile ( loss = keras . losses . categorical_crossentropy , optimizer = keras . optimizers . Adadelta (), metrics = [ 'accuracy' ]) history = model . fit ( train_features , train_labels , batch_size = batch_size , epochs = epochs , verbose = 1 , validation_data = ( test_features , test_labels )) \u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 Train on 70144 samples, validate on 7794 samples Epoch 1/20 70144/70144 [==============================] - 121s 2ms/step - loss: 2.4245 - acc: 0.4524 - val_loss: 0.6344 - val_acc: 0.8467 Epoch 2/20 70144/70144 [==============================] - 112s 2ms/step - loss: 0.7349 - acc: 0.7871 - val_loss: 0.2587 - val_acc: 0.9244 Epoch 3/20 70144/70144 [==============================] - 110s 2ms/step - loss: 0.4527 - acc: 0.8630 - val_loss: 0.2018 - val_acc: 0.9443 Epoch 4/20 70144/70144 [==============================] - 111s 2ms/step - loss: 0.3452 - acc: 0.8942 - val_loss: 0.1557 - val_acc: 0.9528 Epoch 5/20 70144/70144 [==============================] - 115s 2ms/step - loss: 0.2877 - acc: 0.9116 - val_loss: 0.1373 - val_acc: 0.9604 Epoch 6/20 70144/70144 [==============================] - 110s 2ms/step - loss: 0.2498 - acc: 0.9231 - val_loss: 0.1341 - val_acc: 0.9598 Epoch 7/20 70144/70144 [==============================] - 114s 2ms/step - loss: 0.2223 - acc: 0.9314 - val_loss: 0.1230 - val_acc: 0.9642 Epoch 8/20 70144/70144 [==============================] - 119s 2ms/step - loss: 0.2062 - acc: 0.9366 - val_loss: 0.1095 - val_acc: 0.9666 Epoch 9/20 70144/70144 [==============================] - 119s 2ms/step - loss: 0.1904 - acc: 0.9410 - val_loss: 0.1064 - val_acc: 0.9707 Epoch 10/20 70144/70144 [==============================] - 115s 2ms/step - loss: 0.1743 - acc: 0.9462 - val_loss: 0.1030 - val_acc: 0.9706 Epoch 11/20 70144/70144 [==============================] - 108s 2ms/step - loss: 0.1677 - acc: 0.9483 - val_loss: 0.1135 - val_acc: 0.9687 Epoch 12/20 70144/70144 [==============================] - 111s 2ms/step - loss: 0.1562 - acc: 0.9518 - val_loss: 0.1033 - val_acc: 0.9713 Epoch 13/20 70144/70144 [==============================] - 112s 2ms/step - loss: 0.1540 - acc: 0.9524 - val_loss: 0.1031 - val_acc: 0.9718 Epoch 14/20 70144/70144 [==============================] - 108s 2ms/step - loss: 0.1495 - acc: 0.9535 - val_loss: 0.1079 - val_acc: 0.9706 Epoch 15/20 70144/70144 [==============================] - 107s 2ms/step - loss: 0.1426 - acc: 0.9564 - val_loss: 0.1010 - val_acc: 0.9716 Epoch 16/20 70144/70144 [==============================] - 107s 2ms/step - loss: 0.1414 - acc: 0.9563 - val_loss: 0.1018 - val_acc: 0.9729 Epoch 17/20 70144/70144 [==============================] - 111s 2ms/step - loss: 0.1320 - acc: 0.9596 - val_loss: 0.0968 - val_acc: 0.9759 Epoch 18/20 70144/70144 [==============================] - 108s 2ms/step - loss: 0.1318 - acc: 0.9597 - val_loss: 0.1028 - val_acc: 0.9732 Epoch 19/20 70144/70144 [==============================] - 109s 2ms/step - loss: 0.1304 - acc: 0.9604 - val_loss: 0.1061 - val_acc: 0.9723 Epoch 20/20 70144/70144 [==============================] - 110s 2ms/step - loss: 0.1252 - acc: 0.9626 - val_loss: 0.1021 - val_acc: 0.9751 \u30e2\u30c7\u30eb\u306e\u53ef\u8996\u5316 1 2 3 from keras.utils import plot_model ... plot_model ( model , to_file = 'model.png' ) apt-get install graphviz pip install pydot pip install pydot-ng \u30b0\u30e9\u30d5 1 2 3 4 5 6 7 8 9 10 11 from matplotlib import pyplot as plt # \u7cbe\u5ea6\u306eplot plt . plot ( history . history [ 'acc' ], marker = '.' , label = 'acc' ) plt . plot ( history . history [ 'val_acc' ], marker = '.' , label = 'val_acc' ) plt . title ( 'model accuracy' ) plt . grid () plt . xlabel ( 'epoch' ) plt . ylabel ( 'accuracy' ) plt . legend ( loc = 'best' ) plt . show () \u8a55\u4fa1 Python 1 2 3 score = model . evaluate ( test_features , test_labels , verbose = 0 ) print ( 'Test loss:' , score [ 0 ]) print ( 'Test accuracy:' , score [ 1 ]) 1 2 Test loss: 0.10212556134359667 Test accuracy: 0.9751090582652309","title":"\u4fe1\u53f7\u8a8d\u8b58"},{"location":"traffic_sign/#_1","text":"","title":"\u4fe1\u53f7\u8a8d\u8b58"},{"location":"traffic_sign/#dataset","text":"!wget http://benchmark.ini.rub.de/Dataset/GTSRB_Final_Training_Images.zip !unzip GTSRB_Final_Training_Images.zip","title":"Dataset"},{"location":"traffic_sign/#opencv","text":"!apt-get update !apt-get -y install python-opencv","title":"OpenCV\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u3044\u308c\u308b"},{"location":"traffic_sign/#model","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 from __future__ import print_function import keras from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense , Dropout , Flatten from keras.layers import Conv2D , MaxPooling2D from keras import backend as K import cv2 import csv from matplotlib import pyplot as plt import random import numpy as np from sklearn.utils import shuffle from sklearn.model_selection import train_test_split from keras.optimizers import Adam batch_size = 128 num_classes = 42 epochs = 20 rows , cols = 25 , 25 labels = [] features = [] for i in range ( 42 ): logs = [] if i < 10 : file_path = 'GTSRB/Final_Training/Images/0000' + str ( i ) file_name = file_path + '/GT-0000' + str ( i ) + '.csv' elif i < 100 : file_path = 'GTSRB/Final_Training/Images/000' + str ( i ) file_name = file_path + '/GT-000' + str ( i ) + '.csv' with open ( file_name , 'rt' ) as file : reader = csv . reader ( file , delimiter = \";\" , doublequote = True , lineterminator = \" \\r\\n \" , quotechar = '\"' , skipinitialspace = True ) for line in reader : logs . append ( line ) log_labels = logs . pop ( 0 ) for i in range ( len ( logs )): img_name = logs [ i ][ 0 ] img_path = file_path + '/' + img_name img = plt . imread ( img_path ) resized = cv2 . resize (( cv2 . cvtColor ( img , cv2 . COLOR_RGB2XYZ ))[:,:, 1 ],( cols , rows )) features . append ( resized ) labels . append ( int ( logs [ i ][ 7 ])) features = np . array ( features ) . astype ( 'float32' ) labels = np . array ( labels ) . astype ( 'float32' ) features = np . append ( features , features [:,:,:: - 1 ], axis = 0 ) labels = np . append ( labels , - labels , axis = 0 ) features , labels = shuffle ( features , labels ) train_features , test_features , train_labels , test_labels = train_test_split ( features , labels , random_state = 0 , test_size = 0.1 ) #reshape the data to feed into the network train_features = train_features . reshape ( train_features . shape [ 0 ], rows , cols , 1 ) test_features = test_features . reshape ( test_features . shape [ 0 ], rows , cols , 1 ) train_labels = keras . utils . to_categorical ( train_labels , num_classes ) test_labels = keras . utils . to_categorical ( test_labels , num_classes ) model = Sequential () model . add ( Conv2D ( 32 , kernel_size = ( 3 , 3 ), activation = 'relu' , input_shape = ( rows , cols , 1 ))) model . add ( Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Dropout ( 0.25 )) model . add ( Flatten ()) model . add ( Dense ( 128 , activation = 'relu' )) model . add ( Dropout ( 0.5 )) model . add ( Dense ( num_classes , activation = 'softmax' )) model . summary () model . compile ( loss = keras . losses . categorical_crossentropy , optimizer = keras . optimizers . Adadelta (), metrics = [ 'accuracy' ]) history = model . fit ( train_features , train_labels , batch_size = batch_size , epochs = epochs , verbose = 1 , validation_data = ( test_features , test_labels ))","title":"Model"},{"location":"traffic_sign/#_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 Train on 70144 samples, validate on 7794 samples Epoch 1/20 70144/70144 [==============================] - 121s 2ms/step - loss: 2.4245 - acc: 0.4524 - val_loss: 0.6344 - val_acc: 0.8467 Epoch 2/20 70144/70144 [==============================] - 112s 2ms/step - loss: 0.7349 - acc: 0.7871 - val_loss: 0.2587 - val_acc: 0.9244 Epoch 3/20 70144/70144 [==============================] - 110s 2ms/step - loss: 0.4527 - acc: 0.8630 - val_loss: 0.2018 - val_acc: 0.9443 Epoch 4/20 70144/70144 [==============================] - 111s 2ms/step - loss: 0.3452 - acc: 0.8942 - val_loss: 0.1557 - val_acc: 0.9528 Epoch 5/20 70144/70144 [==============================] - 115s 2ms/step - loss: 0.2877 - acc: 0.9116 - val_loss: 0.1373 - val_acc: 0.9604 Epoch 6/20 70144/70144 [==============================] - 110s 2ms/step - loss: 0.2498 - acc: 0.9231 - val_loss: 0.1341 - val_acc: 0.9598 Epoch 7/20 70144/70144 [==============================] - 114s 2ms/step - loss: 0.2223 - acc: 0.9314 - val_loss: 0.1230 - val_acc: 0.9642 Epoch 8/20 70144/70144 [==============================] - 119s 2ms/step - loss: 0.2062 - acc: 0.9366 - val_loss: 0.1095 - val_acc: 0.9666 Epoch 9/20 70144/70144 [==============================] - 119s 2ms/step - loss: 0.1904 - acc: 0.9410 - val_loss: 0.1064 - val_acc: 0.9707 Epoch 10/20 70144/70144 [==============================] - 115s 2ms/step - loss: 0.1743 - acc: 0.9462 - val_loss: 0.1030 - val_acc: 0.9706 Epoch 11/20 70144/70144 [==============================] - 108s 2ms/step - loss: 0.1677 - acc: 0.9483 - val_loss: 0.1135 - val_acc: 0.9687 Epoch 12/20 70144/70144 [==============================] - 111s 2ms/step - loss: 0.1562 - acc: 0.9518 - val_loss: 0.1033 - val_acc: 0.9713 Epoch 13/20 70144/70144 [==============================] - 112s 2ms/step - loss: 0.1540 - acc: 0.9524 - val_loss: 0.1031 - val_acc: 0.9718 Epoch 14/20 70144/70144 [==============================] - 108s 2ms/step - loss: 0.1495 - acc: 0.9535 - val_loss: 0.1079 - val_acc: 0.9706 Epoch 15/20 70144/70144 [==============================] - 107s 2ms/step - loss: 0.1426 - acc: 0.9564 - val_loss: 0.1010 - val_acc: 0.9716 Epoch 16/20 70144/70144 [==============================] - 107s 2ms/step - loss: 0.1414 - acc: 0.9563 - val_loss: 0.1018 - val_acc: 0.9729 Epoch 17/20 70144/70144 [==============================] - 111s 2ms/step - loss: 0.1320 - acc: 0.9596 - val_loss: 0.0968 - val_acc: 0.9759 Epoch 18/20 70144/70144 [==============================] - 108s 2ms/step - loss: 0.1318 - acc: 0.9597 - val_loss: 0.1028 - val_acc: 0.9732 Epoch 19/20 70144/70144 [==============================] - 109s 2ms/step - loss: 0.1304 - acc: 0.9604 - val_loss: 0.1061 - val_acc: 0.9723 Epoch 20/20 70144/70144 [==============================] - 110s 2ms/step - loss: 0.1252 - acc: 0.9626 - val_loss: 0.1021 - val_acc: 0.9751","title":"\u7d50\u679c"},{"location":"traffic_sign/#_3","text":"1 2 3 from keras.utils import plot_model ... plot_model ( model , to_file = 'model.png' ) apt-get install graphviz pip install pydot pip install pydot-ng","title":"\u30e2\u30c7\u30eb\u306e\u53ef\u8996\u5316"},{"location":"traffic_sign/#_4","text":"1 2 3 4 5 6 7 8 9 10 11 from matplotlib import pyplot as plt # \u7cbe\u5ea6\u306eplot plt . plot ( history . history [ 'acc' ], marker = '.' , label = 'acc' ) plt . plot ( history . history [ 'val_acc' ], marker = '.' , label = 'val_acc' ) plt . title ( 'model accuracy' ) plt . grid () plt . xlabel ( 'epoch' ) plt . ylabel ( 'accuracy' ) plt . legend ( loc = 'best' ) plt . show ()","title":"\u30b0\u30e9\u30d5"},{"location":"traffic_sign/#_5","text":"Python 1 2 3 score = model . evaluate ( test_features , test_labels , verbose = 0 ) print ( 'Test loss:' , score [ 0 ]) print ( 'Test accuracy:' , score [ 1 ]) 1 2 Test loss: 0.10212556134359667 Test accuracy: 0.9751090582652309","title":"\u8a55\u4fa1"},{"location":"1.\u958b\u767a\u74b0\u5883\u306e\u69cb\u7bc9/dev/","text":"\u958b\u767a\u74b0\u5883 TensorFlow\u7b49\u306e\u5b9f\u884c\u74b0\u5883\u306f\u3001Docker\u4e0a\u306b\u3001TensorFlow + Jupyter\u3067Local\u306b\u69cb\u7bc9\u3057\u307e\u3059\u3002 Docker\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://www.docker.com/ Docker Community Edition for Mac https://store.docker.com/editions/community/docker-ce-desktop-mac Docker Community Edition for Windows https://store.docker.com/editions/community/docker-ce-desktop-windows \u3092\u5404\u74b0\u5883\u306b\u5408\u308f\u305b\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 TensorFlow Docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Docker\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5b8c\u4e86\u3057\u305f\u3089\u3001\u30de\u30a6\u30b9\u306eDouble\u30af\u30ea\u30c3\u30af\u3067Docker\u3092\u8d77\u52d5\u3057\u3001Docker\u30b3\u30de\u30f3\u30c9\u3067TensorFlow Docker\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u304d\u307e\u3059\u3002 $ docker run -it -p 8888:8888 tensorflow/tensorflow 1 2 3 4 5 [C 16:50:37.847 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://localhost:8888/?token=######################### localhost:8888\u306b\u63a5\u7d9a\u3002\u4e0a\u306e\u30ea\u30f3\u30af\u3092\u30b3\u30d4\u30fc\u3057\u3066\u63a5\u7d9a\u3059\u308b\u3002 \u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8 notebook\u306e\u4f5c\u6210 Keras\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb !pip install keras OpenCV\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb !apt-get update !apt-get -y install python-opencv","title":"\u958b\u767a\u74b0\u5883"},{"location":"1.\u958b\u767a\u74b0\u5883\u306e\u69cb\u7bc9/dev/#_1","text":"TensorFlow\u7b49\u306e\u5b9f\u884c\u74b0\u5883\u306f\u3001Docker\u4e0a\u306b\u3001TensorFlow + Jupyter\u3067Local\u306b\u69cb\u7bc9\u3057\u307e\u3059\u3002","title":"\u958b\u767a\u74b0\u5883"},{"location":"1.\u958b\u767a\u74b0\u5883\u306e\u69cb\u7bc9/dev/#docker","text":"https://www.docker.com/ Docker Community Edition for Mac https://store.docker.com/editions/community/docker-ce-desktop-mac Docker Community Edition for Windows https://store.docker.com/editions/community/docker-ce-desktop-windows \u3092\u5404\u74b0\u5883\u306b\u5408\u308f\u305b\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002","title":"Docker\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"1.\u958b\u767a\u74b0\u5883\u306e\u69cb\u7bc9/dev/#tensorflow-docker","text":"Docker\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5b8c\u4e86\u3057\u305f\u3089\u3001\u30de\u30a6\u30b9\u306eDouble\u30af\u30ea\u30c3\u30af\u3067Docker\u3092\u8d77\u52d5\u3057\u3001Docker\u30b3\u30de\u30f3\u30c9\u3067TensorFlow Docker\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u304d\u307e\u3059\u3002 $ docker run -it -p 8888:8888 tensorflow/tensorflow 1 2 3 4 5 [C 16:50:37.847 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://localhost:8888/?token=######################### localhost:8888\u306b\u63a5\u7d9a\u3002\u4e0a\u306e\u30ea\u30f3\u30af\u3092\u30b3\u30d4\u30fc\u3057\u3066\u63a5\u7d9a\u3059\u308b\u3002","title":"TensorFlow Docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"1.\u958b\u767a\u74b0\u5883\u306e\u69cb\u7bc9/dev/#_2","text":"","title":"\u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8"},{"location":"1.\u958b\u767a\u74b0\u5883\u306e\u69cb\u7bc9/dev/#notebook","text":"","title":"notebook\u306e\u4f5c\u6210"},{"location":"1.\u958b\u767a\u74b0\u5883\u306e\u69cb\u7bc9/dev/#keras","text":"!pip install keras","title":"Keras\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"1.\u958b\u767a\u74b0\u5883\u306e\u69cb\u7bc9/dev/#opencv","text":"!apt-get update !apt-get -y install python-opencv","title":"OpenCV\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"1.\u958b\u767a\u74b0\u5883\u306e\u69cb\u7bc9/mnist/","text":"MNIST MNIST 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 from __future__ import print_function import keras from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense , Dropout , Flatten from keras.layers import Conv2D , MaxPooling2D from keras import backend as K batch_size = 128 num_classes = 10 epochs = 10 img_rows = 28 img_cols = 28 ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () if K . image_data_format () == 'channels_first' : x_train = x_train . reshape ( x_train . shape [ 0 ], 1 , img_rows , img_cols ) x_test = x_test . reshape ( x_test . shape [ 0 ], 1 , img_rows , img_cols ) input_shape = ( 1 , img_rows , img_cols ) else : x_train = x_train . reshape ( x_train . shape [ 0 ], img_rows , img_cols , 1 ) x_test = x_test . reshape ( x_test . shape [ 0 ], img_rows , img_cols , 1 ) input_shape = ( img_rows , img_cols , 1 ) x_train = x_train . astype ( 'float32' ) x_test = x_test . astype ( 'float32' ) x_train /= 255 x_test /= 255 y_train = keras . utils . to_categorical ( y_train , num_classes ) y_test = keras . utils . to_categorical ( y_test , num_classes ) model = Sequential () model . add ( Conv2D ( 32 , kernel_size = ( 3 , 3 ), activation = 'relu' , input_shape = input_shape )) model . add ( Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Flatten ()) model . add ( Dense ( 128 , activation = 'relu' )) model . add ( Dropout ( 0.5 )) model . add ( Dense ( num_classes , activation = 'softmax' )) model . summary () model . compile ( loss = keras . losses . categorical_crossentropy , optimizer = keras . optimizers . Adadelta (), metrics = [ 'accuracy' ]) history = model . fit ( x_train , y_train , batch_size = batch_size , epochs = epochs , verbose = 1 , validation_data = ( x_test , y_test )) History\u306eGraph\u5316 1 2 3 4 5 6 7 8 9 10 from matplotlib import pyplot as plt plt . plot ( history . history [ 'acc' ], marker = '.' , label = 'acc' ) plt . plot ( history . history [ 'val_acc' ], marker = '.' , label = 'val_acc' ) plt . title ( 'model accuracy' ) plt . grid () plt . xlabel ( 'epoch' ) plt . ylabel ( 'accuracy' ) plt . legend ( loc = 'best' ) plt . show () \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 1 2 3 score = model . evaluate ( x_test , y_test , verbose = 0 ) print ( 'Test loss:' , score [ 0 ]) print ( 'Test accuracy:' , score [ 1 ]) \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58 1 2 3 4 5 6 7 8 9 10 11 12 13 from keras.models import load_model # \u30e2\u30c7\u30eb\u306e\u4fdd\u5b58 model . save ( 'my_model.h5' ) # \u30d2\u30fc\u30d7\u30e1\u30e2\u30ea\u4e0a\u306e\u30e2\u30c7\u30eb\u306e\u524a\u9664 del model # \u30e2\u30c7\u30eb\u306e\u8aad\u307f\u8fbc\u307f model = load_model ( 'my_model.h5' ) # \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 score = model . evaluate ( x_test , y_test , verbose = 0 ) print ( 'Test loss:' , score [ 0 ]) print ( 'Test accuracy:' , score [ 1 ])","title":"MNIST"},{"location":"1.\u958b\u767a\u74b0\u5883\u306e\u69cb\u7bc9/mnist/#mnist","text":"","title":"MNIST"},{"location":"1.\u958b\u767a\u74b0\u5883\u306e\u69cb\u7bc9/mnist/#mnist_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 from __future__ import print_function import keras from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense , Dropout , Flatten from keras.layers import Conv2D , MaxPooling2D from keras import backend as K batch_size = 128 num_classes = 10 epochs = 10 img_rows = 28 img_cols = 28 ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () if K . image_data_format () == 'channels_first' : x_train = x_train . reshape ( x_train . shape [ 0 ], 1 , img_rows , img_cols ) x_test = x_test . reshape ( x_test . shape [ 0 ], 1 , img_rows , img_cols ) input_shape = ( 1 , img_rows , img_cols ) else : x_train = x_train . reshape ( x_train . shape [ 0 ], img_rows , img_cols , 1 ) x_test = x_test . reshape ( x_test . shape [ 0 ], img_rows , img_cols , 1 ) input_shape = ( img_rows , img_cols , 1 ) x_train = x_train . astype ( 'float32' ) x_test = x_test . astype ( 'float32' ) x_train /= 255 x_test /= 255 y_train = keras . utils . to_categorical ( y_train , num_classes ) y_test = keras . utils . to_categorical ( y_test , num_classes ) model = Sequential () model . add ( Conv2D ( 32 , kernel_size = ( 3 , 3 ), activation = 'relu' , input_shape = input_shape )) model . add ( Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Flatten ()) model . add ( Dense ( 128 , activation = 'relu' )) model . add ( Dropout ( 0.5 )) model . add ( Dense ( num_classes , activation = 'softmax' )) model . summary () model . compile ( loss = keras . losses . categorical_crossentropy , optimizer = keras . optimizers . Adadelta (), metrics = [ 'accuracy' ]) history = model . fit ( x_train , y_train , batch_size = batch_size , epochs = epochs , verbose = 1 , validation_data = ( x_test , y_test ))","title":"MNIST"},{"location":"1.\u958b\u767a\u74b0\u5883\u306e\u69cb\u7bc9/mnist/#historygraph","text":"1 2 3 4 5 6 7 8 9 10 from matplotlib import pyplot as plt plt . plot ( history . history [ 'acc' ], marker = '.' , label = 'acc' ) plt . plot ( history . history [ 'val_acc' ], marker = '.' , label = 'val_acc' ) plt . title ( 'model accuracy' ) plt . grid () plt . xlabel ( 'epoch' ) plt . ylabel ( 'accuracy' ) plt . legend ( loc = 'best' ) plt . show ()","title":"History\u306eGraph\u5316"},{"location":"1.\u958b\u767a\u74b0\u5883\u306e\u69cb\u7bc9/mnist/#_1","text":"1 2 3 score = model . evaluate ( x_test , y_test , verbose = 0 ) print ( 'Test loss:' , score [ 0 ]) print ( 'Test accuracy:' , score [ 1 ])","title":"\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1"},{"location":"1.\u958b\u767a\u74b0\u5883\u306e\u69cb\u7bc9/mnist/#_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 from keras.models import load_model # \u30e2\u30c7\u30eb\u306e\u4fdd\u5b58 model . save ( 'my_model.h5' ) # \u30d2\u30fc\u30d7\u30e1\u30e2\u30ea\u4e0a\u306e\u30e2\u30c7\u30eb\u306e\u524a\u9664 del model # \u30e2\u30c7\u30eb\u306e\u8aad\u307f\u8fbc\u307f model = load_model ( 'my_model.h5' ) # \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 score = model . evaluate ( x_test , y_test , verbose = 0 ) print ( 'Test loss:' , score [ 0 ]) print ( 'Test accuracy:' , score [ 1 ])","title":"\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58"},{"location":"2.OpenCV/color/","text":"OpenCV \u8272\u6210\u5206\u306e\u5206\u89e3 R\u306e\u307f 1 2 3 4 5 6 7 8 9 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2BGR ) solo_image = cvt_image [:,:, 0 ] plt . imshow ( solo_image , cmap = plt . cm . gray_r ,); plt . show () G\u306e\u307f 1 2 3 4 5 6 7 8 9 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2BGR ) solo_image = cvt_image [:,:, 1 ] plt . imshow ( solo_image , cmap = plt . cm . gray_r ,); plt . show () B\u306e\u307f 1 2 3 4 5 6 7 8 9 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2BGR ) solo_image = cvt_image [:,:, 2 ] plt . imshow ( solo_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"OpenCV"},{"location":"2.OpenCV/color/#opencv","text":"","title":"OpenCV"},{"location":"2.OpenCV/color/#_1","text":"","title":"\u8272\u6210\u5206\u306e\u5206\u89e3"},{"location":"2.OpenCV/color/#r","text":"1 2 3 4 5 6 7 8 9 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2BGR ) solo_image = cvt_image [:,:, 0 ] plt . imshow ( solo_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"R\u306e\u307f"},{"location":"2.OpenCV/color/#g","text":"1 2 3 4 5 6 7 8 9 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2BGR ) solo_image = cvt_image [:,:, 1 ] plt . imshow ( solo_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"G\u306e\u307f"},{"location":"2.OpenCV/color/#b","text":"1 2 3 4 5 6 7 8 9 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2BGR ) solo_image = cvt_image [:,:, 2 ] plt . imshow ( solo_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"B\u306e\u307f"},{"location":"2.OpenCV/colorspace/","text":"OpenCV Color spaces\u306e\u5909\u66f4 RGB2BGR 1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2BGR ) plt . imshow ( cvt_image , cmap = plt . cm . gray_r ,); plt . show () RGB2GRAY 1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2GRAY ) plt . imshow ( cvt_image , cmap = plt . cm . gray_r ,); plt . show () RGB2HLS 1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2HLS ) plt . imshow ( cvt_image , cmap = plt . cm . gray_r ,); plt . show () RGB2HSV 1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2HSV ) plt . imshow ( cvt_image , cmap = plt . cm . gray_r ,); plt . show () RGB2XYZ 1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2XYZ ) plt . imshow ( cvt_image , cmap = plt . cm . gray_r ,); plt . show () RGB2HLS 1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2HLS ) plt . imshow ( cvt_image , cmap = plt . cm . gray_r ,); plt . show () RGB2YUV 1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2YUV ) plt . imshow ( cvt_image , cmap = plt . cm . gray_r ,); plt . show () \u53cd\u8ee2 1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . bitwise_not ( image ) plt . imshow ( cvt_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"OpenCV"},{"location":"2.OpenCV/colorspace/#opencv","text":"","title":"OpenCV"},{"location":"2.OpenCV/colorspace/#color-spaces","text":"","title":"Color spaces\u306e\u5909\u66f4"},{"location":"2.OpenCV/colorspace/#rgb2bgr","text":"1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2BGR ) plt . imshow ( cvt_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"RGB2BGR"},{"location":"2.OpenCV/colorspace/#rgb2gray","text":"1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2GRAY ) plt . imshow ( cvt_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"RGB2GRAY"},{"location":"2.OpenCV/colorspace/#rgb2hls","text":"1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2HLS ) plt . imshow ( cvt_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"RGB2HLS"},{"location":"2.OpenCV/colorspace/#rgb2hsv","text":"1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2HSV ) plt . imshow ( cvt_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"RGB2HSV"},{"location":"2.OpenCV/colorspace/#rgb2xyz","text":"1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2XYZ ) plt . imshow ( cvt_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"RGB2XYZ"},{"location":"2.OpenCV/colorspace/#rgb2hls_1","text":"1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2HLS ) plt . imshow ( cvt_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"RGB2HLS"},{"location":"2.OpenCV/colorspace/#rgb2yuv","text":"1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . cvtColor ( image , cv2 . COLOR_RGB2YUV ) plt . imshow ( cvt_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"RGB2YUV"},{"location":"2.OpenCV/colorspace/#_1","text":"1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) cvt_image = cv2 . bitwise_not ( image ) plt . imshow ( cvt_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"\u53cd\u8ee2"},{"location":"2.OpenCV/crop/","text":"OpenCV \u7bc4\u56f2\u3092\u6307\u5b9a\u3057\u3066Crop 1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) crop_image = image [ 5 : 25 , 0 : 30 , :] plt . imshow ( crop_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"OpenCV"},{"location":"2.OpenCV/crop/#opencv","text":"","title":"OpenCV"},{"location":"2.OpenCV/crop/#crop","text":"1 2 3 4 5 6 7 8 from matplotlib import pyplot as plt import cv2 image_path = 'GTSRB/Final_Training/Images/00000/00000_00000.ppm' image = plt . imread ( image_path ) crop_image = image [ 5 : 25 , 0 : 30 , :] plt . imshow ( crop_image , cmap = plt . cm . gray_r ,); plt . show ()","title":"\u7bc4\u56f2\u3092\u6307\u5b9a\u3057\u3066Crop"}]}